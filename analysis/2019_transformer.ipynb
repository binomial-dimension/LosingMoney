{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E)\n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "\n",
    "input_window = 150  # number of input steps\n",
    "output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "batch_size = 512\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=300, num_layers=1, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size,\n",
    "                                                        nhead=10,\n",
    "                                                        dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer,\n",
    "                                                         num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(feature_size, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,\n",
    "                                          self.src_mask)  #, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(\n",
    "            mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - tw):\n",
    "        train_seq = input_data[i:i + tw]\n",
    "        train_label = input_data[i + output_window:i + tw + output_window]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "\n",
    "def get_data(name):\n",
    "    # construct a littel toy dataset\n",
    "    time = np.arange(0, 400, 0.1)\n",
    "    data = pd.read_csv('../data/399300.csv')\n",
    "    amplitude = data[name].values\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    #loading weather data from a file\n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "    # looks like normalizing input values curtial for the model\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    sampels = int(len(amplitude)*0.8)\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment..\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:\n",
    "                                    -output_window]  #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1)\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]  #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device),scaler\n",
    "\n",
    "\n",
    "def get_batch(source, i, batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    input = torch.stack(\n",
    "        torch.stack([item[0] for item in data]).chunk(input_window,1))  # 1 is feature size\n",
    "    target = torch.stack(\n",
    "        torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data,epoch):\n",
    "    model.train()  # Turn on the train mode \\o/\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                      epoch, batch,\n",
    "                      len(train_data) // batch_size,\n",
    "                      scheduler.get_lr()[0], elapsed * 1000 / log_interval,\n",
    "                      cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def plot_and_loss(eval_model, data_source, epoch):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                    0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "    #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color=\"red\", label=\"prediction\")\n",
    "    pyplot.plot(truth[:len(test_result)], color=\"blue\", label=\"truth\")\n",
    "    pyplot.plot(test_result - truth, color=\"green\", label=\"error\")\n",
    "    pyplot.legend()\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "# predict the next n steps based on the input data\n",
    "def predict_future(eval_model, data_source, steps):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source, 0, 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):\n",
    "            output = eval_model(data[-input_window:])\n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "    data = data.cpu().view(-1)\n",
    "\n",
    "    # I used this plot to visualize if the model pics up any long therm structure within the data.\n",
    "    pyplot.plot(data, color=\"red\")\n",
    "    pyplot.plot(data[:input_window], color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval()  # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "            total_loss += len(data[0]) * criterion(output,\n",
    "                                                   targets).cpu().item()\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get(name):\n",
    "    input_window = 120  # number of input steps\n",
    "    output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "    batch_size = 512\n",
    "    device = torch.device(\"cuda\")\n",
    "    train_data, val_data, scaler = get_data(name)\n",
    "    global model \n",
    "    model = TransAm().to(device)\n",
    "\n",
    "    global criterion\n",
    "    criterion = nn.MSELoss()\n",
    "    lr = 0.001  # learning rate\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    global optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    global scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    global epochs\n",
    "    epochs = 300  # The number of epochs\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_data,epoch)\n",
    "\n",
    "        if (epoch % 100 == 0):\n",
    "            val_loss = plot_and_loss(model, val_data, epoch)\n",
    "            predict_future(model, val_data, 5)\n",
    "        else:\n",
    "            val_loss = evaluate(model, val_data)\n",
    "\n",
    "        if(epoch % 50 ==0):\n",
    "            print('-' * 89)\n",
    "            print(\n",
    "                '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'\n",
    "                .format(epoch, (time.time() - epoch_start_time), val_loss,\n",
    "                        math.exp(val_loss)))\n",
    "            print('-' * 89)\n",
    "        scheduler.step()\n",
    "    \n",
    "    def get_predict(eval_model, data_source):\n",
    "        eval_model.eval()\n",
    "        total_loss = 0.\n",
    "        test_result = torch.Tensor(0)\n",
    "        truth = torch.Tensor(0)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data_source) - 1):\n",
    "                data, target = get_batch(data_source, i, 1)\n",
    "                output = eval_model(data)\n",
    "                total_loss += criterion(output, target).item()\n",
    "                test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                        0)\n",
    "                truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "        #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "        return scaler.inverse_transform(test_result.view(-1, 1)), scaler.inverse_transform(truth.view(-1, 1))\n",
    "\n",
    "    \n",
    "    return get_predict(model, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools\\anaconda3\\envs\\data\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     7/    7 batches | lr 0.001000 | 108.07 ms | loss 30.82093 | ppl 24286263798821.22\n",
      "| epoch   2 |     7/    7 batches | lr 0.000960 | 64.96 ms | loss 0.67021 | ppl     1.95\n",
      "| epoch   3 |     7/    7 batches | lr 0.000941 | 65.02 ms | loss 0.24297 | ppl     1.28\n",
      "| epoch   4 |     7/    7 batches | lr 0.000922 | 65.11 ms | loss 0.10511 | ppl     1.11\n",
      "| epoch   5 |     7/    7 batches | lr 0.000904 | 64.77 ms | loss 0.17858 | ppl     1.20\n",
      "| epoch   6 |     7/    7 batches | lr 0.000886 | 65.10 ms | loss 0.06998 | ppl     1.07\n",
      "| epoch   7 |     7/    7 batches | lr 0.000868 | 65.05 ms | loss 0.08113 | ppl     1.08\n",
      "| epoch   8 |     7/    7 batches | lr 0.000851 | 64.94 ms | loss 0.02371 | ppl     1.02\n",
      "| epoch   9 |     7/    7 batches | lr 0.000834 | 65.13 ms | loss 0.02093 | ppl     1.02\n",
      "| epoch  10 |     7/    7 batches | lr 0.000817 | 65.94 ms | loss 0.02407 | ppl     1.02\n",
      "| epoch  11 |     7/    7 batches | lr 0.000801 | 65.80 ms | loss 0.04480 | ppl     1.05\n",
      "| epoch  12 |     7/    7 batches | lr 0.000785 | 65.44 ms | loss 0.02978 | ppl     1.03\n",
      "| epoch  13 |     7/    7 batches | lr 0.000769 | 66.23 ms | loss 0.04384 | ppl     1.04\n",
      "| epoch  14 |     7/    7 batches | lr 0.000754 | 65.86 ms | loss 0.02828 | ppl     1.03\n",
      "| epoch  15 |     7/    7 batches | lr 0.000739 | 65.46 ms | loss 0.02717 | ppl     1.03\n",
      "| epoch  16 |     7/    7 batches | lr 0.000724 | 65.00 ms | loss 0.02220 | ppl     1.02\n",
      "| epoch  17 |     7/    7 batches | lr 0.000709 | 66.71 ms | loss 0.01884 | ppl     1.02\n",
      "| epoch  18 |     7/    7 batches | lr 0.000695 | 64.94 ms | loss 0.03082 | ppl     1.03\n",
      "| epoch  19 |     7/    7 batches | lr 0.000681 | 64.53 ms | loss 0.03116 | ppl     1.03\n",
      "| epoch  20 |     7/    7 batches | lr 0.000668 | 64.65 ms | loss 0.01007 | ppl     1.01\n",
      "| epoch  21 |     7/    7 batches | lr 0.000654 | 65.66 ms | loss 0.00975 | ppl     1.01\n",
      "| epoch  22 |     7/    7 batches | lr 0.000641 | 65.94 ms | loss 0.01240 | ppl     1.01\n",
      "| epoch  23 |     7/    7 batches | lr 0.000628 | 65.11 ms | loss 0.00938 | ppl     1.01\n",
      "| epoch  24 |     7/    7 batches | lr 0.000616 | 65.08 ms | loss 0.02222 | ppl     1.02\n",
      "| epoch  25 |     7/    7 batches | lr 0.000603 | 65.06 ms | loss 0.03070 | ppl     1.03\n",
      "| epoch  26 |     7/    7 batches | lr 0.000591 | 67.10 ms | loss 0.08644 | ppl     1.09\n",
      "| epoch  27 |     7/    7 batches | lr 0.000580 | 65.23 ms | loss 0.05954 | ppl     1.06\n",
      "| epoch  28 |     7/    7 batches | lr 0.000568 | 65.11 ms | loss 0.06244 | ppl     1.06\n",
      "| epoch  29 |     7/    7 batches | lr 0.000557 | 64.89 ms | loss 0.03126 | ppl     1.03\n",
      "| epoch  30 |     7/    7 batches | lr 0.000545 | 65.16 ms | loss 0.01367 | ppl     1.01\n",
      "| epoch  31 |     7/    7 batches | lr 0.000535 | 65.07 ms | loss 0.03521 | ppl     1.04\n",
      "| epoch  32 |     7/    7 batches | lr 0.000524 | 64.65 ms | loss 0.04699 | ppl     1.05\n",
      "| epoch  33 |     7/    7 batches | lr 0.000513 | 64.93 ms | loss 0.06543 | ppl     1.07\n",
      "| epoch  34 |     7/    7 batches | lr 0.000503 | 64.25 ms | loss 0.02218 | ppl     1.02\n",
      "| epoch  35 |     7/    7 batches | lr 0.000493 | 64.98 ms | loss 0.02830 | ppl     1.03\n",
      "| epoch  36 |     7/    7 batches | lr 0.000483 | 64.73 ms | loss 0.03474 | ppl     1.04\n",
      "| epoch  37 |     7/    7 batches | lr 0.000474 | 65.11 ms | loss 0.04563 | ppl     1.05\n",
      "| epoch  38 |     7/    7 batches | lr 0.000464 | 64.73 ms | loss 0.02328 | ppl     1.02\n",
      "| epoch  39 |     7/    7 batches | lr 0.000455 | 65.13 ms | loss 0.02976 | ppl     1.03\n",
      "| epoch  40 |     7/    7 batches | lr 0.000446 | 64.77 ms | loss 0.04199 | ppl     1.04\n",
      "| epoch  41 |     7/    7 batches | lr 0.000437 | 65.23 ms | loss 0.02301 | ppl     1.02\n",
      "| epoch  42 |     7/    7 batches | lr 0.000428 | 64.13 ms | loss 0.03102 | ppl     1.03\n",
      "| epoch  43 |     7/    7 batches | lr 0.000419 | 63.87 ms | loss 0.04958 | ppl     1.05\n",
      "| epoch  44 |     7/    7 batches | lr 0.000411 | 63.60 ms | loss 0.02530 | ppl     1.03\n",
      "| epoch  45 |     7/    7 batches | lr 0.000403 | 63.86 ms | loss 0.02999 | ppl     1.03\n",
      "| epoch  46 |     7/    7 batches | lr 0.000395 | 63.70 ms | loss 0.03774 | ppl     1.04\n",
      "| epoch  47 |     7/    7 batches | lr 0.000387 | 63.78 ms | loss 0.01254 | ppl     1.01\n",
      "| epoch  48 |     7/    7 batches | lr 0.000379 | 63.92 ms | loss 0.01135 | ppl     1.01\n",
      "| epoch  49 |     7/    7 batches | lr 0.000372 | 63.92 ms | loss 0.03037 | ppl     1.03\n",
      "| epoch  50 |     7/    7 batches | lr 0.000364 | 64.25 ms | loss 0.01642 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time:  0.48s | valid loss 0.00228 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |     7/    7 batches | lr 0.000357 | 64.26 ms | loss 0.00951 | ppl     1.01\n",
      "| epoch  52 |     7/    7 batches | lr 0.000350 | 63.76 ms | loss 0.01325 | ppl     1.01\n",
      "| epoch  53 |     7/    7 batches | lr 0.000343 | 63.71 ms | loss 0.01273 | ppl     1.01\n",
      "| epoch  54 |     7/    7 batches | lr 0.000336 | 64.57 ms | loss 0.01446 | ppl     1.01\n",
      "| epoch  55 |     7/    7 batches | lr 0.000329 | 64.60 ms | loss 0.00766 | ppl     1.01\n",
      "| epoch  56 |     7/    7 batches | lr 0.000323 | 64.21 ms | loss 0.01157 | ppl     1.01\n",
      "| epoch  57 |     7/    7 batches | lr 0.000316 | 64.77 ms | loss 0.00836 | ppl     1.01\n",
      "| epoch  58 |     7/    7 batches | lr 0.000310 | 65.34 ms | loss 0.00446 | ppl     1.00\n",
      "| epoch  59 |     7/    7 batches | lr 0.000304 | 65.86 ms | loss 0.00673 | ppl     1.01\n",
      "| epoch  60 |     7/    7 batches | lr 0.000298 | 65.41 ms | loss 0.00537 | ppl     1.01\n",
      "| epoch  61 |     7/    7 batches | lr 0.000292 | 65.29 ms | loss 0.00497 | ppl     1.00\n",
      "| epoch  62 |     7/    7 batches | lr 0.000286 | 66.26 ms | loss 0.00392 | ppl     1.00\n",
      "| epoch  63 |     7/    7 batches | lr 0.000280 | 65.40 ms | loss 0.00509 | ppl     1.01\n",
      "| epoch  64 |     7/    7 batches | lr 0.000274 | 64.69 ms | loss 0.00717 | ppl     1.01\n",
      "| epoch  65 |     7/    7 batches | lr 0.000269 | 65.02 ms | loss 0.00805 | ppl     1.01\n",
      "| epoch  66 |     7/    7 batches | lr 0.000264 | 64.87 ms | loss 0.00671 | ppl     1.01\n",
      "| epoch  67 |     7/    7 batches | lr 0.000258 | 65.23 ms | loss 0.00755 | ppl     1.01\n",
      "| epoch  68 |     7/    7 batches | lr 0.000253 | 65.28 ms | loss 0.00916 | ppl     1.01\n",
      "| epoch  69 |     7/    7 batches | lr 0.000248 | 65.04 ms | loss 0.00826 | ppl     1.01\n",
      "| epoch  70 |     7/    7 batches | lr 0.000243 | 65.61 ms | loss 0.00720 | ppl     1.01\n",
      "| epoch  71 |     7/    7 batches | lr 0.000238 | 64.90 ms | loss 0.00992 | ppl     1.01\n",
      "| epoch  72 |     7/    7 batches | lr 0.000233 | 65.40 ms | loss 0.00888 | ppl     1.01\n",
      "| epoch  73 |     7/    7 batches | lr 0.000229 | 64.58 ms | loss 0.00510 | ppl     1.01\n",
      "| epoch  74 |     7/    7 batches | lr 0.000224 | 65.17 ms | loss 0.00810 | ppl     1.01\n",
      "| epoch  75 |     7/    7 batches | lr 0.000220 | 65.36 ms | loss 0.00785 | ppl     1.01\n",
      "| epoch  76 |     7/    7 batches | lr 0.000215 | 64.98 ms | loss 0.00686 | ppl     1.01\n",
      "| epoch  77 |     7/    7 batches | lr 0.000211 | 64.80 ms | loss 0.00957 | ppl     1.01\n",
      "| epoch  78 |     7/    7 batches | lr 0.000207 | 64.93 ms | loss 0.00632 | ppl     1.01\n",
      "| epoch  79 |     7/    7 batches | lr 0.000203 | 64.92 ms | loss 0.00662 | ppl     1.01\n",
      "| epoch  80 |     7/    7 batches | lr 0.000199 | 64.97 ms | loss 0.01011 | ppl     1.01\n",
      "| epoch  81 |     7/    7 batches | lr 0.000195 | 65.15 ms | loss 0.00525 | ppl     1.01\n",
      "| epoch  82 |     7/    7 batches | lr 0.000191 | 65.28 ms | loss 0.00642 | ppl     1.01\n",
      "| epoch  83 |     7/    7 batches | lr 0.000187 | 65.39 ms | loss 0.00825 | ppl     1.01\n",
      "| epoch  84 |     7/    7 batches | lr 0.000183 | 65.07 ms | loss 0.00455 | ppl     1.00\n",
      "| epoch  85 |     7/    7 batches | lr 0.000180 | 64.73 ms | loss 0.00519 | ppl     1.01\n",
      "| epoch  86 |     7/    7 batches | lr 0.000176 | 66.03 ms | loss 0.00686 | ppl     1.01\n",
      "| epoch  87 |     7/    7 batches | lr 0.000172 | 64.98 ms | loss 0.00435 | ppl     1.00\n",
      "| epoch  88 |     7/    7 batches | lr 0.000169 | 65.21 ms | loss 0.00419 | ppl     1.00\n",
      "| epoch  89 |     7/    7 batches | lr 0.000166 | 65.26 ms | loss 0.00515 | ppl     1.01\n",
      "| epoch  90 |     7/    7 batches | lr 0.000162 | 65.54 ms | loss 0.00545 | ppl     1.01\n",
      "| epoch  91 |     7/    7 batches | lr 0.000159 | 65.22 ms | loss 0.00690 | ppl     1.01\n",
      "| epoch  92 |     7/    7 batches | lr 0.000156 | 65.16 ms | loss 0.00377 | ppl     1.00\n",
      "| epoch  93 |     7/    7 batches | lr 0.000153 | 65.37 ms | loss 0.00521 | ppl     1.01\n",
      "| epoch  94 |     7/    7 batches | lr 0.000150 | 65.44 ms | loss 0.00552 | ppl     1.01\n",
      "| epoch  95 |     7/    7 batches | lr 0.000147 | 64.76 ms | loss 0.00368 | ppl     1.00\n",
      "| epoch  96 |     7/    7 batches | lr 0.000144 | 65.49 ms | loss 0.00481 | ppl     1.00\n",
      "| epoch  97 |     7/    7 batches | lr 0.000141 | 64.74 ms | loss 0.00410 | ppl     1.00\n",
      "| epoch  98 |     7/    7 batches | lr 0.000138 | 65.26 ms | loss 0.00376 | ppl     1.00\n",
      "| epoch  99 |     7/    7 batches | lr 0.000135 | 65.62 ms | loss 0.00491 | ppl     1.00\n",
      "| epoch 100 |     7/    7 batches | lr 0.000133 | 65.27 ms | loss 0.00365 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time:  1.17s | valid loss 0.00380 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 101 |     7/    7 batches | lr 0.000130 | 66.48 ms | loss 0.00355 | ppl     1.00\n",
      "| epoch 102 |     7/    7 batches | lr 0.000127 | 65.65 ms | loss 0.00412 | ppl     1.00\n",
      "| epoch 103 |     7/    7 batches | lr 0.000125 | 67.54 ms | loss 0.00366 | ppl     1.00\n",
      "| epoch 104 |     7/    7 batches | lr 0.000122 | 67.81 ms | loss 0.00398 | ppl     1.00\n",
      "| epoch 105 |     7/    7 batches | lr 0.000120 | 66.36 ms | loss 0.00363 | ppl     1.00\n",
      "| epoch 106 |     7/    7 batches | lr 0.000117 | 67.92 ms | loss 0.00403 | ppl     1.00\n",
      "| epoch 107 |     7/    7 batches | lr 0.000115 | 66.66 ms | loss 0.00440 | ppl     1.00\n",
      "| epoch 108 |     7/    7 batches | lr 0.000113 | 66.13 ms | loss 0.00309 | ppl     1.00\n",
      "| epoch 109 |     7/    7 batches | lr 0.000111 | 66.34 ms | loss 0.00358 | ppl     1.00\n",
      "| epoch 110 |     7/    7 batches | lr 0.000108 | 66.60 ms | loss 0.00373 | ppl     1.00\n",
      "| epoch 111 |     7/    7 batches | lr 0.000106 | 66.38 ms | loss 0.00313 | ppl     1.00\n",
      "| epoch 112 |     7/    7 batches | lr 0.000104 | 66.68 ms | loss 0.00364 | ppl     1.00\n",
      "| epoch 113 |     7/    7 batches | lr 0.000102 | 65.96 ms | loss 0.00338 | ppl     1.00\n",
      "| epoch 114 |     7/    7 batches | lr 0.000100 | 66.80 ms | loss 0.00295 | ppl     1.00\n",
      "| epoch 115 |     7/    7 batches | lr 0.000098 | 64.58 ms | loss 0.00349 | ppl     1.00\n",
      "| epoch 116 |     7/    7 batches | lr 0.000096 | 78.28 ms | loss 0.00309 | ppl     1.00\n",
      "| epoch 117 |     7/    7 batches | lr 0.000094 | 65.13 ms | loss 0.00288 | ppl     1.00\n",
      "| epoch 118 |     7/    7 batches | lr 0.000092 | 65.23 ms | loss 0.00334 | ppl     1.00\n",
      "| epoch 119 |     7/    7 batches | lr 0.000090 | 66.64 ms | loss 0.00296 | ppl     1.00\n",
      "| epoch 120 |     7/    7 batches | lr 0.000089 | 67.86 ms | loss 0.00283 | ppl     1.00\n",
      "| epoch 121 |     7/    7 batches | lr 0.000087 | 68.07 ms | loss 0.00320 | ppl     1.00\n",
      "| epoch 122 |     7/    7 batches | lr 0.000085 | 67.08 ms | loss 0.00284 | ppl     1.00\n",
      "| epoch 123 |     7/    7 batches | lr 0.000083 | 66.87 ms | loss 0.00274 | ppl     1.00\n",
      "| epoch 124 |     7/    7 batches | lr 0.000082 | 65.62 ms | loss 0.00295 | ppl     1.00\n",
      "| epoch 125 |     7/    7 batches | lr 0.000080 | 66.19 ms | loss 0.00272 | ppl     1.00\n",
      "| epoch 126 |     7/    7 batches | lr 0.000078 | 66.66 ms | loss 0.00267 | ppl     1.00\n",
      "| epoch 127 |     7/    7 batches | lr 0.000077 | 66.98 ms | loss 0.00278 | ppl     1.00\n",
      "| epoch 128 |     7/    7 batches | lr 0.000075 | 66.63 ms | loss 0.00266 | ppl     1.00\n",
      "| epoch 129 |     7/    7 batches | lr 0.000074 | 67.76 ms | loss 0.00264 | ppl     1.00\n",
      "| epoch 130 |     7/    7 batches | lr 0.000072 | 66.95 ms | loss 0.00272 | ppl     1.00\n",
      "| epoch 131 |     7/    7 batches | lr 0.000071 | 65.31 ms | loss 0.00262 | ppl     1.00\n",
      "| epoch 132 |     7/    7 batches | lr 0.000069 | 65.84 ms | loss 0.00257 | ppl     1.00\n",
      "| epoch 133 |     7/    7 batches | lr 0.000068 | 65.66 ms | loss 0.00264 | ppl     1.00\n",
      "| epoch 134 |     7/    7 batches | lr 0.000067 | 65.56 ms | loss 0.00256 | ppl     1.00\n",
      "| epoch 135 |     7/    7 batches | lr 0.000065 | 65.78 ms | loss 0.00248 | ppl     1.00\n",
      "| epoch 136 |     7/    7 batches | lr 0.000064 | 66.62 ms | loss 0.00257 | ppl     1.00\n",
      "| epoch 137 |     7/    7 batches | lr 0.000063 | 67.61 ms | loss 0.00249 | ppl     1.00\n",
      "| epoch 138 |     7/    7 batches | lr 0.000062 | 67.32 ms | loss 0.00242 | ppl     1.00\n",
      "| epoch 139 |     7/    7 batches | lr 0.000060 | 65.62 ms | loss 0.00248 | ppl     1.00\n",
      "| epoch 140 |     7/    7 batches | lr 0.000059 | 66.00 ms | loss 0.00244 | ppl     1.00\n",
      "| epoch 141 |     7/    7 batches | lr 0.000058 | 65.55 ms | loss 0.00237 | ppl     1.00\n",
      "| epoch 142 |     7/    7 batches | lr 0.000057 | 65.49 ms | loss 0.00239 | ppl     1.00\n",
      "| epoch 143 |     7/    7 batches | lr 0.000056 | 65.56 ms | loss 0.00236 | ppl     1.00\n",
      "| epoch 144 |     7/    7 batches | lr 0.000055 | 65.73 ms | loss 0.00232 | ppl     1.00\n",
      "| epoch 145 |     7/    7 batches | lr 0.000053 | 65.96 ms | loss 0.00233 | ppl     1.00\n",
      "| epoch 146 |     7/    7 batches | lr 0.000052 | 66.37 ms | loss 0.00230 | ppl     1.00\n",
      "| epoch 147 |     7/    7 batches | lr 0.000051 | 66.37 ms | loss 0.00230 | ppl     1.00\n",
      "| epoch 148 |     7/    7 batches | lr 0.000050 | 65.79 ms | loss 0.00229 | ppl     1.00\n",
      "| epoch 149 |     7/    7 batches | lr 0.000049 | 65.22 ms | loss 0.00228 | ppl     1.00\n",
      "| epoch 150 |     7/    7 batches | lr 0.000048 | 65.54 ms | loss 0.00227 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time:  0.49s | valid loss 0.00452 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 151 |     7/    7 batches | lr 0.000047 | 65.81 ms | loss 0.00227 | ppl     1.00\n",
      "| epoch 152 |     7/    7 batches | lr 0.000046 | 65.95 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch 153 |     7/    7 batches | lr 0.000045 | 65.09 ms | loss 0.00227 | ppl     1.00\n",
      "| epoch 154 |     7/    7 batches | lr 0.000045 | 66.92 ms | loss 0.00224 | ppl     1.00\n",
      "| epoch 155 |     7/    7 batches | lr 0.000044 | 65.75 ms | loss 0.00226 | ppl     1.00\n",
      "| epoch 156 |     7/    7 batches | lr 0.000043 | 65.70 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch 157 |     7/    7 batches | lr 0.000042 | 66.77 ms | loss 0.00224 | ppl     1.00\n",
      "| epoch 158 |     7/    7 batches | lr 0.000041 | 65.84 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch 159 |     7/    7 batches | lr 0.000040 | 65.54 ms | loss 0.00225 | ppl     1.00\n",
      "| epoch 160 |     7/    7 batches | lr 0.000039 | 65.97 ms | loss 0.00223 | ppl     1.00\n",
      "| epoch 161 |     7/    7 batches | lr 0.000039 | 65.22 ms | loss 0.00223 | ppl     1.00\n",
      "| epoch 162 |     7/    7 batches | lr 0.000038 | 65.77 ms | loss 0.00223 | ppl     1.00\n",
      "| epoch 163 |     7/    7 batches | lr 0.000037 | 67.28 ms | loss 0.00223 | ppl     1.00\n",
      "| epoch 164 |     7/    7 batches | lr 0.000036 | 67.28 ms | loss 0.00222 | ppl     1.00\n",
      "| epoch 165 |     7/    7 batches | lr 0.000036 | 68.40 ms | loss 0.00222 | ppl     1.00\n",
      "| epoch 166 |     7/    7 batches | lr 0.000035 | 67.03 ms | loss 0.00222 | ppl     1.00\n",
      "| epoch 167 |     7/    7 batches | lr 0.000034 | 67.18 ms | loss 0.00222 | ppl     1.00\n",
      "| epoch 168 |     7/    7 batches | lr 0.000034 | 65.80 ms | loss 0.00221 | ppl     1.00\n",
      "| epoch 169 |     7/    7 batches | lr 0.000033 | 65.42 ms | loss 0.00221 | ppl     1.00\n",
      "| epoch 170 |     7/    7 batches | lr 0.000032 | 66.24 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 171 |     7/    7 batches | lr 0.000032 | 66.19 ms | loss 0.00220 | ppl     1.00\n",
      "| epoch 172 |     7/    7 batches | lr 0.000031 | 67.41 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 173 |     7/    7 batches | lr 0.000030 | 65.47 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 174 |     7/    7 batches | lr 0.000030 | 65.97 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 175 |     7/    7 batches | lr 0.000029 | 67.52 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 176 |     7/    7 batches | lr 0.000029 | 66.98 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 177 |     7/    7 batches | lr 0.000028 | 67.39 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 178 |     7/    7 batches | lr 0.000027 | 66.42 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 179 |     7/    7 batches | lr 0.000027 | 65.72 ms | loss 0.00219 | ppl     1.00\n",
      "| epoch 180 |     7/    7 batches | lr 0.000026 | 65.87 ms | loss 0.00218 | ppl     1.00\n",
      "| epoch 181 |     7/    7 batches | lr 0.000026 | 65.93 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 182 |     7/    7 batches | lr 0.000025 | 65.57 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 183 |     7/    7 batches | lr 0.000025 | 65.70 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 184 |     7/    7 batches | lr 0.000024 | 66.17 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 185 |     7/    7 batches | lr 0.000024 | 65.87 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch 186 |     7/    7 batches | lr 0.000023 | 65.93 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 187 |     7/    7 batches | lr 0.000023 | 65.99 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 188 |     7/    7 batches | lr 0.000022 | 65.74 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch 189 |     7/    7 batches | lr 0.000022 | 65.66 ms | loss 0.00217 | ppl     1.00\n",
      "| epoch 190 |     7/    7 batches | lr 0.000022 | 66.18 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch 191 |     7/    7 batches | lr 0.000021 | 66.04 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch 192 |     7/    7 batches | lr 0.000021 | 65.63 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch 193 |     7/    7 batches | lr 0.000020 | 66.06 ms | loss 0.00215 | ppl     1.00\n",
      "| epoch 194 |     7/    7 batches | lr 0.000020 | 66.76 ms | loss 0.00215 | ppl     1.00\n",
      "| epoch 195 |     7/    7 batches | lr 0.000019 | 66.17 ms | loss 0.00215 | ppl     1.00\n",
      "| epoch 196 |     7/    7 batches | lr 0.000019 | 65.43 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 197 |     7/    7 batches | lr 0.000019 | 66.07 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 198 |     7/    7 batches | lr 0.000018 | 65.39 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 199 |     7/    7 batches | lr 0.000018 | 65.69 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 200 |     7/    7 batches | lr 0.000018 | 65.48 ms | loss 0.00215 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 200 | time:  1.12s | valid loss 0.00446 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 201 |     7/    7 batches | lr 0.000017 | 65.33 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 202 |     7/    7 batches | lr 0.000017 | 65.29 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 203 |     7/    7 batches | lr 0.000017 | 65.54 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 204 |     7/    7 batches | lr 0.000016 | 65.59 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 205 |     7/    7 batches | lr 0.000016 | 65.13 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 206 |     7/    7 batches | lr 0.000016 | 65.77 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 207 |     7/    7 batches | lr 0.000015 | 66.38 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 208 |     7/    7 batches | lr 0.000015 | 65.96 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 209 |     7/    7 batches | lr 0.000015 | 65.70 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 210 |     7/    7 batches | lr 0.000014 | 65.99 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 211 |     7/    7 batches | lr 0.000014 | 66.29 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 212 |     7/    7 batches | lr 0.000014 | 66.24 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch 213 |     7/    7 batches | lr 0.000014 | 66.30 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 214 |     7/    7 batches | lr 0.000013 | 66.33 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 215 |     7/    7 batches | lr 0.000013 | 66.13 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 216 |     7/    7 batches | lr 0.000013 | 66.09 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 217 |     7/    7 batches | lr 0.000012 | 65.86 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 218 |     7/    7 batches | lr 0.000012 | 65.61 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 219 |     7/    7 batches | lr 0.000012 | 65.89 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 220 |     7/    7 batches | lr 0.000012 | 66.29 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 221 |     7/    7 batches | lr 0.000012 | 65.95 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 222 |     7/    7 batches | lr 0.000011 | 65.82 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 223 |     7/    7 batches | lr 0.000011 | 66.16 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 224 |     7/    7 batches | lr 0.000011 | 66.06 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 225 |     7/    7 batches | lr 0.000011 | 66.02 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 226 |     7/    7 batches | lr 0.000010 | 65.41 ms | loss 0.00212 | ppl     1.00\n",
      "| epoch 227 |     7/    7 batches | lr 0.000010 | 65.81 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 228 |     7/    7 batches | lr 0.000010 | 65.80 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 229 |     7/    7 batches | lr 0.000010 | 65.97 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 230 |     7/    7 batches | lr 0.000010 | 66.40 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 231 |     7/    7 batches | lr 0.000009 | 66.30 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 232 |     7/    7 batches | lr 0.000009 | 66.34 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 233 |     7/    7 batches | lr 0.000009 | 66.40 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 234 |     7/    7 batches | lr 0.000009 | 66.24 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 235 |     7/    7 batches | lr 0.000009 | 66.12 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 236 |     7/    7 batches | lr 0.000008 | 66.46 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 237 |     7/    7 batches | lr 0.000008 | 66.33 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 238 |     7/    7 batches | lr 0.000008 | 66.35 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 239 |     7/    7 batches | lr 0.000008 | 66.26 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 240 |     7/    7 batches | lr 0.000008 | 66.38 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch 241 |     7/    7 batches | lr 0.000008 | 66.20 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 242 |     7/    7 batches | lr 0.000008 | 66.52 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 243 |     7/    7 batches | lr 0.000007 | 66.39 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 244 |     7/    7 batches | lr 0.000007 | 66.35 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 245 |     7/    7 batches | lr 0.000007 | 66.23 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 246 |     7/    7 batches | lr 0.000007 | 66.41 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 247 |     7/    7 batches | lr 0.000007 | 66.22 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 248 |     7/    7 batches | lr 0.000007 | 66.34 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 249 |     7/    7 batches | lr 0.000007 | 79.78 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 250 |     7/    7 batches | lr 0.000006 | 67.93 ms | loss 0.00209 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 250 | time:  0.51s | valid loss 0.00440 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 251 |     7/    7 batches | lr 0.000006 | 69.46 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 252 |     7/    7 batches | lr 0.000006 | 66.92 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 253 |     7/    7 batches | lr 0.000006 | 69.04 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 254 |     7/    7 batches | lr 0.000006 | 68.56 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 255 |     7/    7 batches | lr 0.000006 | 66.82 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 256 |     7/    7 batches | lr 0.000006 | 68.89 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 257 |     7/    7 batches | lr 0.000006 | 65.68 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 258 |     7/    7 batches | lr 0.000005 | 65.16 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 259 |     7/    7 batches | lr 0.000005 | 65.55 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 260 |     7/    7 batches | lr 0.000005 | 66.29 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 261 |     7/    7 batches | lr 0.000005 | 66.36 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 262 |     7/    7 batches | lr 0.000005 | 66.32 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 263 |     7/    7 batches | lr 0.000005 | 66.15 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 264 |     7/    7 batches | lr 0.000005 | 66.27 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch 265 |     7/    7 batches | lr 0.000005 | 66.26 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 266 |     7/    7 batches | lr 0.000005 | 66.10 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 267 |     7/    7 batches | lr 0.000005 | 66.15 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 268 |     7/    7 batches | lr 0.000004 | 66.33 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 269 |     7/    7 batches | lr 0.000004 | 66.09 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 270 |     7/    7 batches | lr 0.000004 | 66.24 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 271 |     7/    7 batches | lr 0.000004 | 66.31 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 272 |     7/    7 batches | lr 0.000004 | 66.22 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 273 |     7/    7 batches | lr 0.000004 | 66.32 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 274 |     7/    7 batches | lr 0.000004 | 66.21 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 275 |     7/    7 batches | lr 0.000004 | 66.42 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 276 |     7/    7 batches | lr 0.000004 | 66.43 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 277 |     7/    7 batches | lr 0.000004 | 66.31 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 278 |     7/    7 batches | lr 0.000004 | 66.43 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 279 |     7/    7 batches | lr 0.000004 | 66.25 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 280 |     7/    7 batches | lr 0.000003 | 66.32 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 281 |     7/    7 batches | lr 0.000003 | 66.41 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 282 |     7/    7 batches | lr 0.000003 | 66.32 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 283 |     7/    7 batches | lr 0.000003 | 66.48 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 284 |     7/    7 batches | lr 0.000003 | 66.12 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 285 |     7/    7 batches | lr 0.000003 | 66.38 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 286 |     7/    7 batches | lr 0.000003 | 66.30 ms | loss 0.00208 | ppl     1.00\n",
      "| epoch 287 |     7/    7 batches | lr 0.000003 | 66.21 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 288 |     7/    7 batches | lr 0.000003 | 66.09 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 289 |     7/    7 batches | lr 0.000003 | 66.35 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 290 |     7/    7 batches | lr 0.000003 | 66.24 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 291 |     7/    7 batches | lr 0.000003 | 66.18 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 292 |     7/    7 batches | lr 0.000003 | 66.20 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 293 |     7/    7 batches | lr 0.000003 | 66.20 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 294 |     7/    7 batches | lr 0.000003 | 66.24 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 295 |     7/    7 batches | lr 0.000003 | 66.27 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 296 |     7/    7 batches | lr 0.000003 | 66.26 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 297 |     7/    7 batches | lr 0.000002 | 66.22 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 298 |     7/    7 batches | lr 0.000002 | 66.50 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 299 |     7/    7 batches | lr 0.000002 | 65.98 ms | loss 0.00207 | ppl     1.00\n",
      "| epoch 300 |     7/    7 batches | lr 0.000002 | 66.30 ms | loss 0.00206 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 300 | time:  1.18s | valid loss 0.00438 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools\\anaconda3\\envs\\data\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |     7/    7 batches | lr 0.001000 | 66.58 ms | loss 27.30717 | ppl 723355611405.55\n",
      "| epoch   2 |     7/    7 batches | lr 0.000960 | 66.29 ms | loss 0.45337 | ppl     1.57\n",
      "| epoch   3 |     7/    7 batches | lr 0.000941 | 67.32 ms | loss 0.78457 | ppl     2.19\n",
      "| epoch   4 |     7/    7 batches | lr 0.000922 | 66.66 ms | loss 0.28005 | ppl     1.32\n",
      "| epoch   5 |     7/    7 batches | lr 0.000904 | 66.68 ms | loss 0.23596 | ppl     1.27\n",
      "| epoch   6 |     7/    7 batches | lr 0.000886 | 67.05 ms | loss 0.04309 | ppl     1.04\n",
      "| epoch   7 |     7/    7 batches | lr 0.000868 | 67.56 ms | loss 0.11042 | ppl     1.12\n",
      "| epoch   8 |     7/    7 batches | lr 0.000851 | 66.84 ms | loss 0.16548 | ppl     1.18\n",
      "| epoch   9 |     7/    7 batches | lr 0.000834 | 66.39 ms | loss 0.25196 | ppl     1.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m----> 3\u001b[0m     predict,truth \u001b[39m=\u001b[39m train_and_get(i)\n\u001b[0;32m      4\u001b[0m     result\u001b[39m.\u001b[39mupdate({i:predict})\n\u001b[0;32m      5\u001b[0m     result\u001b[39m.\u001b[39mupdate({i\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_truth\u001b[39m\u001b[39m'\u001b[39m:truth})\n",
      "Cell \u001b[1;32mIn [50], line 32\u001b[0m, in \u001b[0;36mtrain_and_get\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     30\u001b[0m     predict_future(model, val_data, \u001b[39m5\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model, val_data)\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m(epoch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m89\u001b[39m)\n",
      "Cell \u001b[1;32mIn [40], line 232\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(eval_model, data_source)\u001b[0m\n\u001b[0;32m    230\u001b[0m         data, targets \u001b[39m=\u001b[39m get_batch(data_source, i, eval_batch_size)\n\u001b[0;32m    231\u001b[0m         output \u001b[39m=\u001b[39m eval_model(data)\n\u001b[1;32m--> 232\u001b[0m         total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data[\u001b[39m0\u001b[39m]) \u001b[39m*\u001b[39m criterion(output,\n\u001b[0;32m    233\u001b[0m                                                targets)\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(data_source)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = dict()\n",
    "for i in ['open','close','high','low']:\n",
    "    predict,truth = train_and_get(i)\n",
    "    result.update({i:predict})\n",
    "    result.update({i+'_truth':truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result1.keys():\n",
    "    result1[i] = result1[i].reshape(-1)\n",
    "output = pd.DataFrame(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00d1462882258b783cb181eb86adde5698c4f2204c9695f847918151df6a5450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
