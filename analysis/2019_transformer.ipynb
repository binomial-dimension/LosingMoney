{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from EarlyStopping import EarlyStopping\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E)\n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "\n",
    "input_window = 150  # number of input steps\n",
    "output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "batch_size = 512\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=6000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=400, num_layers=1, dropout=0.15):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size,\n",
    "                                                        nhead=10,\n",
    "                                                        dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer,\n",
    "                                                         num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(feature_size, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,\n",
    "                                          self.src_mask)  #, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(\n",
    "            mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - tw):\n",
    "        train_seq = input_data[i:i + tw]\n",
    "        train_label = input_data[i + output_window:i + tw + output_window]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "\n",
    "def get_data(name):\n",
    "    # construct a littel toy dataset\n",
    "    time = np.arange(0, 400, 0.1)\n",
    "    data = pd.read_csv('../data/hs300.csv')\n",
    "    amplitude = data[name].values\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    #loading weather data from a file\n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "    # looks like normalizing input values curtial for the model\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    sampels = int(len(amplitude)*0.9)\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment..\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:\n",
    "                                    -output_window]  #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1)\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]  #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device),scaler\n",
    "\n",
    "\n",
    "def get_batch(source, i, batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    input = torch.stack(\n",
    "        torch.stack([item[0] for item in data]).chunk(input_window,1))  # 1 is feature size\n",
    "    target = torch.stack(\n",
    "        torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data,epoch,train_losses):\n",
    "    model.train()  # Turn on the train mode \\o/\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = torch.sqrt(criterion(output, targets))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "def plot_and_loss(eval_model, data_source, epoch):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            total_loss += torch.sqrt(criterion(output, target)).item()\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                    0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "    #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color=\"red\", label=\"prediction\")\n",
    "    pyplot.plot(truth[:len(test_result)], color=\"blue\", label=\"truth\")\n",
    "    pyplot.plot(test_result - truth, color=\"green\", label=\"error\")\n",
    "    pyplot.legend()\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "# predict the next n steps based on the input data\n",
    "def predict_future(eval_model, data_source, steps):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source, 0, 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):\n",
    "            output = eval_model(data[-input_window:])\n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "    data = data.cpu().view(-1)\n",
    "\n",
    "    # I used this plot to visualize if the model pics up any long therm structure within the data.\n",
    "    pyplot.plot(data, color=\"red\")\n",
    "    pyplot.plot(data[:input_window], color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval()  # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 500\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "            total_loss += len(data[0]) * torch.sqrt(criterion(output, targets)).cpu().item()\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get(name):\n",
    "    input_window = 120  # number of input steps\n",
    "    output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "    batch_size = 1024\n",
    "    device = torch.device(\"cuda\")\n",
    "    train_data, val_data, scaler = get_data(name)\n",
    "    global model \n",
    "    model = TransAm().to(device)\n",
    "\n",
    "    global criterion\n",
    "    criterion = nn.MSELoss()\n",
    "    lr = 0.00005  # learning rate\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    global optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    global scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    global epochs\n",
    "    epochs = 300  # The number of epochs\n",
    "    best_model = None\n",
    "\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_data,epoch,train_losses)\n",
    "\n",
    "        if (epoch % 100 == 0):\n",
    "            val_loss = plot_and_loss(model, val_data, epoch)\n",
    "            predict_future(model, val_data, 5)\n",
    "        else:\n",
    "            val_loss = evaluate(model, val_data)\n",
    "\n",
    "        valid_losses.append(val_loss)\n",
    "\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    def get_predict(eval_model, data_source):\n",
    "        eval_model.eval()\n",
    "        total_loss = 0.\n",
    "        test_result = torch.Tensor(0)\n",
    "        truth = torch.Tensor(0)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data_source) - 1):\n",
    "                data, target = get_batch(data_source, i, 1)\n",
    "                output = eval_model(data)\n",
    "                total_loss += torch.sqrt(criterion(output, target)).item()\n",
    "                test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                        0)\n",
    "                truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "        #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "        return scaler.inverse_transform(test_result.view(-1, 1))[120:], scaler.inverse_transform(truth.view(-1, 1))[120:]\n",
    "\n",
    "    \n",
    "    return get_predict(model, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/300] train_loss: 0.87266 valid_loss: 0.70983\n",
      "Validation loss decreased (inf --> 0.709830).  Saving model ...\n",
      "[  2/300] train_loss: 0.49264 valid_loss: 0.28439\n",
      "Validation loss decreased (0.709830 --> 0.284389).  Saving model ...\n",
      "[  3/300] train_loss: 0.55962 valid_loss: 0.33729\n",
      "[  4/300] train_loss: 0.31418 valid_loss: 0.12731\n",
      "Validation loss decreased (0.284389 --> 0.127306).  Saving model ...\n",
      "[  5/300] train_loss: 0.28900 valid_loss: 0.12510\n",
      "Validation loss decreased (0.127306 --> 0.125100).  Saving model ...\n",
      "[  6/300] train_loss: 0.25649 valid_loss: 0.16195\n",
      "[  7/300] train_loss: 0.25362 valid_loss: 0.11403\n",
      "Validation loss decreased (0.125100 --> 0.114028).  Saving model ...\n",
      "[  8/300] train_loss: 0.28755 valid_loss: 0.13488\n",
      "[  9/300] train_loss: 0.25562 valid_loss: 0.13522\n",
      "[ 10/300] train_loss: 0.27212 valid_loss: 0.09297\n",
      "Validation loss decreased (0.114028 --> 0.092972).  Saving model ...\n",
      "[ 11/300] train_loss: 0.20132 valid_loss: 0.07633\n",
      "Validation loss decreased (0.092972 --> 0.076326).  Saving model ...\n",
      "[ 12/300] train_loss: 0.19871 valid_loss: 0.18973\n",
      "[ 13/300] train_loss: 0.21976 valid_loss: 0.12668\n",
      "[ 14/300] train_loss: 0.17787 valid_loss: 0.11397\n",
      "[ 15/300] train_loss: 0.21754 valid_loss: 0.17608\n",
      "[ 16/300] train_loss: 0.18172 valid_loss: 0.11504\n",
      "[ 17/300] train_loss: 0.16763 valid_loss: 0.09341\n",
      "[ 18/300] train_loss: 0.15556 valid_loss: 0.11266\n",
      "[ 19/300] train_loss: 0.15930 valid_loss: 0.10334\n",
      "[ 20/300] train_loss: 0.15951 valid_loss: 0.05409\n",
      "Validation loss decreased (0.076326 --> 0.054091).  Saving model ...\n",
      "[ 21/300] train_loss: 0.15429 valid_loss: 0.05864\n",
      "[ 22/300] train_loss: 0.14995 valid_loss: 0.08748\n",
      "[ 23/300] train_loss: 0.14209 valid_loss: 0.08877\n",
      "[ 24/300] train_loss: 0.16903 valid_loss: 0.06883\n",
      "[ 25/300] train_loss: 0.15039 valid_loss: 0.07423\n",
      "[ 26/300] train_loss: 0.14662 valid_loss: 0.06642\n",
      "[ 27/300] train_loss: 0.12439 valid_loss: 0.05520\n",
      "[ 28/300] train_loss: 0.12683 valid_loss: 0.04706\n",
      "Validation loss decreased (0.054091 --> 0.047061).  Saving model ...\n",
      "[ 29/300] train_loss: 0.11947 valid_loss: 0.04894\n",
      "[ 30/300] train_loss: 0.12276 valid_loss: 0.04427\n",
      "Validation loss decreased (0.047061 --> 0.044266).  Saving model ...\n",
      "[ 31/300] train_loss: 0.11607 valid_loss: 0.04666\n",
      "[ 32/300] train_loss: 0.11957 valid_loss: 0.04288\n",
      "Validation loss decreased (0.044266 --> 0.042885).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11185 valid_loss: 0.04835\n",
      "[ 34/300] train_loss: 0.11342 valid_loss: 0.04737\n",
      "[ 35/300] train_loss: 0.11131 valid_loss: 0.04185\n",
      "Validation loss decreased (0.042885 --> 0.041851).  Saving model ...\n",
      "[ 36/300] train_loss: 0.11014 valid_loss: 0.04698\n",
      "[ 37/300] train_loss: 0.11206 valid_loss: 0.05756\n",
      "[ 38/300] train_loss: 0.11037 valid_loss: 0.05324\n",
      "[ 39/300] train_loss: 0.14332 valid_loss: 0.06429\n",
      "[ 40/300] train_loss: 0.10104 valid_loss: 0.04284\n",
      "[ 41/300] train_loss: 0.10077 valid_loss: 0.07024\n",
      "[ 42/300] train_loss: 0.10137 valid_loss: 0.05873\n",
      "[ 43/300] train_loss: 0.09934 valid_loss: 0.04476\n",
      "[ 44/300] train_loss: 0.09670 valid_loss: 0.03764\n",
      "Validation loss decreased (0.041851 --> 0.037638).  Saving model ...\n",
      "[ 45/300] train_loss: 0.09641 valid_loss: 0.04801\n",
      "[ 46/300] train_loss: 0.09674 valid_loss: 0.05841\n",
      "[ 47/300] train_loss: 0.09500 valid_loss: 0.04964\n",
      "[ 48/300] train_loss: 0.11762 valid_loss: 0.07142\n",
      "[ 49/300] train_loss: 0.10509 valid_loss: 0.05714\n",
      "[ 50/300] train_loss: 0.09150 valid_loss: 0.04537\n",
      "[ 51/300] train_loss: 0.09328 valid_loss: 0.04114\n",
      "[ 52/300] train_loss: 0.09202 valid_loss: 0.03961\n",
      "[ 53/300] train_loss: 0.08773 valid_loss: 0.03908\n",
      "[ 54/300] train_loss: 0.08912 valid_loss: 0.03610\n",
      "Validation loss decreased (0.037638 --> 0.036097).  Saving model ...\n",
      "[ 55/300] train_loss: 0.08896 valid_loss: 0.04735\n",
      "[ 56/300] train_loss: 0.08686 valid_loss: 0.05095\n",
      "[ 57/300] train_loss: 0.09005 valid_loss: 0.05404\n",
      "[ 58/300] train_loss: 0.09002 valid_loss: 0.04847\n",
      "[ 59/300] train_loss: 0.09739 valid_loss: 0.04417\n",
      "[ 60/300] train_loss: 0.08885 valid_loss: 0.03880\n",
      "[ 61/300] train_loss: 0.09307 valid_loss: 0.04234\n",
      "[ 62/300] train_loss: 0.09502 valid_loss: 0.04160\n",
      "[ 63/300] train_loss: 0.08811 valid_loss: 0.04459\n",
      "[ 64/300] train_loss: 0.11629 valid_loss: 0.08076\n",
      "[ 65/300] train_loss: 0.09457 valid_loss: 0.08103\n",
      "[ 66/300] train_loss: 0.09407 valid_loss: 0.04804\n",
      "[ 67/300] train_loss: 0.08708 valid_loss: 0.03516\n",
      "Validation loss decreased (0.036097 --> 0.035162).  Saving model ...\n",
      "[ 68/300] train_loss: 0.08356 valid_loss: 0.04857\n",
      "[ 69/300] train_loss: 0.08743 valid_loss: 0.04274\n",
      "[ 70/300] train_loss: 0.09374 valid_loss: 0.05836\n",
      "[ 71/300] train_loss: 0.08200 valid_loss: 0.03478\n",
      "Validation loss decreased (0.035162 --> 0.034781).  Saving model ...\n",
      "[ 72/300] train_loss: 0.08266 valid_loss: 0.03877\n",
      "[ 73/300] train_loss: 0.08144 valid_loss: 0.03596\n",
      "[ 74/300] train_loss: 0.08117 valid_loss: 0.04418\n",
      "[ 75/300] train_loss: 0.07867 valid_loss: 0.03558\n",
      "[ 76/300] train_loss: 0.08364 valid_loss: 0.03295\n",
      "Validation loss decreased (0.034781 --> 0.032952).  Saving model ...\n",
      "[ 77/300] train_loss: 0.07982 valid_loss: 0.03248\n",
      "Validation loss decreased (0.032952 --> 0.032477).  Saving model ...\n",
      "[ 78/300] train_loss: 0.08317 valid_loss: 0.04489\n",
      "[ 79/300] train_loss: 0.08282 valid_loss: 0.03249\n",
      "[ 80/300] train_loss: 0.07796 valid_loss: 0.03328\n",
      "[ 81/300] train_loss: 0.08634 valid_loss: 0.04283\n",
      "[ 82/300] train_loss: 0.07586 valid_loss: 0.03540\n",
      "[ 83/300] train_loss: 0.07739 valid_loss: 0.04127\n",
      "[ 84/300] train_loss: 0.07565 valid_loss: 0.03329\n",
      "[ 85/300] train_loss: 0.07565 valid_loss: 0.03594\n",
      "[ 86/300] train_loss: 0.07683 valid_loss: 0.04150\n",
      "[ 87/300] train_loss: 0.07489 valid_loss: 0.03438\n",
      "[ 88/300] train_loss: 0.07507 valid_loss: 0.03459\n",
      "[ 89/300] train_loss: 0.07451 valid_loss: 0.04353\n",
      "[ 90/300] train_loss: 0.07680 valid_loss: 0.03732\n",
      "[ 91/300] train_loss: 0.08219 valid_loss: 0.04562\n",
      "[ 92/300] train_loss: 0.07377 valid_loss: 0.03443\n",
      "[ 93/300] train_loss: 0.07746 valid_loss: 0.03588\n",
      "[ 94/300] train_loss: 0.07418 valid_loss: 0.03571\n",
      "[ 95/300] train_loss: 0.07376 valid_loss: 0.03942\n",
      "[ 96/300] train_loss: 0.07302 valid_loss: 0.03243\n",
      "Validation loss decreased (0.032477 --> 0.032433).  Saving model ...\n",
      "[ 97/300] train_loss: 0.07288 valid_loss: 0.03307\n",
      "[ 98/300] train_loss: 0.07320 valid_loss: 0.03804\n",
      "[ 99/300] train_loss: 0.07190 valid_loss: 0.03382\n",
      "[100/300] train_loss: 0.07148 valid_loss: 0.03306\n",
      "[101/300] train_loss: 0.07165 valid_loss: 0.03311\n",
      "[102/300] train_loss: 0.07135 valid_loss: 0.03512\n",
      "[103/300] train_loss: 0.07099 valid_loss: 0.03238\n",
      "Validation loss decreased (0.032433 --> 0.032380).  Saving model ...\n",
      "[104/300] train_loss: 0.07102 valid_loss: 0.03714\n",
      "[105/300] train_loss: 0.07080 valid_loss: 0.03238\n",
      "[106/300] train_loss: 0.07082 valid_loss: 0.03167\n",
      "Validation loss decreased (0.032380 --> 0.031666).  Saving model ...\n",
      "[107/300] train_loss: 0.07095 valid_loss: 0.03705\n",
      "[108/300] train_loss: 0.07079 valid_loss: 0.03302\n",
      "[109/300] train_loss: 0.07087 valid_loss: 0.03644\n",
      "[110/300] train_loss: 0.07573 valid_loss: 0.04379\n",
      "[111/300] train_loss: 0.07043 valid_loss: 0.04000\n",
      "[112/300] train_loss: 0.07230 valid_loss: 0.03955\n",
      "[113/300] train_loss: 0.07385 valid_loss: 0.03371\n",
      "[114/300] train_loss: 0.07111 valid_loss: 0.03660\n",
      "[115/300] train_loss: 0.06935 valid_loss: 0.03188\n",
      "[116/300] train_loss: 0.07189 valid_loss: 0.03647\n",
      "[117/300] train_loss: 0.06899 valid_loss: 0.03083\n",
      "Validation loss decreased (0.031666 --> 0.030826).  Saving model ...\n",
      "[118/300] train_loss: 0.06943 valid_loss: 0.03386\n",
      "[119/300] train_loss: 0.06878 valid_loss: 0.03381\n",
      "[120/300] train_loss: 0.06944 valid_loss: 0.03166\n",
      "[121/300] train_loss: 0.07196 valid_loss: 0.03957\n",
      "[122/300] train_loss: 0.06821 valid_loss: 0.03141\n",
      "[123/300] train_loss: 0.06875 valid_loss: 0.03541\n",
      "[124/300] train_loss: 0.06806 valid_loss: 0.03054\n",
      "Validation loss decreased (0.030826 --> 0.030543).  Saving model ...\n",
      "[125/300] train_loss: 0.06817 valid_loss: 0.03313\n",
      "[126/300] train_loss: 0.06860 valid_loss: 0.03091\n",
      "[127/300] train_loss: 0.06770 valid_loss: 0.03139\n",
      "[128/300] train_loss: 0.06835 valid_loss: 0.03886\n",
      "[129/300] train_loss: 0.06756 valid_loss: 0.03137\n",
      "[130/300] train_loss: 0.06742 valid_loss: 0.03287\n",
      "[131/300] train_loss: 0.06725 valid_loss: 0.03353\n",
      "[132/300] train_loss: 0.06743 valid_loss: 0.03150\n",
      "[133/300] train_loss: 0.06717 valid_loss: 0.03206\n",
      "[134/300] train_loss: 0.06772 valid_loss: 0.03759\n",
      "[135/300] train_loss: 0.06649 valid_loss: 0.03163\n",
      "[136/300] train_loss: 0.06655 valid_loss: 0.03157\n",
      "[137/300] train_loss: 0.06701 valid_loss: 0.03647\n",
      "[138/300] train_loss: 0.06666 valid_loss: 0.03096\n",
      "[139/300] train_loss: 0.06637 valid_loss: 0.03269\n",
      "[140/300] train_loss: 0.06612 valid_loss: 0.03254\n",
      "[141/300] train_loss: 0.06628 valid_loss: 0.03097\n",
      "[142/300] train_loss: 0.06654 valid_loss: 0.03382\n",
      "[143/300] train_loss: 0.06593 valid_loss: 0.03171\n",
      "[144/300] train_loss: 0.06589 valid_loss: 0.03312\n",
      "[145/300] train_loss: 0.06569 valid_loss: 0.03173\n",
      "[146/300] train_loss: 0.06583 valid_loss: 0.03364\n",
      "[147/300] train_loss: 0.06578 valid_loss: 0.03174\n",
      "[148/300] train_loss: 0.06555 valid_loss: 0.03270\n",
      "[149/300] train_loss: 0.06577 valid_loss: 0.03638\n",
      "[150/300] train_loss: 0.06547 valid_loss: 0.03144\n",
      "[151/300] train_loss: 0.06531 valid_loss: 0.03351\n",
      "[152/300] train_loss: 0.06505 valid_loss: 0.03212\n",
      "[153/300] train_loss: 0.06495 valid_loss: 0.03204\n",
      "[154/300] train_loss: 0.06484 valid_loss: 0.03234\n",
      "[155/300] train_loss: 0.06474 valid_loss: 0.03231\n",
      "[156/300] train_loss: 0.06472 valid_loss: 0.03213\n",
      "[157/300] train_loss: 0.06466 valid_loss: 0.03234\n",
      "[158/300] train_loss: 0.06470 valid_loss: 0.03225\n",
      "[159/300] train_loss: 0.06454 valid_loss: 0.03267\n",
      "[160/300] train_loss: 0.06439 valid_loss: 0.03250\n",
      "[161/300] train_loss: 0.06449 valid_loss: 0.03256\n",
      "[162/300] train_loss: 0.06438 valid_loss: 0.03272\n",
      "[163/300] train_loss: 0.06420 valid_loss: 0.03293\n",
      "[164/300] train_loss: 0.06419 valid_loss: 0.03248\n",
      "[165/300] train_loss: 0.06398 valid_loss: 0.03244\n",
      "[166/300] train_loss: 0.06419 valid_loss: 0.03248\n",
      "[167/300] train_loss: 0.06400 valid_loss: 0.03252\n",
      "[168/300] train_loss: 0.06390 valid_loss: 0.03262\n",
      "[169/300] train_loss: 0.06389 valid_loss: 0.03282\n",
      "[170/300] train_loss: 0.06390 valid_loss: 0.03265\n",
      "[171/300] train_loss: 0.06375 valid_loss: 0.03295\n",
      "[172/300] train_loss: 0.06371 valid_loss: 0.03252\n",
      "[173/300] train_loss: 0.06362 valid_loss: 0.03320\n",
      "[174/300] train_loss: 0.06362 valid_loss: 0.03257\n",
      "Early stopping\n",
      "[  1/300] train_loss: 0.77531 valid_loss: 0.39185\n",
      "Validation loss decreased (inf --> 0.391853).  Saving model ...\n",
      "[  2/300] train_loss: 0.58815 valid_loss: 0.29747\n",
      "Validation loss decreased (0.391853 --> 0.297470).  Saving model ...\n",
      "[  3/300] train_loss: 0.37124 valid_loss: 0.19319\n",
      "Validation loss decreased (0.297470 --> 0.193187).  Saving model ...\n",
      "[  4/300] train_loss: 0.32925 valid_loss: 0.30857\n",
      "[  5/300] train_loss: 0.44826 valid_loss: 0.16139\n",
      "Validation loss decreased (0.193187 --> 0.161388).  Saving model ...\n",
      "[  6/300] train_loss: 0.30618 valid_loss: 0.11244\n",
      "Validation loss decreased (0.161388 --> 0.112437).  Saving model ...\n",
      "[  7/300] train_loss: 0.25753 valid_loss: 0.11711\n",
      "[  8/300] train_loss: 0.24672 valid_loss: 0.09880\n",
      "Validation loss decreased (0.112437 --> 0.098796).  Saving model ...\n",
      "[  9/300] train_loss: 0.24786 valid_loss: 0.12119\n",
      "[ 10/300] train_loss: 0.27681 valid_loss: 0.11213\n",
      "[ 11/300] train_loss: 0.21735 valid_loss: 0.14543\n",
      "[ 12/300] train_loss: 0.20503 valid_loss: 0.06475\n",
      "Validation loss decreased (0.098796 --> 0.064751).  Saving model ...\n",
      "[ 13/300] train_loss: 0.24061 valid_loss: 0.12473\n",
      "[ 14/300] train_loss: 0.18645 valid_loss: 0.09662\n",
      "[ 15/300] train_loss: 0.18693 valid_loss: 0.07978\n",
      "[ 16/300] train_loss: 0.17257 valid_loss: 0.09924\n",
      "[ 17/300] train_loss: 0.17134 valid_loss: 0.09323\n",
      "[ 18/300] train_loss: 0.17262 valid_loss: 0.13608\n",
      "[ 19/300] train_loss: 0.17649 valid_loss: 0.06578\n",
      "[ 20/300] train_loss: 0.14884 valid_loss: 0.06795\n",
      "[ 21/300] train_loss: 0.15559 valid_loss: 0.05276\n",
      "Validation loss decreased (0.064751 --> 0.052764).  Saving model ...\n",
      "[ 22/300] train_loss: 0.14312 valid_loss: 0.05983\n",
      "[ 23/300] train_loss: 0.15779 valid_loss: 0.10995\n",
      "[ 24/300] train_loss: 0.15219 valid_loss: 0.06471\n",
      "[ 25/300] train_loss: 0.14746 valid_loss: 0.06387\n",
      "[ 26/300] train_loss: 0.14057 valid_loss: 0.07659\n",
      "[ 27/300] train_loss: 0.14201 valid_loss: 0.05819\n",
      "[ 28/300] train_loss: 0.13701 valid_loss: 0.06154\n",
      "[ 29/300] train_loss: 0.12806 valid_loss: 0.07737\n",
      "[ 30/300] train_loss: 0.12867 valid_loss: 0.06494\n",
      "[ 31/300] train_loss: 0.13343 valid_loss: 0.13475\n",
      "[ 32/300] train_loss: 0.13118 valid_loss: 0.05199\n",
      "Validation loss decreased (0.052764 --> 0.051989).  Saving model ...\n",
      "[ 33/300] train_loss: 0.11608 valid_loss: 0.05230\n",
      "[ 34/300] train_loss: 0.11815 valid_loss: 0.04394\n",
      "Validation loss decreased (0.051989 --> 0.043939).  Saving model ...\n",
      "[ 35/300] train_loss: 0.11248 valid_loss: 0.04663\n",
      "[ 36/300] train_loss: 0.11719 valid_loss: 0.04614\n",
      "[ 37/300] train_loss: 0.10970 valid_loss: 0.05765\n",
      "[ 38/300] train_loss: 0.11245 valid_loss: 0.06211\n",
      "[ 39/300] train_loss: 0.10826 valid_loss: 0.06097\n",
      "[ 40/300] train_loss: 0.11123 valid_loss: 0.04332\n",
      "Validation loss decreased (0.043939 --> 0.043321).  Saving model ...\n",
      "[ 41/300] train_loss: 0.10642 valid_loss: 0.04349\n",
      "[ 42/300] train_loss: 0.10534 valid_loss: 0.04236\n",
      "Validation loss decreased (0.043321 --> 0.042361).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10064 valid_loss: 0.04318\n",
      "[ 44/300] train_loss: 0.10047 valid_loss: 0.04145\n",
      "Validation loss decreased (0.042361 --> 0.041449).  Saving model ...\n",
      "[ 45/300] train_loss: 0.09795 valid_loss: 0.04415\n",
      "[ 46/300] train_loss: 0.10203 valid_loss: 0.06199\n",
      "[ 47/300] train_loss: 0.10029 valid_loss: 0.06577\n",
      "[ 48/300] train_loss: 0.10581 valid_loss: 0.05786\n",
      "[ 49/300] train_loss: 0.09567 valid_loss: 0.04180\n",
      "[ 50/300] train_loss: 0.09421 valid_loss: 0.07747\n",
      "[ 51/300] train_loss: 0.09571 valid_loss: 0.04025\n",
      "Validation loss decreased (0.041449 --> 0.040246).  Saving model ...\n",
      "[ 52/300] train_loss: 0.09164 valid_loss: 0.06467\n",
      "[ 53/300] train_loss: 0.09172 valid_loss: 0.05759\n",
      "[ 54/300] train_loss: 0.09654 valid_loss: 0.06130\n",
      "[ 55/300] train_loss: 0.09348 valid_loss: 0.04499\n",
      "[ 56/300] train_loss: 0.09208 valid_loss: 0.03984\n",
      "Validation loss decreased (0.040246 --> 0.039845).  Saving model ...\n",
      "[ 57/300] train_loss: 0.10011 valid_loss: 0.05390\n",
      "[ 58/300] train_loss: 0.09854 valid_loss: 0.04311\n",
      "[ 59/300] train_loss: 0.08842 valid_loss: 0.07841\n",
      "[ 60/300] train_loss: 0.08820 valid_loss: 0.04125\n",
      "[ 61/300] train_loss: 0.08646 valid_loss: 0.03934\n",
      "Validation loss decreased (0.039845 --> 0.039339).  Saving model ...\n",
      "[ 62/300] train_loss: 0.08666 valid_loss: 0.05875\n",
      "[ 63/300] train_loss: 0.09091 valid_loss: 0.04533\n",
      "[ 64/300] train_loss: 0.08354 valid_loss: 0.06418\n",
      "[ 65/300] train_loss: 0.08770 valid_loss: 0.04919\n",
      "[ 66/300] train_loss: 0.08728 valid_loss: 0.03833\n",
      "Validation loss decreased (0.039339 --> 0.038329).  Saving model ...\n",
      "[ 67/300] train_loss: 0.09951 valid_loss: 0.05099\n",
      "[ 68/300] train_loss: 0.10384 valid_loss: 0.12179\n",
      "[ 69/300] train_loss: 0.09113 valid_loss: 0.05069\n",
      "[ 70/300] train_loss: 0.08204 valid_loss: 0.04953\n",
      "[ 71/300] train_loss: 0.08213 valid_loss: 0.06552\n",
      "[ 72/300] train_loss: 0.08496 valid_loss: 0.04207\n",
      "[ 73/300] train_loss: 0.08098 valid_loss: 0.04954\n",
      "[ 74/300] train_loss: 0.08005 valid_loss: 0.04228\n",
      "[ 75/300] train_loss: 0.08193 valid_loss: 0.03813\n",
      "Validation loss decreased (0.038329 --> 0.038129).  Saving model ...\n",
      "[ 76/300] train_loss: 0.08888 valid_loss: 0.05166\n",
      "[ 77/300] train_loss: 0.08731 valid_loss: 0.05043\n",
      "[ 78/300] train_loss: 0.07944 valid_loss: 0.03702\n",
      "Validation loss decreased (0.038129 --> 0.037015).  Saving model ...\n",
      "[ 79/300] train_loss: 0.08599 valid_loss: 0.04338\n",
      "[ 80/300] train_loss: 0.08012 valid_loss: 0.04026\n",
      "[ 81/300] train_loss: 0.08079 valid_loss: 0.05077\n",
      "[ 82/300] train_loss: 0.08581 valid_loss: 0.04894\n",
      "[ 83/300] train_loss: 0.08636 valid_loss: 0.06430\n",
      "[ 84/300] train_loss: 0.08565 valid_loss: 0.06964\n",
      "[ 85/300] train_loss: 0.08208 valid_loss: 0.03886\n",
      "[ 86/300] train_loss: 0.08238 valid_loss: 0.03725\n",
      "[ 87/300] train_loss: 0.08068 valid_loss: 0.04488\n",
      "[ 88/300] train_loss: 0.08227 valid_loss: 0.03818\n",
      "[ 89/300] train_loss: 0.08368 valid_loss: 0.03642\n",
      "Validation loss decreased (0.037015 --> 0.036424).  Saving model ...\n",
      "[ 90/300] train_loss: 0.07628 valid_loss: 0.04587\n",
      "[ 91/300] train_loss: 0.07770 valid_loss: 0.03766\n",
      "[ 92/300] train_loss: 0.07686 valid_loss: 0.03701\n",
      "[ 93/300] train_loss: 0.07608 valid_loss: 0.04090\n",
      "[ 94/300] train_loss: 0.07821 valid_loss: 0.03739\n",
      "[ 95/300] train_loss: 0.07597 valid_loss: 0.03744\n",
      "[ 96/300] train_loss: 0.07572 valid_loss: 0.03845\n",
      "[ 97/300] train_loss: 0.07570 valid_loss: 0.03725\n",
      "[ 98/300] train_loss: 0.07559 valid_loss: 0.03861\n",
      "[ 99/300] train_loss: 0.07621 valid_loss: 0.03734\n",
      "[100/300] train_loss: 0.07534 valid_loss: 0.03662\n",
      "[101/300] train_loss: 0.07873 valid_loss: 0.03850\n",
      "[102/300] train_loss: 0.07771 valid_loss: 0.04192\n",
      "[103/300] train_loss: 0.07368 valid_loss: 0.03534\n",
      "Validation loss decreased (0.036424 --> 0.035343).  Saving model ...\n",
      "[104/300] train_loss: 0.07448 valid_loss: 0.03310\n",
      "Validation loss decreased (0.035343 --> 0.033105).  Saving model ...\n",
      "[105/300] train_loss: 0.07564 valid_loss: 0.03460\n",
      "[106/300] train_loss: 0.07291 valid_loss: 0.03765\n",
      "[107/300] train_loss: 0.07377 valid_loss: 0.03546\n",
      "[108/300] train_loss: 0.07364 valid_loss: 0.03305\n",
      "Validation loss decreased (0.033105 --> 0.033050).  Saving model ...\n",
      "[109/300] train_loss: 0.07582 valid_loss: 0.03598\n",
      "[110/300] train_loss: 0.07240 valid_loss: 0.04037\n",
      "[111/300] train_loss: 0.07324 valid_loss: 0.03537\n",
      "[112/300] train_loss: 0.07305 valid_loss: 0.03405\n",
      "[113/300] train_loss: 0.07347 valid_loss: 0.03916\n",
      "[114/300] train_loss: 0.07169 valid_loss: 0.04146\n",
      "[115/300] train_loss: 0.07231 valid_loss: 0.03421\n",
      "[116/300] train_loss: 0.07212 valid_loss: 0.03740\n",
      "[117/300] train_loss: 0.07232 valid_loss: 0.03664\n",
      "[118/300] train_loss: 0.07187 valid_loss: 0.03426\n",
      "[119/300] train_loss: 0.07150 valid_loss: 0.03303\n",
      "Validation loss decreased (0.033050 --> 0.033028).  Saving model ...\n",
      "[120/300] train_loss: 0.07215 valid_loss: 0.03708\n",
      "[121/300] train_loss: 0.07065 valid_loss: 0.03746\n",
      "[122/300] train_loss: 0.07123 valid_loss: 0.03517\n",
      "[123/300] train_loss: 0.07107 valid_loss: 0.03530\n",
      "[124/300] train_loss: 0.07128 valid_loss: 0.03802\n",
      "[125/300] train_loss: 0.07038 valid_loss: 0.04003\n",
      "[126/300] train_loss: 0.07089 valid_loss: 0.03589\n",
      "[127/300] train_loss: 0.07090 valid_loss: 0.03234\n",
      "Validation loss decreased (0.033028 --> 0.032340).  Saving model ...\n",
      "[128/300] train_loss: 0.07161 valid_loss: 0.03463\n",
      "[129/300] train_loss: 0.06984 valid_loss: 0.03856\n",
      "[130/300] train_loss: 0.07042 valid_loss: 0.03465\n",
      "[131/300] train_loss: 0.07015 valid_loss: 0.03538\n",
      "[132/300] train_loss: 0.07035 valid_loss: 0.03447\n",
      "[133/300] train_loss: 0.06940 valid_loss: 0.03641\n",
      "[134/300] train_loss: 0.06976 valid_loss: 0.03503\n",
      "[135/300] train_loss: 0.06947 valid_loss: 0.03528\n",
      "[136/300] train_loss: 0.06921 valid_loss: 0.03451\n",
      "[137/300] train_loss: 0.06957 valid_loss: 0.03450\n",
      "[138/300] train_loss: 0.06880 valid_loss: 0.03524\n",
      "[139/300] train_loss: 0.06866 valid_loss: 0.03486\n",
      "[140/300] train_loss: 0.06866 valid_loss: 0.03585\n",
      "[141/300] train_loss: 0.06867 valid_loss: 0.03693\n",
      "[142/300] train_loss: 0.06863 valid_loss: 0.03552\n",
      "[143/300] train_loss: 0.06862 valid_loss: 0.03589\n",
      "[144/300] train_loss: 0.06841 valid_loss: 0.03497\n",
      "[145/300] train_loss: 0.06827 valid_loss: 0.03524\n",
      "[146/300] train_loss: 0.06815 valid_loss: 0.03511\n",
      "[147/300] train_loss: 0.06814 valid_loss: 0.03587\n",
      "[148/300] train_loss: 0.06837 valid_loss: 0.03485\n",
      "[149/300] train_loss: 0.06785 valid_loss: 0.03472\n",
      "[150/300] train_loss: 0.06801 valid_loss: 0.03506\n",
      "[151/300] train_loss: 0.06771 valid_loss: 0.03724\n",
      "[152/300] train_loss: 0.06781 valid_loss: 0.03584\n",
      "[153/300] train_loss: 0.06803 valid_loss: 0.03829\n",
      "[154/300] train_loss: 0.06839 valid_loss: 0.03522\n",
      "[155/300] train_loss: 0.06810 valid_loss: 0.03443\n",
      "[156/300] train_loss: 0.06767 valid_loss: 0.03525\n",
      "[157/300] train_loss: 0.06732 valid_loss: 0.03613\n",
      "[158/300] train_loss: 0.06738 valid_loss: 0.03540\n",
      "[159/300] train_loss: 0.06722 valid_loss: 0.03569\n",
      "[160/300] train_loss: 0.06733 valid_loss: 0.03528\n",
      "[161/300] train_loss: 0.06712 valid_loss: 0.03514\n",
      "[162/300] train_loss: 0.06698 valid_loss: 0.03533\n",
      "[163/300] train_loss: 0.06690 valid_loss: 0.03580\n",
      "[164/300] train_loss: 0.06689 valid_loss: 0.03614\n",
      "[165/300] train_loss: 0.06688 valid_loss: 0.03555\n",
      "[166/300] train_loss: 0.06685 valid_loss: 0.03535\n",
      "[167/300] train_loss: 0.06674 valid_loss: 0.03545\n",
      "[168/300] train_loss: 0.06665 valid_loss: 0.03560\n",
      "[169/300] train_loss: 0.06666 valid_loss: 0.03569\n",
      "[170/300] train_loss: 0.06641 valid_loss: 0.03585\n",
      "[171/300] train_loss: 0.06657 valid_loss: 0.03596\n",
      "[172/300] train_loss: 0.06647 valid_loss: 0.03581\n",
      "[173/300] train_loss: 0.06642 valid_loss: 0.03578\n",
      "[174/300] train_loss: 0.06630 valid_loss: 0.03585\n",
      "[175/300] train_loss: 0.06627 valid_loss: 0.03597\n",
      "[176/300] train_loss: 0.06624 valid_loss: 0.03584\n",
      "[177/300] train_loss: 0.06621 valid_loss: 0.03581\n",
      "Early stopping\n",
      "[  1/300] train_loss: 0.84669 valid_loss: 0.41676\n",
      "Validation loss decreased (inf --> 0.416755).  Saving model ...\n",
      "[  2/300] train_loss: 0.65820 valid_loss: 0.32787\n",
      "Validation loss decreased (0.416755 --> 0.327869).  Saving model ...\n",
      "[  3/300] train_loss: 0.37083 valid_loss: 0.20317\n",
      "Validation loss decreased (0.327869 --> 0.203168).  Saving model ...\n",
      "[  4/300] train_loss: 0.31531 valid_loss: 0.32360\n",
      "[  5/300] train_loss: 0.40393 valid_loss: 0.30376\n",
      "[  6/300] train_loss: 0.32947 valid_loss: 0.17368\n",
      "Validation loss decreased (0.203168 --> 0.173683).  Saving model ...\n",
      "[  7/300] train_loss: 0.26434 valid_loss: 0.22154\n",
      "[  8/300] train_loss: 0.24239 valid_loss: 0.15090\n",
      "Validation loss decreased (0.173683 --> 0.150896).  Saving model ...\n",
      "[  9/300] train_loss: 0.23576 valid_loss: 0.11619\n",
      "Validation loss decreased (0.150896 --> 0.116189).  Saving model ...\n",
      "[ 10/300] train_loss: 0.30576 valid_loss: 0.11478\n",
      "Validation loss decreased (0.116189 --> 0.114778).  Saving model ...\n",
      "[ 11/300] train_loss: 0.23091 valid_loss: 0.14449\n",
      "[ 12/300] train_loss: 0.21993 valid_loss: 0.15140\n",
      "[ 13/300] train_loss: 0.20146 valid_loss: 0.14432\n",
      "[ 14/300] train_loss: 0.19663 valid_loss: 0.10067\n",
      "Validation loss decreased (0.114778 --> 0.100669).  Saving model ...\n",
      "[ 15/300] train_loss: 0.19155 valid_loss: 0.09215\n",
      "Validation loss decreased (0.100669 --> 0.092146).  Saving model ...\n",
      "[ 16/300] train_loss: 0.17734 valid_loss: 0.07498\n",
      "Validation loss decreased (0.092146 --> 0.074978).  Saving model ...\n",
      "[ 17/300] train_loss: 0.16714 valid_loss: 0.18569\n",
      "[ 18/300] train_loss: 0.17042 valid_loss: 0.05990\n",
      "Validation loss decreased (0.074978 --> 0.059897).  Saving model ...\n",
      "[ 19/300] train_loss: 0.17432 valid_loss: 0.15712\n",
      "[ 20/300] train_loss: 0.17991 valid_loss: 0.07244\n",
      "[ 21/300] train_loss: 0.20437 valid_loss: 0.12727\n",
      "[ 22/300] train_loss: 0.18443 valid_loss: 0.12733\n",
      "[ 23/300] train_loss: 0.18216 valid_loss: 0.06910\n",
      "[ 24/300] train_loss: 0.19735 valid_loss: 0.05725\n",
      "Validation loss decreased (0.059897 --> 0.057253).  Saving model ...\n",
      "[ 25/300] train_loss: 0.15510 valid_loss: 0.14365\n",
      "[ 26/300] train_loss: 0.15429 valid_loss: 0.08684\n",
      "[ 27/300] train_loss: 0.15251 valid_loss: 0.05204\n",
      "Validation loss decreased (0.057253 --> 0.052037).  Saving model ...\n",
      "[ 28/300] train_loss: 0.13053 valid_loss: 0.05602\n",
      "[ 29/300] train_loss: 0.13321 valid_loss: 0.05436\n",
      "[ 30/300] train_loss: 0.12580 valid_loss: 0.04920\n",
      "Validation loss decreased (0.052037 --> 0.049202).  Saving model ...\n",
      "[ 31/300] train_loss: 0.13721 valid_loss: 0.06760\n",
      "[ 32/300] train_loss: 0.14268 valid_loss: 0.08259\n",
      "[ 33/300] train_loss: 0.12834 valid_loss: 0.09004\n",
      "[ 34/300] train_loss: 0.12620 valid_loss: 0.05378\n",
      "[ 35/300] train_loss: 0.11788 valid_loss: 0.04782\n",
      "Validation loss decreased (0.049202 --> 0.047815).  Saving model ...\n",
      "[ 36/300] train_loss: 0.12656 valid_loss: 0.04172\n",
      "Validation loss decreased (0.047815 --> 0.041720).  Saving model ...\n",
      "[ 37/300] train_loss: 0.12101 valid_loss: 0.08301\n",
      "[ 38/300] train_loss: 0.11669 valid_loss: 0.04733\n",
      "[ 39/300] train_loss: 0.11442 valid_loss: 0.05298\n",
      "[ 40/300] train_loss: 0.13641 valid_loss: 0.10295\n",
      "[ 41/300] train_loss: 0.15063 valid_loss: 0.09576\n",
      "[ 42/300] train_loss: 0.14666 valid_loss: 0.18984\n",
      "[ 43/300] train_loss: 0.12024 valid_loss: 0.06464\n",
      "[ 44/300] train_loss: 0.11735 valid_loss: 0.06308\n",
      "[ 45/300] train_loss: 0.10416 valid_loss: 0.05267\n",
      "[ 46/300] train_loss: 0.11982 valid_loss: 0.09698\n",
      "[ 47/300] train_loss: 0.13008 valid_loss: 0.05290\n",
      "[ 48/300] train_loss: 0.11719 valid_loss: 0.07507\n",
      "[ 49/300] train_loss: 0.10490 valid_loss: 0.04508\n",
      "[ 50/300] train_loss: 0.13282 valid_loss: 0.07692\n",
      "[ 51/300] train_loss: 0.10253 valid_loss: 0.04357\n",
      "[ 52/300] train_loss: 0.11948 valid_loss: 0.11509\n",
      "[ 53/300] train_loss: 0.12737 valid_loss: 0.06285\n",
      "[ 54/300] train_loss: 0.10433 valid_loss: 0.03939\n",
      "Validation loss decreased (0.041720 --> 0.039393).  Saving model ...\n",
      "[ 55/300] train_loss: 0.10230 valid_loss: 0.08353\n",
      "[ 56/300] train_loss: 0.10083 valid_loss: 0.06024\n",
      "[ 57/300] train_loss: 0.09462 valid_loss: 0.04057\n",
      "[ 58/300] train_loss: 0.09647 valid_loss: 0.03925\n",
      "Validation loss decreased (0.039393 --> 0.039247).  Saving model ...\n",
      "[ 59/300] train_loss: 0.09346 valid_loss: 0.03843\n",
      "Validation loss decreased (0.039247 --> 0.038426).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09609 valid_loss: 0.04184\n",
      "[ 61/300] train_loss: 0.09840 valid_loss: 0.09329\n",
      "[ 62/300] train_loss: 0.09415 valid_loss: 0.05386\n",
      "[ 63/300] train_loss: 0.09445 valid_loss: 0.03887\n",
      "[ 64/300] train_loss: 0.08963 valid_loss: 0.05796\n",
      "[ 65/300] train_loss: 0.09014 valid_loss: 0.04326\n",
      "[ 66/300] train_loss: 0.08900 valid_loss: 0.03934\n",
      "[ 67/300] train_loss: 0.08722 valid_loss: 0.04619\n",
      "[ 68/300] train_loss: 0.08814 valid_loss: 0.06115\n",
      "[ 69/300] train_loss: 0.08721 valid_loss: 0.05893\n",
      "[ 70/300] train_loss: 0.09582 valid_loss: 0.03905\n",
      "[ 71/300] train_loss: 0.09905 valid_loss: 0.08473\n",
      "[ 72/300] train_loss: 0.08659 valid_loss: 0.05048\n",
      "[ 73/300] train_loss: 0.08697 valid_loss: 0.04773\n",
      "[ 74/300] train_loss: 0.08608 valid_loss: 0.04279\n",
      "[ 75/300] train_loss: 0.08527 valid_loss: 0.03427\n",
      "Validation loss decreased (0.038426 --> 0.034269).  Saving model ...\n",
      "[ 76/300] train_loss: 0.08526 valid_loss: 0.05945\n",
      "[ 77/300] train_loss: 0.08517 valid_loss: 0.03732\n",
      "[ 78/300] train_loss: 0.08449 valid_loss: 0.03636\n",
      "[ 79/300] train_loss: 0.08295 valid_loss: 0.03338\n",
      "Validation loss decreased (0.034269 --> 0.033379).  Saving model ...\n",
      "[ 80/300] train_loss: 0.08127 valid_loss: 0.04335\n",
      "[ 81/300] train_loss: 0.08123 valid_loss: 0.05045\n",
      "[ 82/300] train_loss: 0.08055 valid_loss: 0.04375\n",
      "[ 83/300] train_loss: 0.08048 valid_loss: 0.04213\n",
      "[ 84/300] train_loss: 0.08109 valid_loss: 0.03692\n",
      "[ 85/300] train_loss: 0.07944 valid_loss: 0.03605\n",
      "[ 86/300] train_loss: 0.08014 valid_loss: 0.05430\n",
      "[ 87/300] train_loss: 0.07907 valid_loss: 0.04472\n",
      "[ 88/300] train_loss: 0.07944 valid_loss: 0.03345\n",
      "[ 89/300] train_loss: 0.07756 valid_loss: 0.03422\n",
      "[ 90/300] train_loss: 0.07855 valid_loss: 0.04316\n",
      "[ 91/300] train_loss: 0.08322 valid_loss: 0.03573\n",
      "[ 92/300] train_loss: 0.07667 valid_loss: 0.03215\n",
      "Validation loss decreased (0.033379 --> 0.032153).  Saving model ...\n",
      "[ 93/300] train_loss: 0.07725 valid_loss: 0.03700\n",
      "[ 94/300] train_loss: 0.08094 valid_loss: 0.03874\n",
      "[ 95/300] train_loss: 0.08108 valid_loss: 0.05050\n",
      "[ 96/300] train_loss: 0.07736 valid_loss: 0.03403\n",
      "[ 97/300] train_loss: 0.07560 valid_loss: 0.03627\n",
      "[ 98/300] train_loss: 0.07662 valid_loss: 0.03427\n",
      "[ 99/300] train_loss: 0.07654 valid_loss: 0.03443\n",
      "[100/300] train_loss: 0.07911 valid_loss: 0.03358\n",
      "[101/300] train_loss: 0.07805 valid_loss: 0.04409\n",
      "[102/300] train_loss: 0.07605 valid_loss: 0.03110\n",
      "Validation loss decreased (0.032153 --> 0.031097).  Saving model ...\n",
      "[103/300] train_loss: 0.07649 valid_loss: 0.03218\n",
      "[104/300] train_loss: 0.07380 valid_loss: 0.03182\n",
      "[105/300] train_loss: 0.07454 valid_loss: 0.03493\n",
      "[106/300] train_loss: 0.07551 valid_loss: 0.03255\n",
      "[107/300] train_loss: 0.07522 valid_loss: 0.03497\n",
      "[108/300] train_loss: 0.07384 valid_loss: 0.02990\n",
      "Validation loss decreased (0.031097 --> 0.029901).  Saving model ...\n",
      "[109/300] train_loss: 0.07605 valid_loss: 0.03566\n",
      "[110/300] train_loss: 0.07282 valid_loss: 0.03697\n",
      "[111/300] train_loss: 0.07405 valid_loss: 0.03423\n",
      "[112/300] train_loss: 0.07299 valid_loss: 0.03000\n",
      "[113/300] train_loss: 0.07507 valid_loss: 0.03062\n",
      "[114/300] train_loss: 0.07229 valid_loss: 0.03081\n",
      "[115/300] train_loss: 0.07220 valid_loss: 0.03370\n",
      "[116/300] train_loss: 0.07354 valid_loss: 0.03489\n",
      "[117/300] train_loss: 0.07337 valid_loss: 0.03083\n",
      "[118/300] train_loss: 0.07142 valid_loss: 0.03420\n",
      "[119/300] train_loss: 0.07185 valid_loss: 0.03109\n",
      "[120/300] train_loss: 0.07135 valid_loss: 0.03196\n",
      "[121/300] train_loss: 0.07248 valid_loss: 0.02870\n",
      "Validation loss decreased (0.029901 --> 0.028703).  Saving model ...\n",
      "[122/300] train_loss: 0.07290 valid_loss: 0.03157\n",
      "[123/300] train_loss: 0.07067 valid_loss: 0.03907\n",
      "[124/300] train_loss: 0.07140 valid_loss: 0.03044\n",
      "[125/300] train_loss: 0.07135 valid_loss: 0.03422\n",
      "[126/300] train_loss: 0.07210 valid_loss: 0.03131\n",
      "[127/300] train_loss: 0.07025 valid_loss: 0.03299\n",
      "[128/300] train_loss: 0.07119 valid_loss: 0.03438\n",
      "[129/300] train_loss: 0.07015 valid_loss: 0.02916\n",
      "[130/300] train_loss: 0.07095 valid_loss: 0.03087\n",
      "[131/300] train_loss: 0.06989 valid_loss: 0.03015\n",
      "[132/300] train_loss: 0.06973 valid_loss: 0.03088\n",
      "[133/300] train_loss: 0.06966 valid_loss: 0.03134\n",
      "[134/300] train_loss: 0.07003 valid_loss: 0.03122\n",
      "[135/300] train_loss: 0.06959 valid_loss: 0.03212\n",
      "[136/300] train_loss: 0.06934 valid_loss: 0.03058\n",
      "[137/300] train_loss: 0.06887 valid_loss: 0.03166\n",
      "[138/300] train_loss: 0.06893 valid_loss: 0.03072\n",
      "[139/300] train_loss: 0.06887 valid_loss: 0.03112\n",
      "[140/300] train_loss: 0.06885 valid_loss: 0.03149\n",
      "[141/300] train_loss: 0.06864 valid_loss: 0.03165\n",
      "[142/300] train_loss: 0.06852 valid_loss: 0.03188\n",
      "[143/300] train_loss: 0.06840 valid_loss: 0.03135\n",
      "[144/300] train_loss: 0.06827 valid_loss: 0.03096\n",
      "[145/300] train_loss: 0.06801 valid_loss: 0.03240\n",
      "[146/300] train_loss: 0.06809 valid_loss: 0.03262\n",
      "[147/300] train_loss: 0.06798 valid_loss: 0.03227\n",
      "[148/300] train_loss: 0.06780 valid_loss: 0.03169\n",
      "[149/300] train_loss: 0.06777 valid_loss: 0.03149\n",
      "[150/300] train_loss: 0.06763 valid_loss: 0.03139\n",
      "[151/300] train_loss: 0.06746 valid_loss: 0.03226\n",
      "[152/300] train_loss: 0.06757 valid_loss: 0.03158\n",
      "[153/300] train_loss: 0.06743 valid_loss: 0.03117\n",
      "[154/300] train_loss: 0.06730 valid_loss: 0.03155\n",
      "[155/300] train_loss: 0.06723 valid_loss: 0.03317\n",
      "[156/300] train_loss: 0.06720 valid_loss: 0.03216\n",
      "[157/300] train_loss: 0.06731 valid_loss: 0.03343\n",
      "[158/300] train_loss: 0.06731 valid_loss: 0.03197\n",
      "[159/300] train_loss: 0.06695 valid_loss: 0.03400\n",
      "[160/300] train_loss: 0.06705 valid_loss: 0.03201\n",
      "[161/300] train_loss: 0.06692 valid_loss: 0.03384\n",
      "[162/300] train_loss: 0.06683 valid_loss: 0.03208\n",
      "[163/300] train_loss: 0.06665 valid_loss: 0.03345\n",
      "[164/300] train_loss: 0.06673 valid_loss: 0.03194\n",
      "[165/300] train_loss: 0.06671 valid_loss: 0.03231\n",
      "[166/300] train_loss: 0.06646 valid_loss: 0.03182\n",
      "[167/300] train_loss: 0.06662 valid_loss: 0.03178\n",
      "[168/300] train_loss: 0.06638 valid_loss: 0.03163\n",
      "[169/300] train_loss: 0.06630 valid_loss: 0.03164\n",
      "[170/300] train_loss: 0.06625 valid_loss: 0.03161\n",
      "[171/300] train_loss: 0.06613 valid_loss: 0.03216\n",
      "Early stopping\n",
      "[  1/300] train_loss: 0.98913 valid_loss: 0.62204\n",
      "Validation loss decreased (inf --> 0.622038).  Saving model ...\n",
      "[  2/300] train_loss: 0.53176 valid_loss: 0.30879\n",
      "Validation loss decreased (0.622038 --> 0.308786).  Saving model ...\n",
      "[  3/300] train_loss: 0.45005 valid_loss: 0.36565\n",
      "[  4/300] train_loss: 0.40198 valid_loss: 0.17401\n",
      "Validation loss decreased (0.308786 --> 0.174008).  Saving model ...\n",
      "[  5/300] train_loss: 0.36649 valid_loss: 0.19217\n",
      "[  6/300] train_loss: 0.30145 valid_loss: 0.21722\n",
      "[  7/300] train_loss: 0.35562 valid_loss: 0.18897\n",
      "[  8/300] train_loss: 0.29951 valid_loss: 0.20483\n",
      "[  9/300] train_loss: 0.25215 valid_loss: 0.13309\n",
      "Validation loss decreased (0.174008 --> 0.133085).  Saving model ...\n",
      "[ 10/300] train_loss: 0.22201 valid_loss: 0.08248\n",
      "Validation loss decreased (0.133085 --> 0.082475).  Saving model ...\n",
      "[ 11/300] train_loss: 0.21250 valid_loss: 0.10637\n",
      "[ 12/300] train_loss: 0.25698 valid_loss: 0.32896\n",
      "[ 13/300] train_loss: 0.20906 valid_loss: 0.07268\n",
      "Validation loss decreased (0.082475 --> 0.072677).  Saving model ...\n",
      "[ 14/300] train_loss: 0.19290 valid_loss: 0.07227\n",
      "Validation loss decreased (0.072677 --> 0.072266).  Saving model ...\n",
      "[ 15/300] train_loss: 0.19741 valid_loss: 0.10424\n",
      "[ 16/300] train_loss: 0.24573 valid_loss: 0.11521\n",
      "[ 17/300] train_loss: 0.21942 valid_loss: 0.08596\n",
      "[ 18/300] train_loss: 0.19347 valid_loss: 0.10396\n",
      "[ 19/300] train_loss: 0.23458 valid_loss: 0.07831\n",
      "[ 20/300] train_loss: 0.18808 valid_loss: 0.17624\n",
      "[ 21/300] train_loss: 0.18861 valid_loss: 0.10906\n",
      "[ 22/300] train_loss: 0.16697 valid_loss: 0.09263\n",
      "[ 23/300] train_loss: 0.16614 valid_loss: 0.07418\n",
      "[ 24/300] train_loss: 0.17201 valid_loss: 0.08440\n",
      "[ 25/300] train_loss: 0.14978 valid_loss: 0.06257\n",
      "Validation loss decreased (0.072266 --> 0.062574).  Saving model ...\n",
      "[ 26/300] train_loss: 0.16147 valid_loss: 0.09976\n",
      "[ 27/300] train_loss: 0.14173 valid_loss: 0.06032\n",
      "Validation loss decreased (0.062574 --> 0.060324).  Saving model ...\n",
      "[ 28/300] train_loss: 0.13820 valid_loss: 0.05555\n",
      "Validation loss decreased (0.060324 --> 0.055555).  Saving model ...\n",
      "[ 29/300] train_loss: 0.14147 valid_loss: 0.06097\n",
      "[ 30/300] train_loss: 0.12997 valid_loss: 0.06629\n",
      "[ 31/300] train_loss: 0.15649 valid_loss: 0.18321\n",
      "[ 32/300] train_loss: 0.13675 valid_loss: 0.10969\n",
      "[ 33/300] train_loss: 0.12417 valid_loss: 0.06334\n",
      "[ 34/300] train_loss: 0.12473 valid_loss: 0.05019\n",
      "Validation loss decreased (0.055555 --> 0.050192).  Saving model ...\n",
      "[ 35/300] train_loss: 0.15144 valid_loss: 0.06544\n",
      "[ 36/300] train_loss: 0.11787 valid_loss: 0.06861\n",
      "[ 37/300] train_loss: 0.12534 valid_loss: 0.05786\n",
      "[ 38/300] train_loss: 0.11427 valid_loss: 0.06319\n",
      "[ 39/300] train_loss: 0.14737 valid_loss: 0.07292\n",
      "[ 40/300] train_loss: 0.14015 valid_loss: 0.05268\n",
      "[ 41/300] train_loss: 0.11131 valid_loss: 0.05675\n",
      "[ 42/300] train_loss: 0.11117 valid_loss: 0.04839\n",
      "Validation loss decreased (0.050192 --> 0.048389).  Saving model ...\n",
      "[ 43/300] train_loss: 0.10668 valid_loss: 0.05130\n",
      "[ 44/300] train_loss: 0.10655 valid_loss: 0.06536\n",
      "[ 45/300] train_loss: 0.11597 valid_loss: 0.07113\n",
      "[ 46/300] train_loss: 0.11342 valid_loss: 0.04746\n",
      "Validation loss decreased (0.048389 --> 0.047462).  Saving model ...\n",
      "[ 47/300] train_loss: 0.10945 valid_loss: 0.06181\n",
      "[ 48/300] train_loss: 0.10412 valid_loss: 0.05657\n",
      "[ 49/300] train_loss: 0.10748 valid_loss: 0.09616\n",
      "[ 50/300] train_loss: 0.09995 valid_loss: 0.05452\n",
      "[ 51/300] train_loss: 0.10141 valid_loss: 0.04976\n",
      "[ 52/300] train_loss: 0.09991 valid_loss: 0.07943\n",
      "[ 53/300] train_loss: 0.09738 valid_loss: 0.04870\n",
      "[ 54/300] train_loss: 0.09908 valid_loss: 0.07018\n",
      "[ 55/300] train_loss: 0.10581 valid_loss: 0.06148\n",
      "[ 56/300] train_loss: 0.10835 valid_loss: 0.04816\n",
      "[ 57/300] train_loss: 0.09420 valid_loss: 0.05655\n",
      "[ 58/300] train_loss: 0.09265 valid_loss: 0.05065\n",
      "[ 59/300] train_loss: 0.09457 valid_loss: 0.04554\n",
      "Validation loss decreased (0.047462 --> 0.045539).  Saving model ...\n",
      "[ 60/300] train_loss: 0.09243 valid_loss: 0.05981\n",
      "[ 61/300] train_loss: 0.09030 valid_loss: 0.05252\n",
      "[ 62/300] train_loss: 0.08934 valid_loss: 0.05059\n",
      "[ 63/300] train_loss: 0.08857 valid_loss: 0.04879\n",
      "[ 64/300] train_loss: 0.08789 valid_loss: 0.04696\n",
      "[ 65/300] train_loss: 0.08811 valid_loss: 0.06007\n",
      "[ 66/300] train_loss: 0.09047 valid_loss: 0.05521\n",
      "[ 67/300] train_loss: 0.09249 valid_loss: 0.05287\n",
      "[ 68/300] train_loss: 0.09765 valid_loss: 0.04533\n",
      "Validation loss decreased (0.045539 --> 0.045326).  Saving model ...\n",
      "[ 69/300] train_loss: 0.09075 valid_loss: 0.04179\n",
      "Validation loss decreased (0.045326 --> 0.041789).  Saving model ...\n",
      "[ 70/300] train_loss: 0.09339 valid_loss: 0.04656\n",
      "[ 71/300] train_loss: 0.09365 valid_loss: 0.03822\n",
      "Validation loss decreased (0.041789 --> 0.038216).  Saving model ...\n",
      "[ 72/300] train_loss: 0.08655 valid_loss: 0.05934\n",
      "[ 73/300] train_loss: 0.09792 valid_loss: 0.06151\n",
      "[ 74/300] train_loss: 0.09833 valid_loss: 0.06374\n",
      "[ 75/300] train_loss: 0.08978 valid_loss: 0.05299\n",
      "[ 76/300] train_loss: 0.08368 valid_loss: 0.05062\n",
      "[ 77/300] train_loss: 0.08463 valid_loss: 0.04664\n",
      "[ 78/300] train_loss: 0.08815 valid_loss: 0.05895\n",
      "[ 79/300] train_loss: 0.08214 valid_loss: 0.06268\n",
      "[ 80/300] train_loss: 0.08610 valid_loss: 0.04643\n",
      "[ 81/300] train_loss: 0.08485 valid_loss: 0.04083\n",
      "[ 82/300] train_loss: 0.09468 valid_loss: 0.07594\n",
      "[ 83/300] train_loss: 0.08840 valid_loss: 0.06391\n",
      "[ 84/300] train_loss: 0.08223 valid_loss: 0.04606\n",
      "[ 85/300] train_loss: 0.08120 valid_loss: 0.05108\n",
      "[ 86/300] train_loss: 0.08352 valid_loss: 0.05244\n",
      "[ 87/300] train_loss: 0.08050 valid_loss: 0.04129\n",
      "[ 88/300] train_loss: 0.07970 valid_loss: 0.04355\n",
      "[ 89/300] train_loss: 0.07952 valid_loss: 0.04538\n",
      "[ 90/300] train_loss: 0.08093 valid_loss: 0.04174\n",
      "[ 91/300] train_loss: 0.07863 valid_loss: 0.05787\n",
      "[ 92/300] train_loss: 0.08027 valid_loss: 0.04349\n",
      "[ 93/300] train_loss: 0.07798 valid_loss: 0.04823\n",
      "[ 94/300] train_loss: 0.07772 valid_loss: 0.04473\n",
      "[ 95/300] train_loss: 0.07788 valid_loss: 0.04516\n",
      "[ 96/300] train_loss: 0.07666 valid_loss: 0.04267\n",
      "[ 97/300] train_loss: 0.07672 valid_loss: 0.04571\n",
      "[ 98/300] train_loss: 0.07694 valid_loss: 0.04660\n",
      "[ 99/300] train_loss: 0.07718 valid_loss: 0.04345\n",
      "[100/300] train_loss: 0.07533 valid_loss: 0.04576\n",
      "[101/300] train_loss: 0.07618 valid_loss: 0.04260\n",
      "[102/300] train_loss: 0.07563 valid_loss: 0.04803\n",
      "[103/300] train_loss: 0.07709 valid_loss: 0.04380\n",
      "[104/300] train_loss: 0.07624 valid_loss: 0.03868\n",
      "[105/300] train_loss: 0.07877 valid_loss: 0.04342\n",
      "[106/300] train_loss: 0.07459 valid_loss: 0.04132\n",
      "[107/300] train_loss: 0.07531 valid_loss: 0.04053\n",
      "[108/300] train_loss: 0.07642 valid_loss: 0.03805\n",
      "Validation loss decreased (0.038216 --> 0.038053).  Saving model ...\n",
      "[109/300] train_loss: 0.07807 valid_loss: 0.05632\n",
      "[110/300] train_loss: 0.07494 valid_loss: 0.04051\n",
      "[111/300] train_loss: 0.07545 valid_loss: 0.04082\n",
      "[112/300] train_loss: 0.07467 valid_loss: 0.04249\n",
      "[113/300] train_loss: 0.07361 valid_loss: 0.04063\n",
      "[114/300] train_loss: 0.07340 valid_loss: 0.04188\n",
      "[115/300] train_loss: 0.07453 valid_loss: 0.04402\n",
      "[116/300] train_loss: 0.07376 valid_loss: 0.03993\n",
      "[117/300] train_loss: 0.07344 valid_loss: 0.03767\n",
      "Validation loss decreased (0.038053 --> 0.037669).  Saving model ...\n",
      "[118/300] train_loss: 0.07937 valid_loss: 0.04459\n",
      "[119/300] train_loss: 0.07749 valid_loss: 0.04504\n",
      "[120/300] train_loss: 0.07217 valid_loss: 0.04231\n",
      "[121/300] train_loss: 0.07278 valid_loss: 0.04012\n",
      "[122/300] train_loss: 0.07222 valid_loss: 0.03946\n",
      "[123/300] train_loss: 0.07216 valid_loss: 0.04115\n",
      "[124/300] train_loss: 0.07186 valid_loss: 0.04090\n",
      "[125/300] train_loss: 0.07162 valid_loss: 0.04182\n",
      "[126/300] train_loss: 0.07154 valid_loss: 0.04057\n",
      "[127/300] train_loss: 0.07150 valid_loss: 0.04224\n",
      "[128/300] train_loss: 0.07098 valid_loss: 0.04157\n",
      "[129/300] train_loss: 0.07219 valid_loss: 0.03922\n",
      "[130/300] train_loss: 0.07288 valid_loss: 0.04073\n",
      "[131/300] train_loss: 0.07176 valid_loss: 0.04093\n",
      "[132/300] train_loss: 0.07092 valid_loss: 0.04040\n",
      "[133/300] train_loss: 0.07061 valid_loss: 0.04046\n",
      "[134/300] train_loss: 0.07120 valid_loss: 0.04164\n",
      "[135/300] train_loss: 0.07075 valid_loss: 0.03965\n",
      "[136/300] train_loss: 0.07033 valid_loss: 0.03955\n",
      "[137/300] train_loss: 0.07023 valid_loss: 0.04089\n",
      "[138/300] train_loss: 0.07001 valid_loss: 0.04076\n",
      "[139/300] train_loss: 0.07042 valid_loss: 0.03900\n",
      "[140/300] train_loss: 0.07105 valid_loss: 0.04111\n",
      "[141/300] train_loss: 0.07069 valid_loss: 0.03924\n",
      "[142/300] train_loss: 0.07013 valid_loss: 0.04078\n",
      "[143/300] train_loss: 0.07005 valid_loss: 0.03912\n",
      "[144/300] train_loss: 0.06992 valid_loss: 0.04097\n",
      "[145/300] train_loss: 0.06954 valid_loss: 0.03991\n",
      "[146/300] train_loss: 0.06931 valid_loss: 0.03968\n",
      "[147/300] train_loss: 0.06904 valid_loss: 0.04051\n",
      "[148/300] train_loss: 0.06932 valid_loss: 0.04098\n",
      "[149/300] train_loss: 0.06896 valid_loss: 0.04113\n",
      "[150/300] train_loss: 0.06896 valid_loss: 0.04131\n",
      "[151/300] train_loss: 0.06879 valid_loss: 0.04155\n",
      "[152/300] train_loss: 0.06876 valid_loss: 0.04138\n",
      "[153/300] train_loss: 0.06874 valid_loss: 0.04105\n",
      "[154/300] train_loss: 0.06847 valid_loss: 0.04160\n",
      "[155/300] train_loss: 0.06834 valid_loss: 0.04091\n",
      "[156/300] train_loss: 0.06849 valid_loss: 0.04120\n",
      "[157/300] train_loss: 0.06835 valid_loss: 0.04153\n",
      "[158/300] train_loss: 0.06822 valid_loss: 0.04121\n",
      "[159/300] train_loss: 0.06815 valid_loss: 0.04023\n",
      "[160/300] train_loss: 0.06822 valid_loss: 0.04127\n",
      "[161/300] train_loss: 0.06808 valid_loss: 0.04113\n",
      "[162/300] train_loss: 0.06772 valid_loss: 0.04044\n",
      "[163/300] train_loss: 0.06780 valid_loss: 0.04025\n",
      "[164/300] train_loss: 0.06769 valid_loss: 0.04071\n",
      "[165/300] train_loss: 0.06766 valid_loss: 0.04102\n",
      "[166/300] train_loss: 0.06764 valid_loss: 0.04074\n",
      "[167/300] train_loss: 0.06764 valid_loss: 0.04079\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "result = dict()\n",
    "for i in ['open','close','high','low']:\n",
    "    predict,truth = train_and_get(i)\n",
    "    result.update({i:predict})\n",
    "    result.update({i+'_truth':truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result.keys():\n",
    "    result[i] = result[i].reshape(-1)\n",
    "output = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('result1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00d1462882258b783cb181eb86adde5698c4f2204c9695f847918151df6a5450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
