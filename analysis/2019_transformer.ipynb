{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E)\n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "\n",
    "input_window = 150  # number of input steps\n",
    "output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "batch_size = 512\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=6000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=300, num_layers=1, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size,\n",
    "                                                        nhead=10,\n",
    "                                                        dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer,\n",
    "                                                         num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(feature_size, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,\n",
    "                                          self.src_mask)  #, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(\n",
    "            mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - tw):\n",
    "        train_seq = input_data[i:i + tw]\n",
    "        train_label = input_data[i + output_window:i + tw + output_window]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return torch.FloatTensor(inout_seq)\n",
    "\n",
    "\n",
    "def get_data(name):\n",
    "    # construct a littel toy dataset\n",
    "    time = np.arange(0, 400, 0.1)\n",
    "    data = pd.read_csv('../data/399300.csv')\n",
    "    amplitude = data[name].values\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    #loading weather data from a file\n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "    # looks like normalizing input values curtial for the model\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    sampels = int(len(amplitude)*0.8)\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment..\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:\n",
    "                                    -output_window]  #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1)\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]  #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device),scaler\n",
    "\n",
    "\n",
    "def get_batch(source, i, batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    input = torch.stack(\n",
    "        torch.stack([item[0] for item in data]).chunk(input_window,1))  # 1 is feature size\n",
    "    target = torch.stack(\n",
    "        torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data,epoch):\n",
    "    model.train()  # Turn on the train mode \\o/\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                      epoch, batch,\n",
    "                      len(train_data) // batch_size,\n",
    "                      scheduler.get_lr()[0], elapsed * 1000 / log_interval,\n",
    "                      cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def plot_and_loss(eval_model, data_source, epoch):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                    0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "    #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result, color=\"red\", label=\"prediction\")\n",
    "    pyplot.plot(truth[:len(test_result)], color=\"blue\", label=\"truth\")\n",
    "    pyplot.plot(test_result - truth, color=\"green\", label=\"error\")\n",
    "    pyplot.legend()\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    pyplot.close()\n",
    "\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "# predict the next n steps based on the input data\n",
    "def predict_future(eval_model, data_source, steps):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source, 0, 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):\n",
    "            output = eval_model(data[-input_window:])\n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "    data = data.cpu().view(-1)\n",
    "\n",
    "    # I used this plot to visualize if the model pics up any long therm structure within the data.\n",
    "    pyplot.plot(data, color=\"red\")\n",
    "    pyplot.plot(data[:input_window], color=\"blue\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png' % steps)\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval()  # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "            total_loss += len(data[0]) * criterion(output,\n",
    "                                                   targets).cpu().item()\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get(name):\n",
    "    input_window = 120  # number of input steps\n",
    "    output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "    batch_size = 512\n",
    "    device = torch.device(\"cuda\")\n",
    "    train_data, val_data, scaler = get_data(name)\n",
    "    global model \n",
    "    model = TransAm().to(device)\n",
    "\n",
    "    global criterion\n",
    "    criterion = nn.MSELoss()\n",
    "    lr = 0.001  # learning rate\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    global optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    global scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    global epochs\n",
    "    epochs = 300  # The number of epochs\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_data,epoch)\n",
    "\n",
    "        if (epoch % 100 == 0):\n",
    "            val_loss = plot_and_loss(model, val_data, epoch)\n",
    "            predict_future(model, val_data, 5)\n",
    "        else:\n",
    "            val_loss = evaluate(model, val_data)\n",
    "\n",
    "        if(epoch % 50 ==0):\n",
    "            print('-' * 89)\n",
    "            print(\n",
    "                '| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'\n",
    "                .format(epoch, (time.time() - epoch_start_time), val_loss,\n",
    "                        math.exp(val_loss)))\n",
    "            print('-' * 89)\n",
    "        scheduler.step()\n",
    "    \n",
    "    def get_predict(eval_model, data_source):\n",
    "        eval_model.eval()\n",
    "        total_loss = 0.\n",
    "        test_result = torch.Tensor(0)\n",
    "        truth = torch.Tensor(0)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data_source) - 1):\n",
    "                data, target = get_batch(data_source, i, 1)\n",
    "                output = eval_model(data)\n",
    "                total_loss += criterion(output, target).item()\n",
    "                test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                        0)\n",
    "                truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "        #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "        return scaler.inverse_transform(test_result.view(-1, 1)), scaler.inverse_transform(truth.view(-1, 1))\n",
    "\n",
    "    \n",
    "    return get_predict(model, val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 131 |     7/    7 batches | lr 0.000071 | 89.76 ms | loss 0.00292 | ppl     1.00\n",
      "| epoch 132 |     7/    7 batches | lr 0.000069 | 89.85 ms | loss 0.00313 | ppl     1.00\n",
      "| epoch 133 |     7/    7 batches | lr 0.000068 | 90.41 ms | loss 0.00254 | ppl     1.00\n",
      "| epoch 134 |     7/    7 batches | lr 0.000067 | 90.55 ms | loss 0.00264 | ppl     1.00\n",
      "| epoch 135 |     7/    7 batches | lr 0.000065 | 90.11 ms | loss 0.00269 | ppl     1.00\n",
      "| epoch 136 |     7/    7 batches | lr 0.000064 | 90.06 ms | loss 0.00268 | ppl     1.00\n",
      "| epoch 137 |     7/    7 batches | lr 0.000063 | 90.21 ms | loss 0.00305 | ppl     1.00\n",
      "| epoch 138 |     7/    7 batches | lr 0.000062 | 89.75 ms | loss 0.00235 | ppl     1.00\n",
      "| epoch 139 |     7/    7 batches | lr 0.000060 | 89.93 ms | loss 0.00248 | ppl     1.00\n",
      "| epoch 140 |     7/    7 batches | lr 0.000059 | 90.28 ms | loss 0.00256 | ppl     1.00\n",
      "| epoch 141 |     7/    7 batches | lr 0.000058 | 90.16 ms | loss 0.00240 | ppl     1.00\n",
      "| epoch 142 |     7/    7 batches | lr 0.000057 | 90.20 ms | loss 0.00261 | ppl     1.00\n",
      "| epoch 143 |     7/    7 batches | lr 0.000056 | 90.34 ms | loss 0.00232 | ppl     1.00\n",
      "| epoch 144 |     7/    7 batches | lr 0.000055 | 90.07 ms | loss 0.00237 | ppl     1.00\n",
      "| epoch 145 |     7/    7 batches | lr 0.000053 | 90.30 ms | loss 0.00237 | ppl     1.00\n",
      "| epoch 146 |     7/    7 batches | lr 0.000052 | 90.19 ms | loss 0.00224 | ppl     1.00\n",
      "| epoch 147 |     7/    7 batches | lr 0.000051 | 89.21 ms | loss 0.00234 | ppl     1.00\n",
      "| epoch 148 |     7/    7 batches | lr 0.000050 | 88.86 ms | loss 0.00224 | ppl     1.00\n",
      "| epoch 149 |     7/    7 batches | lr 0.000049 | 89.48 ms | loss 0.00218 | ppl     1.00\n",
      "| epoch 150 |     7/    7 batches | lr 0.000048 | 88.67 ms | loss 0.00226 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 150 | time:  0.67s | valid loss 0.00037 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 151 |     7/    7 batches | lr 0.000047 | 88.62 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch 152 |     7/    7 batches | lr 0.000046 | 88.97 ms | loss 0.00206 | ppl     1.00\n",
      "| epoch 153 |     7/    7 batches | lr 0.000045 | 88.63 ms | loss 0.00211 | ppl     1.00\n",
      "| epoch 154 |     7/    7 batches | lr 0.000045 | 89.32 ms | loss 0.00205 | ppl     1.00\n",
      "| epoch 155 |     7/    7 batches | lr 0.000044 | 90.10 ms | loss 0.00200 | ppl     1.00\n",
      "| epoch 156 |     7/    7 batches | lr 0.000043 | 90.21 ms | loss 0.00203 | ppl     1.00\n",
      "| epoch 157 |     7/    7 batches | lr 0.000042 | 90.12 ms | loss 0.00202 | ppl     1.00\n",
      "| epoch 158 |     7/    7 batches | lr 0.000041 | 90.05 ms | loss 0.00197 | ppl     1.00\n",
      "| epoch 159 |     7/    7 batches | lr 0.000040 | 89.95 ms | loss 0.00198 | ppl     1.00\n",
      "| epoch 160 |     7/    7 batches | lr 0.000039 | 89.89 ms | loss 0.00197 | ppl     1.00\n",
      "| epoch 161 |     7/    7 batches | lr 0.000039 | 89.85 ms | loss 0.00194 | ppl     1.00\n",
      "| epoch 162 |     7/    7 batches | lr 0.000038 | 89.85 ms | loss 0.00194 | ppl     1.00\n",
      "| epoch 163 |     7/    7 batches | lr 0.000037 | 89.89 ms | loss 0.00193 | ppl     1.00\n",
      "| epoch 164 |     7/    7 batches | lr 0.000036 | 90.29 ms | loss 0.00192 | ppl     1.00\n",
      "| epoch 165 |     7/    7 batches | lr 0.000036 | 90.70 ms | loss 0.00192 | ppl     1.00\n",
      "| epoch 166 |     7/    7 batches | lr 0.000035 | 89.64 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch 167 |     7/    7 batches | lr 0.000034 | 90.15 ms | loss 0.00191 | ppl     1.00\n",
      "| epoch 168 |     7/    7 batches | lr 0.000034 | 89.98 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch 169 |     7/    7 batches | lr 0.000033 | 89.97 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch 170 |     7/    7 batches | lr 0.000032 | 89.81 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch 171 |     7/    7 batches | lr 0.000032 | 90.37 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch 172 |     7/    7 batches | lr 0.000031 | 90.15 ms | loss 0.00189 | ppl     1.00\n",
      "| epoch 173 |     7/    7 batches | lr 0.000030 | 90.04 ms | loss 0.00189 | ppl     1.00\n",
      "| epoch 174 |     7/    7 batches | lr 0.000030 | 90.33 ms | loss 0.00188 | ppl     1.00\n",
      "| epoch 175 |     7/    7 batches | lr 0.000029 | 90.45 ms | loss 0.00189 | ppl     1.00\n",
      "| epoch 176 |     7/    7 batches | lr 0.000029 | 89.73 ms | loss 0.00188 | ppl     1.00\n",
      "| epoch 177 |     7/    7 batches | lr 0.000028 | 90.17 ms | loss 0.00188 | ppl     1.00\n",
      "| epoch 178 |     7/    7 batches | lr 0.000027 | 89.89 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch 179 |     7/    7 batches | lr 0.000027 | 90.33 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch 180 |     7/    7 batches | lr 0.000026 | 90.14 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch 181 |     7/    7 batches | lr 0.000026 | 89.88 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch 182 |     7/    7 batches | lr 0.000025 | 90.23 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch 183 |     7/    7 batches | lr 0.000025 | 90.36 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch 184 |     7/    7 batches | lr 0.000024 | 90.17 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch 185 |     7/    7 batches | lr 0.000024 | 90.16 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch 186 |     7/    7 batches | lr 0.000023 | 89.99 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 187 |     7/    7 batches | lr 0.000023 | 89.93 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch 188 |     7/    7 batches | lr 0.000022 | 90.03 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch 189 |     7/    7 batches | lr 0.000022 | 89.69 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 190 |     7/    7 batches | lr 0.000022 | 90.47 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 191 |     7/    7 batches | lr 0.000021 | 90.01 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 192 |     7/    7 batches | lr 0.000021 | 90.39 ms | loss 0.00184 | ppl     1.00\n",
      "| epoch 193 |     7/    7 batches | lr 0.000020 | 90.26 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 194 |     7/    7 batches | lr 0.000020 | 90.43 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 195 |     7/    7 batches | lr 0.000019 | 90.08 ms | loss 0.00185 | ppl     1.00\n",
      "| epoch 196 |     7/    7 batches | lr 0.000019 | 90.29 ms | loss 0.00184 | ppl     1.00\n",
      "| epoch 197 |     7/    7 batches | lr 0.000019 | 89.89 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 198 |     7/    7 batches | lr 0.000018 | 89.82 ms | loss 0.00184 | ppl     1.00\n",
      "| epoch 199 |     7/    7 batches | lr 0.000018 | 90.27 ms | loss 0.00184 | ppl     1.00\n",
      "| epoch 200 |     7/    7 batches | lr 0.000018 | 90.23 ms | loss 0.00183 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 200 | time:  1.36s | valid loss 0.00032 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 201 |     7/    7 batches | lr 0.000017 | 90.48 ms | loss 0.00184 | ppl     1.00\n",
      "| epoch 202 |     7/    7 batches | lr 0.000017 | 91.05 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 203 |     7/    7 batches | lr 0.000017 | 90.71 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 204 |     7/    7 batches | lr 0.000016 | 91.34 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 205 |     7/    7 batches | lr 0.000016 | 90.44 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 206 |     7/    7 batches | lr 0.000016 | 90.07 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 207 |     7/    7 batches | lr 0.000015 | 90.64 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 208 |     7/    7 batches | lr 0.000015 | 90.23 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 209 |     7/    7 batches | lr 0.000015 | 90.04 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 210 |     7/    7 batches | lr 0.000014 | 89.82 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch 211 |     7/    7 batches | lr 0.000014 | 90.00 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 212 |     7/    7 batches | lr 0.000014 | 90.18 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 213 |     7/    7 batches | lr 0.000014 | 89.71 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 214 |     7/    7 batches | lr 0.000013 | 89.65 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 215 |     7/    7 batches | lr 0.000013 | 89.84 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch 216 |     7/    7 batches | lr 0.000013 | 89.50 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 217 |     7/    7 batches | lr 0.000012 | 89.89 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 218 |     7/    7 batches | lr 0.000012 | 90.29 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 219 |     7/    7 batches | lr 0.000012 | 89.81 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 220 |     7/    7 batches | lr 0.000012 | 89.74 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 221 |     7/    7 batches | lr 0.000012 | 89.65 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 222 |     7/    7 batches | lr 0.000011 | 89.51 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 223 |     7/    7 batches | lr 0.000011 | 89.92 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 224 |     7/    7 batches | lr 0.000011 | 89.69 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 225 |     7/    7 batches | lr 0.000011 | 90.00 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 226 |     7/    7 batches | lr 0.000010 | 89.49 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 227 |     7/    7 batches | lr 0.000010 | 89.87 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 228 |     7/    7 batches | lr 0.000010 | 89.45 ms | loss 0.00181 | ppl     1.00\n",
      "| epoch 229 |     7/    7 batches | lr 0.000010 | 89.77 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 230 |     7/    7 batches | lr 0.000010 | 90.21 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 231 |     7/    7 batches | lr 0.000009 | 90.14 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 232 |     7/    7 batches | lr 0.000009 | 89.88 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 233 |     7/    7 batches | lr 0.000009 | 90.18 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 234 |     7/    7 batches | lr 0.000009 | 90.03 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 235 |     7/    7 batches | lr 0.000009 | 89.50 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 236 |     7/    7 batches | lr 0.000008 | 89.91 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 237 |     7/    7 batches | lr 0.000008 | 89.64 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 238 |     7/    7 batches | lr 0.000008 | 90.26 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 239 |     7/    7 batches | lr 0.000008 | 89.87 ms | loss 0.00180 | ppl     1.00\n",
      "| epoch 240 |     7/    7 batches | lr 0.000008 | 90.24 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 241 |     7/    7 batches | lr 0.000008 | 89.71 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 242 |     7/    7 batches | lr 0.000008 | 89.97 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 243 |     7/    7 batches | lr 0.000007 | 89.82 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 244 |     7/    7 batches | lr 0.000007 | 89.56 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 245 |     7/    7 batches | lr 0.000007 | 89.86 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 246 |     7/    7 batches | lr 0.000007 | 89.61 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 247 |     7/    7 batches | lr 0.000007 | 89.83 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 248 |     7/    7 batches | lr 0.000007 | 89.99 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 249 |     7/    7 batches | lr 0.000007 | 89.64 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 250 |     7/    7 batches | lr 0.000006 | 89.76 ms | loss 0.00178 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 250 | time:  0.67s | valid loss 0.00032 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 251 |     7/    7 batches | lr 0.000006 | 90.13 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch 252 |     7/    7 batches | lr 0.000006 | 90.01 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 253 |     7/    7 batches | lr 0.000006 | 89.94 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 254 |     7/    7 batches | lr 0.000006 | 89.94 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 255 |     7/    7 batches | lr 0.000006 | 89.73 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 256 |     7/    7 batches | lr 0.000006 | 89.54 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 257 |     7/    7 batches | lr 0.000006 | 89.79 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 258 |     7/    7 batches | lr 0.000005 | 90.31 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 259 |     7/    7 batches | lr 0.000005 | 90.05 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 260 |     7/    7 batches | lr 0.000005 | 89.89 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 261 |     7/    7 batches | lr 0.000005 | 90.44 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 262 |     7/    7 batches | lr 0.000005 | 104.54 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 263 |     7/    7 batches | lr 0.000005 | 90.34 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 264 |     7/    7 batches | lr 0.000005 | 90.72 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 265 |     7/    7 batches | lr 0.000005 | 91.03 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 266 |     7/    7 batches | lr 0.000005 | 89.78 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 267 |     7/    7 batches | lr 0.000005 | 89.82 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 268 |     7/    7 batches | lr 0.000004 | 89.46 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 269 |     7/    7 batches | lr 0.000004 | 89.72 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 270 |     7/    7 batches | lr 0.000004 | 90.11 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 271 |     7/    7 batches | lr 0.000004 | 90.00 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 272 |     7/    7 batches | lr 0.000004 | 89.67 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 273 |     7/    7 batches | lr 0.000004 | 89.97 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch 274 |     7/    7 batches | lr 0.000004 | 89.83 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 275 |     7/    7 batches | lr 0.000004 | 90.41 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 276 |     7/    7 batches | lr 0.000004 | 90.54 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 277 |     7/    7 batches | lr 0.000004 | 90.43 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 278 |     7/    7 batches | lr 0.000004 | 89.95 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 279 |     7/    7 batches | lr 0.000004 | 89.90 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 280 |     7/    7 batches | lr 0.000003 | 90.17 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 281 |     7/    7 batches | lr 0.000003 | 90.00 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 282 |     7/    7 batches | lr 0.000003 | 90.28 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 283 |     7/    7 batches | lr 0.000003 | 89.76 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 284 |     7/    7 batches | lr 0.000003 | 89.99 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 285 |     7/    7 batches | lr 0.000003 | 89.52 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 286 |     7/    7 batches | lr 0.000003 | 89.82 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 287 |     7/    7 batches | lr 0.000003 | 89.89 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 288 |     7/    7 batches | lr 0.000003 | 90.01 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 289 |     7/    7 batches | lr 0.000003 | 90.23 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 290 |     7/    7 batches | lr 0.000003 | 89.77 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 291 |     7/    7 batches | lr 0.000003 | 90.12 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 292 |     7/    7 batches | lr 0.000003 | 89.94 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 293 |     7/    7 batches | lr 0.000003 | 90.19 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 294 |     7/    7 batches | lr 0.000003 | 90.51 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 295 |     7/    7 batches | lr 0.000003 | 90.14 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 296 |     7/    7 batches | lr 0.000003 | 90.13 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch 297 |     7/    7 batches | lr 0.000002 | 90.18 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 298 |     7/    7 batches | lr 0.000002 | 90.38 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 299 |     7/    7 batches | lr 0.000002 | 90.09 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch 300 |     7/    7 batches | lr 0.000002 | 90.74 ms | loss 0.00177 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 300 | time:  1.35s | valid loss 0.00033 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = dict()\n",
    "for i in ['open','close','high','low']:\n",
    "    predict,truth = train_and_get(i)\n",
    "    result.update({i:predict})\n",
    "    result.update({i+'_truth':truth})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result.keys():\n",
    "    result[i] = result[i].reshape(-1)\n",
    "output = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('result1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00d1462882258b783cb181eb86adde5698c4f2204c9695f847918151df6a5450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
