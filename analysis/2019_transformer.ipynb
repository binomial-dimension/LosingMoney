{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from EarlyStopping import EarlyStopping\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E)\n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "\n",
    "input_window = 120  # number of input steps\n",
    "output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "batch_size = 512\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=7000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self, feature_size=300, num_layers=3, dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size,\n",
    "                                                        nhead=10,\n",
    "                                                        dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer,\n",
    "                                                         num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(feature_size, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,\n",
    "                                          self.src_mask)  #, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(\n",
    "            mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - tw):\n",
    "        train_seq = input_data[i:i + tw]\n",
    "        train_label = input_data[i + output_window:i + tw + output_window]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return torch.FloatTensor(np.array(inout_seq))\n",
    "\n",
    "\n",
    "def get_data(name):\n",
    "    # construct a littel toy dataset\n",
    "    time = np.arange(0, 400, 0.1)\n",
    "    data = pd.read_csv('../data/hs300.csv')\n",
    "    amplitude = data[name].values\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    #loading weather data from a file\n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "\n",
    "    # looks like normalizing input values curtial for the model\n",
    "    scaler = StandardScaler()\n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    train_data = amplitude[:-212]\n",
    "    test_data = amplitude[-212:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment..\n",
    "    train_sequence = create_inout_sequences(train_data, input_window)\n",
    "    train_sequence = train_sequence[:\n",
    "                                    -output_window]  #todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1)\n",
    "    test_data = create_inout_sequences(test_data, input_window)\n",
    "    test_data = test_data[:-output_window]  #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device), test_data.to(device),scaler\n",
    "\n",
    "\n",
    "def get_batch(source, i, batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    input = torch.stack(\n",
    "        torch.stack([item[0] for item in data]).chunk(input_window,1))  # 1 is feature size\n",
    "    target = torch.stack(\n",
    "        torch.stack([item[1] for item in data]).chunk(input_window, 1))\n",
    "    return input, target\n",
    "\n",
    "\n",
    "def train(train_data,epoch,train_losses):\n",
    "    model.train()  # Turn on the train mode \\o/\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data) - 1, batch_size)):\n",
    "        data, targets = get_batch(train_data, i, batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.8)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "def plot_and_loss(eval_model, data_source, epoch):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            output = eval_model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                    0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "    #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "    len(test_result)\n",
    "\n",
    "    plt.plot(test_result, color=\"red\", label=\"prediction\")\n",
    "    plt.plot(truth[:len(test_result)], color=\"blue\", label=\"truth\")\n",
    "    plt.plot(test_result - truth, color=\"green\", label=\"error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-epoch%d.png' % epoch)\n",
    "    plt.close()\n",
    "\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "# predict the next n steps based on the input data\n",
    "def predict_future(eval_model, data_source, steps):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source, 0, 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):\n",
    "            output = eval_model(data[-input_window:])\n",
    "            data = torch.cat((data, output[-1:]))\n",
    "\n",
    "    data = data.cpu().view(-1)\n",
    "\n",
    "    # I used this plot to visualize if the model pics up any long therm structure within the data.\n",
    "    plt.plot(data, color=\"red\")\n",
    "    plt.plot(data[:input_window], color=\"blue\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-future%d.png' % steps)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval()  # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 500\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1, eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i, eval_batch_size)\n",
    "            output = eval_model(data)\n",
    "            total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "    return total_loss / len(data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_get(name):\n",
    "    input_window = 120  # number of input steps\n",
    "    output_window = 1  # number of prediction steps, in this model its fixed to one\n",
    "    batch_size = 800\n",
    "    device = torch.device(\"cuda\")\n",
    "    train_data, val_data, scaler = get_data(name)\n",
    "    global model \n",
    "    model = TransAm().to(device)\n",
    "\n",
    "    global criterion\n",
    "    criterion = nn.MSELoss()\n",
    "    lr = 0.02  # learning rate\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    global optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    global scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.99)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, verbose=True,factor=0.7)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    global epochs\n",
    "    epochs = 500  # The number of epochs\n",
    "    best_model = None\n",
    "\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=50, verbose=True)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(train_data,epoch,train_losses)\n",
    "\n",
    "        if (epoch % 100 == 0):\n",
    "            val_loss = plot_and_loss(model, val_data, epoch)\n",
    "            predict_future(model, val_data, 5)\n",
    "        else:\n",
    "            val_loss = evaluate(model, val_data)\n",
    "\n",
    "        valid_losses.append(val_loss)\n",
    "\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(epochs))\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        early_stopping(valid_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "    \n",
    "    def get_predict(eval_model, data_source):\n",
    "        eval_model.eval()\n",
    "        total_loss = 0.\n",
    "        test_result = torch.Tensor(0)\n",
    "        truth = torch.Tensor(0)\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(data_source) - 1):\n",
    "                data, target = get_batch(data_source, i, 1)\n",
    "                output = eval_model(data)\n",
    "                total_loss += torch.sqrt(criterion(output, target)).item()\n",
    "                test_result = torch.cat((test_result, output[-1].view(-1).cpu()),\n",
    "                                        0)\n",
    "                truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "        #test_result = test_result.cpu().numpy() -> no need to detach stuff..\n",
    "        return scaler.inverse_transform(test_result.view(-1, 1)), scaler.inverse_transform(truth.view(-1, 1))\n",
    "\n",
    "    \n",
    "    return get_predict(model, val_data)\n",
    "\n",
    "def get_truth(name):\n",
    "    train_data, val_data, scaler = get_data(name)\n",
    "    data_source = val_data\n",
    "    test_result = torch.Tensor(0)\n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i, 1)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "\n",
    "    return scaler.inverse_transform(truth.view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/500] train_loss: 1.20790 valid_loss: 0.97378\n",
      "Validation loss decreased (inf --> 0.973782).  Saving model ...\n",
      "[  2/500] train_loss: 1.09650 valid_loss: 0.98541\n",
      "[  3/500] train_loss: 1.06403 valid_loss: 0.96639\n",
      "Validation loss decreased (0.973782 --> 0.966394).  Saving model ...\n",
      "[  4/500] train_loss: 1.04754 valid_loss: 0.95233\n",
      "Validation loss decreased (0.966394 --> 0.952328).  Saving model ...\n",
      "[  5/500] train_loss: 1.04340 valid_loss: 0.94214\n",
      "Validation loss decreased (0.952328 --> 0.942136).  Saving model ...\n",
      "[  6/500] train_loss: 1.03840 valid_loss: 0.92965\n",
      "Validation loss decreased (0.942136 --> 0.929652).  Saving model ...\n",
      "[  7/500] train_loss: 1.01555 valid_loss: 0.92146\n",
      "Validation loss decreased (0.929652 --> 0.921460).  Saving model ...\n",
      "[  8/500] train_loss: 1.01462 valid_loss: 0.91344\n",
      "Validation loss decreased (0.921460 --> 0.913437).  Saving model ...\n",
      "[  9/500] train_loss: 1.00072 valid_loss: 0.90688\n",
      "Validation loss decreased (0.913437 --> 0.906877).  Saving model ...\n",
      "[ 10/500] train_loss: 0.98739 valid_loss: 0.90281\n",
      "Validation loss decreased (0.906877 --> 0.902805).  Saving model ...\n",
      "[ 11/500] train_loss: 0.98179 valid_loss: 0.89633\n",
      "Validation loss decreased (0.902805 --> 0.896330).  Saving model ...\n",
      "[ 12/500] train_loss: 0.98557 valid_loss: 0.89207\n",
      "Validation loss decreased (0.896330 --> 0.892073).  Saving model ...\n",
      "[ 13/500] train_loss: 0.98403 valid_loss: 0.88708\n",
      "Validation loss decreased (0.892073 --> 0.887077).  Saving model ...\n",
      "[ 14/500] train_loss: 0.97483 valid_loss: 0.87937\n",
      "Validation loss decreased (0.887077 --> 0.879369).  Saving model ...\n",
      "[ 15/500] train_loss: 0.96616 valid_loss: 0.87518\n",
      "Validation loss decreased (0.879369 --> 0.875179).  Saving model ...\n",
      "[ 16/500] train_loss: 0.95865 valid_loss: 0.87080\n",
      "Validation loss decreased (0.875179 --> 0.870795).  Saving model ...\n",
      "[ 17/500] train_loss: 0.96127 valid_loss: 0.86870\n",
      "Validation loss decreased (0.870795 --> 0.868701).  Saving model ...\n",
      "[ 18/500] train_loss: 0.95431 valid_loss: 0.86506\n",
      "Validation loss decreased (0.868701 --> 0.865058).  Saving model ...\n",
      "[ 19/500] train_loss: 0.95299 valid_loss: 0.86147\n",
      "Validation loss decreased (0.865058 --> 0.861475).  Saving model ...\n",
      "[ 20/500] train_loss: 0.94825 valid_loss: 0.85869\n",
      "Validation loss decreased (0.861475 --> 0.858692).  Saving model ...\n",
      "[ 21/500] train_loss: 0.94670 valid_loss: 0.85628\n",
      "Validation loss decreased (0.858692 --> 0.856277).  Saving model ...\n",
      "[ 22/500] train_loss: 0.94974 valid_loss: 0.85198\n",
      "Validation loss decreased (0.856277 --> 0.851980).  Saving model ...\n",
      "[ 23/500] train_loss: 0.94873 valid_loss: 0.84942\n",
      "Validation loss decreased (0.851980 --> 0.849417).  Saving model ...\n",
      "[ 24/500] train_loss: 0.94674 valid_loss: 0.84887\n",
      "Validation loss decreased (0.849417 --> 0.848869).  Saving model ...\n",
      "[ 25/500] train_loss: 0.93718 valid_loss: 0.84598\n",
      "Validation loss decreased (0.848869 --> 0.845979).  Saving model ...\n",
      "[ 26/500] train_loss: 0.93630 valid_loss: 0.84481\n",
      "Validation loss decreased (0.845979 --> 0.844811).  Saving model ...\n",
      "[ 27/500] train_loss: 0.93804 valid_loss: 0.84294\n",
      "Validation loss decreased (0.844811 --> 0.842938).  Saving model ...\n",
      "[ 28/500] train_loss: 0.93117 valid_loss: 0.84110\n",
      "Validation loss decreased (0.842938 --> 0.841098).  Saving model ...\n",
      "[ 29/500] train_loss: 0.92959 valid_loss: 0.83910\n",
      "Validation loss decreased (0.841098 --> 0.839101).  Saving model ...\n",
      "[ 30/500] train_loss: 0.93815 valid_loss: 0.83820\n",
      "Validation loss decreased (0.839101 --> 0.838204).  Saving model ...\n",
      "[ 31/500] train_loss: 0.92975 valid_loss: 0.83551\n",
      "Validation loss decreased (0.838204 --> 0.835511).  Saving model ...\n",
      "[ 32/500] train_loss: 0.93253 valid_loss: 0.83378\n",
      "Validation loss decreased (0.835511 --> 0.833783).  Saving model ...\n",
      "[ 33/500] train_loss: 0.92909 valid_loss: 0.83249\n",
      "Validation loss decreased (0.833783 --> 0.832491).  Saving model ...\n",
      "[ 34/500] train_loss: 0.92722 valid_loss: 0.83210\n",
      "Validation loss decreased (0.832491 --> 0.832096).  Saving model ...\n",
      "[ 35/500] train_loss: 0.92243 valid_loss: 0.82943\n",
      "Validation loss decreased (0.832096 --> 0.829432).  Saving model ...\n",
      "[ 36/500] train_loss: 0.92598 valid_loss: 0.82795\n",
      "Validation loss decreased (0.829432 --> 0.827948).  Saving model ...\n",
      "[ 37/500] train_loss: 0.92915 valid_loss: 0.82736\n",
      "Validation loss decreased (0.827948 --> 0.827355).  Saving model ...\n",
      "[ 38/500] train_loss: 0.92105 valid_loss: 0.82623\n",
      "Validation loss decreased (0.827355 --> 0.826227).  Saving model ...\n",
      "[ 39/500] train_loss: 0.92096 valid_loss: 0.82605\n",
      "Validation loss decreased (0.826227 --> 0.826053).  Saving model ...\n",
      "[ 40/500] train_loss: 0.92316 valid_loss: 0.82478\n",
      "Validation loss decreased (0.826053 --> 0.824777).  Saving model ...\n",
      "[ 41/500] train_loss: 0.91982 valid_loss: 0.82429\n",
      "Validation loss decreased (0.824777 --> 0.824285).  Saving model ...\n",
      "[ 42/500] train_loss: 0.91603 valid_loss: 0.82283\n",
      "Validation loss decreased (0.824285 --> 0.822832).  Saving model ...\n",
      "[ 43/500] train_loss: 0.91401 valid_loss: 0.82236\n",
      "Validation loss decreased (0.822832 --> 0.822363).  Saving model ...\n",
      "[ 44/500] train_loss: 0.91990 valid_loss: 0.82135\n",
      "Validation loss decreased (0.822363 --> 0.821354).  Saving model ...\n",
      "[ 45/500] train_loss: 0.91925 valid_loss: 0.81987\n",
      "Validation loss decreased (0.821354 --> 0.819873).  Saving model ...\n",
      "[ 46/500] train_loss: 0.91830 valid_loss: 0.82036\n",
      "[ 47/500] train_loss: 0.91278 valid_loss: 0.81949\n",
      "Validation loss decreased (0.819873 --> 0.819490).  Saving model ...\n",
      "[ 48/500] train_loss: 0.92136 valid_loss: 0.81931\n",
      "Validation loss decreased (0.819490 --> 0.819309).  Saving model ...\n",
      "[ 49/500] train_loss: 0.91739 valid_loss: 0.81884\n",
      "Validation loss decreased (0.819309 --> 0.818839).  Saving model ...\n",
      "[ 50/500] train_loss: 0.91403 valid_loss: 0.81780\n",
      "Validation loss decreased (0.818839 --> 0.817800).  Saving model ...\n",
      "[ 51/500] train_loss: 0.91631 valid_loss: 0.81773\n",
      "Validation loss decreased (0.817800 --> 0.817729).  Saving model ...\n",
      "[ 52/500] train_loss: 0.91180 valid_loss: 0.81724\n",
      "Validation loss decreased (0.817729 --> 0.817237).  Saving model ...\n",
      "[ 53/500] train_loss: 0.91194 valid_loss: 0.81679\n",
      "Validation loss decreased (0.817237 --> 0.816791).  Saving model ...\n",
      "[ 54/500] train_loss: 0.91578 valid_loss: 0.81586\n",
      "Validation loss decreased (0.816791 --> 0.815863).  Saving model ...\n",
      "[ 55/500] train_loss: 0.91126 valid_loss: 0.81562\n",
      "Validation loss decreased (0.815863 --> 0.815622).  Saving model ...\n",
      "[ 56/500] train_loss: 0.91493 valid_loss: 0.81524\n",
      "Validation loss decreased (0.815622 --> 0.815240).  Saving model ...\n",
      "[ 57/500] train_loss: 0.91221 valid_loss: 0.81502\n",
      "Validation loss decreased (0.815240 --> 0.815017).  Saving model ...\n",
      "[ 58/500] train_loss: 0.90682 valid_loss: 0.81398\n",
      "Validation loss decreased (0.815017 --> 0.813981).  Saving model ...\n",
      "[ 59/500] train_loss: 0.91396 valid_loss: 0.81345\n",
      "Validation loss decreased (0.813981 --> 0.813447).  Saving model ...\n",
      "[ 60/500] train_loss: 0.90912 valid_loss: 0.81372\n",
      "[ 61/500] train_loss: 0.90991 valid_loss: 0.81349\n",
      "[ 62/500] train_loss: 0.91232 valid_loss: 0.81297\n",
      "Validation loss decreased (0.813447 --> 0.812968).  Saving model ...\n",
      "[ 63/500] train_loss: 0.91073 valid_loss: 0.81223\n",
      "Validation loss decreased (0.812968 --> 0.812229).  Saving model ...\n",
      "[ 64/500] train_loss: 0.90905 valid_loss: 0.81242\n",
      "[ 65/500] train_loss: 0.91269 valid_loss: 0.81232\n",
      "[ 66/500] train_loss: 0.91684 valid_loss: 0.81258\n",
      "[ 67/500] train_loss: 0.90820 valid_loss: 0.81195\n",
      "Validation loss decreased (0.812229 --> 0.811951).  Saving model ...\n",
      "[ 68/500] train_loss: 0.90734 valid_loss: 0.81187\n",
      "Validation loss decreased (0.811951 --> 0.811869).  Saving model ...\n",
      "[ 69/500] train_loss: 0.90826 valid_loss: 0.81113\n",
      "Validation loss decreased (0.811869 --> 0.811133).  Saving model ...\n",
      "[ 70/500] train_loss: 0.90456 valid_loss: 0.81101\n",
      "Validation loss decreased (0.811133 --> 0.811011).  Saving model ...\n",
      "[ 71/500] train_loss: 0.90972 valid_loss: 0.81052\n",
      "Validation loss decreased (0.811011 --> 0.810521).  Saving model ...\n",
      "[ 72/500] train_loss: 0.90475 valid_loss: 0.81017\n",
      "Validation loss decreased (0.810521 --> 0.810167).  Saving model ...\n",
      "[ 73/500] train_loss: 0.91156 valid_loss: 0.80975\n",
      "Validation loss decreased (0.810167 --> 0.809748).  Saving model ...\n",
      "[ 74/500] train_loss: 0.90482 valid_loss: 0.80961\n",
      "Validation loss decreased (0.809748 --> 0.809607).  Saving model ...\n",
      "[ 75/500] train_loss: 0.90388 valid_loss: 0.80944\n",
      "Validation loss decreased (0.809607 --> 0.809439).  Saving model ...\n",
      "[ 76/500] train_loss: 0.90812 valid_loss: 0.81051\n",
      "[ 77/500] train_loss: 0.90804 valid_loss: 0.81073\n",
      "[ 78/500] train_loss: 0.90177 valid_loss: 0.81034\n",
      "[ 79/500] train_loss: 0.90692 valid_loss: 0.80974\n",
      "[ 80/500] train_loss: 0.90172 valid_loss: 0.81050\n",
      "[ 81/500] train_loss: 0.90182 valid_loss: 0.80940\n",
      "Validation loss decreased (0.809439 --> 0.809404).  Saving model ...\n",
      "[ 82/500] train_loss: 0.90030 valid_loss: 0.80944\n",
      "[ 83/500] train_loss: 0.90765 valid_loss: 0.80954\n",
      "[ 84/500] train_loss: 0.90195 valid_loss: 0.80931\n",
      "Validation loss decreased (0.809404 --> 0.809315).  Saving model ...\n",
      "[ 85/500] train_loss: 0.89544 valid_loss: 0.80843\n",
      "Validation loss decreased (0.809315 --> 0.808434).  Saving model ...\n",
      "[ 86/500] train_loss: 0.90212 valid_loss: 0.80871\n",
      "[ 87/500] train_loss: 0.89708 valid_loss: 0.80898\n",
      "[ 88/500] train_loss: 0.90233 valid_loss: 0.80870\n",
      "[ 89/500] train_loss: 0.89923 valid_loss: 0.80836\n",
      "Validation loss decreased (0.808434 --> 0.808358).  Saving model ...\n",
      "[ 90/500] train_loss: 0.89632 valid_loss: 0.80903\n",
      "[ 91/500] train_loss: 0.89198 valid_loss: 0.80863\n",
      "[ 92/500] train_loss: 0.89645 valid_loss: 0.80922\n",
      "[ 93/500] train_loss: 0.89826 valid_loss: 0.80876\n",
      "[ 94/500] train_loss: 0.90125 valid_loss: 0.80870\n",
      "[ 95/500] train_loss: 0.89656 valid_loss: 0.80854\n",
      "[ 96/500] train_loss: 0.90069 valid_loss: 0.80805\n",
      "Validation loss decreased (0.808358 --> 0.808048).  Saving model ...\n",
      "[ 97/500] train_loss: 0.89519 valid_loss: 0.80788\n",
      "Validation loss decreased (0.808048 --> 0.807881).  Saving model ...\n",
      "[ 98/500] train_loss: 0.89166 valid_loss: 0.80810\n",
      "[ 99/500] train_loss: 0.89380 valid_loss: 0.80795\n",
      "[100/500] train_loss: 0.89035 valid_loss: 0.83436\n",
      "[101/500] train_loss: 0.89823 valid_loss: 0.80758\n",
      "Validation loss decreased (0.807881 --> 0.807582).  Saving model ...\n",
      "[102/500] train_loss: 0.89653 valid_loss: 0.80755\n",
      "Validation loss decreased (0.807582 --> 0.807553).  Saving model ...\n",
      "[103/500] train_loss: 0.88997 valid_loss: 0.80714\n",
      "Validation loss decreased (0.807553 --> 0.807138).  Saving model ...\n",
      "[104/500] train_loss: 0.89107 valid_loss: 0.80834\n",
      "[105/500] train_loss: 0.89049 valid_loss: 0.80720\n",
      "[106/500] train_loss: 0.89148 valid_loss: 0.80768\n",
      "[107/500] train_loss: 0.88926 valid_loss: 0.80754\n",
      "[108/500] train_loss: 0.89035 valid_loss: 0.80747\n",
      "[109/500] train_loss: 0.89483 valid_loss: 0.80834\n",
      "[110/500] train_loss: 0.89434 valid_loss: 0.80805\n",
      "[111/500] train_loss: 0.89078 valid_loss: 0.80784\n",
      "[112/500] train_loss: 0.89301 valid_loss: 0.80794\n",
      "[113/500] train_loss: 0.89276 valid_loss: 0.80782\n",
      "[114/500] train_loss: 0.88656 valid_loss: 0.80769\n",
      "Epoch 00114: reducing learning rate of group 0 to 1.4000e-02.\n",
      "[115/500] train_loss: 0.86711 valid_loss: 0.79796\n",
      "Validation loss decreased (0.807138 --> 0.797960).  Saving model ...\n",
      "[116/500] train_loss: 0.87048 valid_loss: 0.80394\n",
      "[117/500] train_loss: 0.87037 valid_loss: 0.79938\n",
      "[118/500] train_loss: 0.86859 valid_loss: 0.80214\n",
      "[119/500] train_loss: 0.86931 valid_loss: 0.79991\n",
      "[120/500] train_loss: 0.86832 valid_loss: 0.80145\n",
      "[121/500] train_loss: 0.86789 valid_loss: 0.80082\n",
      "[122/500] train_loss: 0.86921 valid_loss: 0.80048\n",
      "[123/500] train_loss: 0.87352 valid_loss: 0.80105\n",
      "[124/500] train_loss: 0.86920 valid_loss: 0.80010\n",
      "[125/500] train_loss: 0.87080 valid_loss: 0.80190\n",
      "[126/500] train_loss: 0.86639 valid_loss: 0.80125\n",
      "Epoch 00126: reducing learning rate of group 0 to 9.8000e-03.\n",
      "[127/500] train_loss: 0.85796 valid_loss: 0.79088\n",
      "Validation loss decreased (0.797960 --> 0.790878).  Saving model ...\n",
      "[128/500] train_loss: 0.87677 valid_loss: 0.79225\n",
      "[129/500] train_loss: 0.87940 valid_loss: 0.79290\n",
      "[130/500] train_loss: 0.87290 valid_loss: 0.79324\n",
      "[131/500] train_loss: 0.87293 valid_loss: 0.79339\n",
      "[132/500] train_loss: 0.87134 valid_loss: 0.79311\n",
      "[133/500] train_loss: 0.87118 valid_loss: 0.79308\n",
      "[134/500] train_loss: 0.87333 valid_loss: 0.79296\n",
      "[135/500] train_loss: 0.87182 valid_loss: 0.79296\n",
      "[136/500] train_loss: 0.87045 valid_loss: 0.79276\n",
      "[137/500] train_loss: 0.86998 valid_loss: 0.79281\n",
      "[138/500] train_loss: 0.86841 valid_loss: 0.79268\n",
      "Epoch 00138: reducing learning rate of group 0 to 6.8600e-03.\n",
      "[139/500] train_loss: 0.86942 valid_loss: 0.79194\n",
      "[140/500] train_loss: 0.86910 valid_loss: 0.79131\n",
      "[141/500] train_loss: 0.86482 valid_loss: 0.79123\n",
      "[142/500] train_loss: 0.86404 valid_loss: 0.79113\n",
      "[143/500] train_loss: 0.86517 valid_loss: 0.79103\n",
      "[144/500] train_loss: 0.86586 valid_loss: 0.79098\n",
      "[145/500] train_loss: 0.86601 valid_loss: 0.79082\n",
      "Validation loss decreased (0.790878 --> 0.790815).  Saving model ...\n",
      "[146/500] train_loss: 0.86224 valid_loss: 0.79082\n",
      "[147/500] train_loss: 0.86370 valid_loss: 0.79073\n",
      "Validation loss decreased (0.790815 --> 0.790734).  Saving model ...\n",
      "[148/500] train_loss: 0.85918 valid_loss: 0.79070\n",
      "Validation loss decreased (0.790734 --> 0.790695).  Saving model ...\n",
      "[149/500] train_loss: 0.86832 valid_loss: 0.79042\n",
      "Validation loss decreased (0.790695 --> 0.790422).  Saving model ...\n",
      "[150/500] train_loss: 0.86706 valid_loss: 0.79036\n",
      "Validation loss decreased (0.790422 --> 0.790356).  Saving model ...\n",
      "[151/500] train_loss: 0.86546 valid_loss: 0.79019\n",
      "Validation loss decreased (0.790356 --> 0.790193).  Saving model ...\n",
      "[152/500] train_loss: 0.86574 valid_loss: 0.79024\n",
      "[153/500] train_loss: 0.86246 valid_loss: 0.79021\n",
      "[154/500] train_loss: 0.86476 valid_loss: 0.79028\n",
      "[155/500] train_loss: 0.86316 valid_loss: 0.79016\n",
      "Validation loss decreased (0.790193 --> 0.790161).  Saving model ...\n",
      "[156/500] train_loss: 0.86264 valid_loss: 0.78990\n",
      "Validation loss decreased (0.790161 --> 0.789898).  Saving model ...\n",
      "[157/500] train_loss: 0.86282 valid_loss: 0.78985\n",
      "Validation loss decreased (0.789898 --> 0.789845).  Saving model ...\n",
      "[158/500] train_loss: 0.85927 valid_loss: 0.78992\n",
      "[159/500] train_loss: 0.86350 valid_loss: 0.78985\n",
      "[160/500] train_loss: 0.86849 valid_loss: 0.78997\n",
      "[161/500] train_loss: 0.85733 valid_loss: 0.79002\n",
      "[162/500] train_loss: 0.86329 valid_loss: 0.78988\n",
      "[163/500] train_loss: 0.86454 valid_loss: 0.78990\n",
      "[164/500] train_loss: 0.85874 valid_loss: 0.78970\n",
      "Validation loss decreased (0.789845 --> 0.789696).  Saving model ...\n",
      "[165/500] train_loss: 0.86418 valid_loss: 0.78977\n",
      "[166/500] train_loss: 0.86219 valid_loss: 0.78971\n",
      "[167/500] train_loss: 0.86644 valid_loss: 0.78969\n",
      "Validation loss decreased (0.789696 --> 0.789690).  Saving model ...\n",
      "[168/500] train_loss: 0.85768 valid_loss: 0.78954\n",
      "Validation loss decreased (0.789690 --> 0.789539).  Saving model ...\n",
      "[169/500] train_loss: 0.86466 valid_loss: 0.78957\n",
      "[170/500] train_loss: 0.86139 valid_loss: 0.78945\n",
      "Validation loss decreased (0.789539 --> 0.789447).  Saving model ...\n",
      "[171/500] train_loss: 0.86644 valid_loss: 0.78944\n",
      "Validation loss decreased (0.789447 --> 0.789440).  Saving model ...\n",
      "[172/500] train_loss: 0.86521 valid_loss: 0.78958\n",
      "[173/500] train_loss: 0.85943 valid_loss: 0.78948\n",
      "[174/500] train_loss: 0.86272 valid_loss: 0.78933\n",
      "Validation loss decreased (0.789440 --> 0.789327).  Saving model ...\n",
      "[175/500] train_loss: 0.86206 valid_loss: 0.78920\n",
      "Validation loss decreased (0.789327 --> 0.789197).  Saving model ...\n",
      "[176/500] train_loss: 0.86257 valid_loss: 0.78926\n",
      "[177/500] train_loss: 0.86412 valid_loss: 0.78920\n",
      "[178/500] train_loss: 0.86266 valid_loss: 0.78931\n",
      "[179/500] train_loss: 0.86319 valid_loss: 0.78923\n",
      "[180/500] train_loss: 0.86427 valid_loss: 0.78930\n",
      "[181/500] train_loss: 0.86637 valid_loss: 0.78921\n",
      "[182/500] train_loss: 0.86105 valid_loss: 0.78919\n",
      "Validation loss decreased (0.789197 --> 0.789194).  Saving model ...\n",
      "[183/500] train_loss: 0.86089 valid_loss: 0.78937\n",
      "[184/500] train_loss: 0.85981 valid_loss: 0.78909\n",
      "Validation loss decreased (0.789194 --> 0.789086).  Saving model ...\n",
      "[185/500] train_loss: 0.86119 valid_loss: 0.78907\n",
      "Validation loss decreased (0.789086 --> 0.789074).  Saving model ...\n",
      "[186/500] train_loss: 0.85758 valid_loss: 0.78908\n",
      "[187/500] train_loss: 0.86528 valid_loss: 0.78895\n",
      "Validation loss decreased (0.789074 --> 0.788950).  Saving model ...\n",
      "[188/500] train_loss: 0.86545 valid_loss: 0.78893\n",
      "Validation loss decreased (0.788950 --> 0.788926).  Saving model ...\n",
      "[189/500] train_loss: 0.86162 valid_loss: 0.78894\n",
      "[190/500] train_loss: 0.85996 valid_loss: 0.78909\n",
      "[191/500] train_loss: 0.86337 valid_loss: 0.78894\n",
      "[192/500] train_loss: 0.86591 valid_loss: 0.78896\n",
      "[193/500] train_loss: 0.86488 valid_loss: 0.78890\n",
      "Validation loss decreased (0.788926 --> 0.788902).  Saving model ...\n",
      "[194/500] train_loss: 0.86161 valid_loss: 0.78872\n",
      "Validation loss decreased (0.788902 --> 0.788724).  Saving model ...\n",
      "[195/500] train_loss: 0.86289 valid_loss: 0.78871\n",
      "Validation loss decreased (0.788724 --> 0.788708).  Saving model ...\n",
      "[196/500] train_loss: 0.86226 valid_loss: 0.78874\n",
      "[197/500] train_loss: 0.85885 valid_loss: 0.78883\n",
      "[198/500] train_loss: 0.86057 valid_loss: 0.78861\n",
      "Validation loss decreased (0.788708 --> 0.788611).  Saving model ...\n",
      "[199/500] train_loss: 0.86071 valid_loss: 0.78891\n",
      "[200/500] train_loss: 0.86187 valid_loss: 0.81418\n",
      "[201/500] train_loss: 0.85958 valid_loss: 0.78871\n",
      "[202/500] train_loss: 0.85548 valid_loss: 0.78858\n",
      "Validation loss decreased (0.788611 --> 0.788579).  Saving model ...\n",
      "[203/500] train_loss: 0.85696 valid_loss: 0.78852\n",
      "Validation loss decreased (0.788579 --> 0.788517).  Saving model ...\n",
      "[204/500] train_loss: 0.85499 valid_loss: 0.78871\n",
      "[205/500] train_loss: 0.86528 valid_loss: 0.78862\n",
      "[206/500] train_loss: 0.86249 valid_loss: 0.78852\n",
      "[207/500] train_loss: 0.85645 valid_loss: 0.78841\n",
      "Validation loss decreased (0.788517 --> 0.788414).  Saving model ...\n",
      "[208/500] train_loss: 0.86177 valid_loss: 0.78837\n",
      "Validation loss decreased (0.788414 --> 0.788371).  Saving model ...\n",
      "[209/500] train_loss: 0.86401 valid_loss: 0.78816\n",
      "Validation loss decreased (0.788371 --> 0.788160).  Saving model ...\n",
      "[210/500] train_loss: 0.86049 valid_loss: 0.78837\n",
      "[211/500] train_loss: 0.86152 valid_loss: 0.78858\n",
      "[212/500] train_loss: 0.86087 valid_loss: 0.78851\n",
      "[213/500] train_loss: 0.86148 valid_loss: 0.78856\n",
      "[214/500] train_loss: 0.85909 valid_loss: 0.78861\n",
      "[215/500] train_loss: 0.85941 valid_loss: 0.78865\n",
      "[216/500] train_loss: 0.85880 valid_loss: 0.78864\n",
      "[217/500] train_loss: 0.86264 valid_loss: 0.78861\n",
      "[218/500] train_loss: 0.86065 valid_loss: 0.78853\n",
      "[219/500] train_loss: 0.85983 valid_loss: 0.78861\n",
      "[220/500] train_loss: 0.86084 valid_loss: 0.78859\n",
      "Epoch 00220: reducing learning rate of group 0 to 4.8020e-03.\n",
      "[221/500] train_loss: 0.85300 valid_loss: 0.78827\n",
      "[222/500] train_loss: 0.85221 valid_loss: 0.78807\n",
      "Validation loss decreased (0.788160 --> 0.788073).  Saving model ...\n",
      "[223/500] train_loss: 0.85327 valid_loss: 0.78806\n",
      "Validation loss decreased (0.788073 --> 0.788058).  Saving model ...\n",
      "[224/500] train_loss: 0.85619 valid_loss: 0.78814\n",
      "[225/500] train_loss: 0.85603 valid_loss: 0.78808\n",
      "[226/500] train_loss: 0.85550 valid_loss: 0.78823\n",
      "[227/500] train_loss: 0.85925 valid_loss: 0.78811\n",
      "[228/500] train_loss: 0.85650 valid_loss: 0.78820\n",
      "[229/500] train_loss: 0.85756 valid_loss: 0.78816\n",
      "[230/500] train_loss: 0.85891 valid_loss: 0.78798\n",
      "Validation loss decreased (0.788058 --> 0.787977).  Saving model ...\n",
      "[231/500] train_loss: 0.85475 valid_loss: 0.78798\n",
      "[232/500] train_loss: 0.85847 valid_loss: 0.78764\n",
      "Validation loss decreased (0.787977 --> 0.787644).  Saving model ...\n",
      "[233/500] train_loss: 0.85800 valid_loss: 0.78768\n",
      "[234/500] train_loss: 0.85479 valid_loss: 0.78766\n",
      "[235/500] train_loss: 0.85838 valid_loss: 0.78785\n",
      "[236/500] train_loss: 0.85746 valid_loss: 0.78794\n",
      "[237/500] train_loss: 0.85352 valid_loss: 0.78767\n",
      "[238/500] train_loss: 0.86076 valid_loss: 0.78768\n",
      "[239/500] train_loss: 0.85714 valid_loss: 0.78765\n",
      "[240/500] train_loss: 0.85511 valid_loss: 0.78766\n",
      "[241/500] train_loss: 0.85629 valid_loss: 0.78770\n",
      "[242/500] train_loss: 0.85472 valid_loss: 0.78766\n",
      "[243/500] train_loss: 0.85153 valid_loss: 0.78769\n",
      "Epoch 00243: reducing learning rate of group 0 to 3.3614e-03.\n",
      "[244/500] train_loss: 0.85670 valid_loss: 0.78766\n",
      "[245/500] train_loss: 0.85567 valid_loss: 0.78763\n",
      "Validation loss decreased (0.787644 --> 0.787631).  Saving model ...\n",
      "[246/500] train_loss: 0.85489 valid_loss: 0.78759\n",
      "Validation loss decreased (0.787631 --> 0.787589).  Saving model ...\n",
      "[247/500] train_loss: 0.85182 valid_loss: 0.78756\n",
      "Validation loss decreased (0.787589 --> 0.787559).  Saving model ...\n",
      "[248/500] train_loss: 0.85712 valid_loss: 0.78762\n",
      "[249/500] train_loss: 0.85630 valid_loss: 0.78768\n",
      "[250/500] train_loss: 0.85358 valid_loss: 0.78758\n",
      "[251/500] train_loss: 0.85163 valid_loss: 0.78760\n",
      "[252/500] train_loss: 0.85365 valid_loss: 0.78766\n",
      "[253/500] train_loss: 0.85091 valid_loss: 0.78775\n",
      "[254/500] train_loss: 0.85393 valid_loss: 0.78774\n",
      "[255/500] train_loss: 0.85217 valid_loss: 0.78757\n",
      "[256/500] train_loss: 0.85621 valid_loss: 0.78760\n",
      "[257/500] train_loss: 0.85418 valid_loss: 0.78751\n",
      "Validation loss decreased (0.787559 --> 0.787510).  Saving model ...\n",
      "[258/500] train_loss: 0.85226 valid_loss: 0.78757\n",
      "Epoch 00258: reducing learning rate of group 0 to 2.3530e-03.\n",
      "[259/500] train_loss: 0.85349 valid_loss: 0.78749\n",
      "Validation loss decreased (0.787510 --> 0.787486).  Saving model ...\n",
      "[260/500] train_loss: 0.85276 valid_loss: 0.78760\n",
      "[261/500] train_loss: 0.85312 valid_loss: 0.78770\n",
      "[262/500] train_loss: 0.85235 valid_loss: 0.78768\n",
      "[263/500] train_loss: 0.85244 valid_loss: 0.78777\n",
      "[264/500] train_loss: 0.85267 valid_loss: 0.78779\n",
      "[265/500] train_loss: 0.85308 valid_loss: 0.78780\n",
      "[266/500] train_loss: 0.85221 valid_loss: 0.78792\n",
      "[267/500] train_loss: 0.85094 valid_loss: 0.78794\n",
      "[268/500] train_loss: 0.85256 valid_loss: 0.78794\n",
      "[269/500] train_loss: 0.85095 valid_loss: 0.78790\n",
      "Epoch 00269: reducing learning rate of group 0 to 1.6471e-03.\n",
      "[270/500] train_loss: 0.85050 valid_loss: 0.78792\n",
      "[271/500] train_loss: 0.85031 valid_loss: 0.78799\n",
      "[272/500] train_loss: 0.84767 valid_loss: 0.78801\n",
      "[273/500] train_loss: 0.85076 valid_loss: 0.78804\n",
      "[274/500] train_loss: 0.85041 valid_loss: 0.78795\n",
      "[275/500] train_loss: 0.85311 valid_loss: 0.78804\n",
      "[276/500] train_loss: 0.85355 valid_loss: 0.78804\n",
      "[277/500] train_loss: 0.84929 valid_loss: 0.78810\n",
      "[278/500] train_loss: 0.84689 valid_loss: 0.78809\n",
      "[279/500] train_loss: 0.85335 valid_loss: 0.78819\n",
      "[280/500] train_loss: 0.85363 valid_loss: 0.78815\n",
      "Epoch 00280: reducing learning rate of group 0 to 1.1530e-03.\n",
      "[281/500] train_loss: 0.85001 valid_loss: 0.78834\n",
      "[282/500] train_loss: 0.84971 valid_loss: 0.78829\n",
      "[283/500] train_loss: 0.84646 valid_loss: 0.78835\n",
      "[284/500] train_loss: 0.85038 valid_loss: 0.78831\n",
      "[285/500] train_loss: 0.84890 valid_loss: 0.78833\n",
      "[286/500] train_loss: 0.85225 valid_loss: 0.78836\n",
      "[287/500] train_loss: 0.84950 valid_loss: 0.78833\n",
      "[288/500] train_loss: 0.85405 valid_loss: 0.78831\n",
      "[289/500] train_loss: 0.85289 valid_loss: 0.78834\n",
      "[290/500] train_loss: 0.84904 valid_loss: 0.78821\n",
      "[291/500] train_loss: 0.85016 valid_loss: 0.78820\n",
      "Epoch 00291: reducing learning rate of group 0 to 8.0707e-04.\n",
      "[292/500] train_loss: 0.85481 valid_loss: 0.78822\n",
      "[293/500] train_loss: 0.84928 valid_loss: 0.78824\n",
      "[294/500] train_loss: 0.85435 valid_loss: 0.78823\n",
      "[295/500] train_loss: 0.85127 valid_loss: 0.78823\n",
      "[296/500] train_loss: 0.85085 valid_loss: 0.78824\n",
      "[297/500] train_loss: 0.85412 valid_loss: 0.78824\n",
      "[298/500] train_loss: 0.85362 valid_loss: 0.78820\n",
      "[299/500] train_loss: 0.84882 valid_loss: 0.78821\n",
      "[300/500] train_loss: 0.84386 valid_loss: 0.81362\n",
      "[301/500] train_loss: 0.84950 valid_loss: 0.78820\n",
      "[302/500] train_loss: 0.84907 valid_loss: 0.78821\n",
      "Epoch 00302: reducing learning rate of group 0 to 5.6495e-04.\n",
      "[303/500] train_loss: 0.85470 valid_loss: 0.78819\n",
      "[304/500] train_loss: 0.85012 valid_loss: 0.78823\n",
      "[305/500] train_loss: 0.85183 valid_loss: 0.78825\n",
      "[306/500] train_loss: 0.84909 valid_loss: 0.78827\n",
      "[307/500] train_loss: 0.85252 valid_loss: 0.78825\n",
      "[308/500] train_loss: 0.84696 valid_loss: 0.78822\n",
      "[309/500] train_loss: 0.84792 valid_loss: 0.78824\n",
      "Early stopping\n",
      "[  1/500] train_loss: 1.40353 valid_loss: 0.96717\n",
      "Validation loss decreased (inf --> 0.967170).  Saving model ...\n",
      "[  2/500] train_loss: 1.13949 valid_loss: 0.88538\n",
      "Validation loss decreased (0.967170 --> 0.885381).  Saving model ...\n",
      "[  3/500] train_loss: 1.07676 valid_loss: 0.83361\n",
      "Validation loss decreased (0.885381 --> 0.833612).  Saving model ...\n",
      "[  4/500] train_loss: 1.04441 valid_loss: 0.80206\n",
      "Validation loss decreased (0.833612 --> 0.802058).  Saving model ...\n",
      "[  5/500] train_loss: 1.00921 valid_loss: 0.77981\n",
      "Validation loss decreased (0.802058 --> 0.779806).  Saving model ...\n",
      "[  6/500] train_loss: 0.99115 valid_loss: 0.76494\n",
      "Validation loss decreased (0.779806 --> 0.764938).  Saving model ...\n",
      "[  7/500] train_loss: 0.98325 valid_loss: 0.75621\n",
      "Validation loss decreased (0.764938 --> 0.756210).  Saving model ...\n",
      "[  8/500] train_loss: 0.97480 valid_loss: 0.74806\n",
      "Validation loss decreased (0.756210 --> 0.748060).  Saving model ...\n",
      "[  9/500] train_loss: 0.96890 valid_loss: 0.74291\n",
      "Validation loss decreased (0.748060 --> 0.742908).  Saving model ...\n",
      "[ 10/500] train_loss: 0.95661 valid_loss: 0.73888\n",
      "Validation loss decreased (0.742908 --> 0.738879).  Saving model ...\n",
      "[ 11/500] train_loss: 0.94850 valid_loss: 0.73521\n",
      "Validation loss decreased (0.738879 --> 0.735208).  Saving model ...\n",
      "[ 12/500] train_loss: 0.94975 valid_loss: 0.73284\n",
      "Validation loss decreased (0.735208 --> 0.732841).  Saving model ...\n",
      "[ 13/500] train_loss: 0.94515 valid_loss: 0.73020\n",
      "Validation loss decreased (0.732841 --> 0.730198).  Saving model ...\n",
      "[ 14/500] train_loss: 0.93939 valid_loss: 0.72867\n",
      "Validation loss decreased (0.730198 --> 0.728675).  Saving model ...\n",
      "[ 15/500] train_loss: 0.92872 valid_loss: 0.72688\n",
      "Validation loss decreased (0.728675 --> 0.726879).  Saving model ...\n",
      "[ 16/500] train_loss: 0.92951 valid_loss: 0.72485\n",
      "Validation loss decreased (0.726879 --> 0.724853).  Saving model ...\n",
      "[ 17/500] train_loss: 0.93524 valid_loss: 0.72366\n",
      "Validation loss decreased (0.724853 --> 0.723658).  Saving model ...\n",
      "[ 18/500] train_loss: 0.92823 valid_loss: 0.72277\n",
      "Validation loss decreased (0.723658 --> 0.722772).  Saving model ...\n",
      "[ 19/500] train_loss: 0.92372 valid_loss: 0.72209\n",
      "Validation loss decreased (0.722772 --> 0.722093).  Saving model ...\n",
      "[ 20/500] train_loss: 0.92608 valid_loss: 0.72126\n",
      "Validation loss decreased (0.722093 --> 0.721258).  Saving model ...\n",
      "[ 21/500] train_loss: 0.92105 valid_loss: 0.71908\n",
      "Validation loss decreased (0.721258 --> 0.719075).  Saving model ...\n",
      "[ 22/500] train_loss: 0.91406 valid_loss: 0.71724\n",
      "Validation loss decreased (0.719075 --> 0.717238).  Saving model ...\n",
      "[ 23/500] train_loss: 0.91839 valid_loss: 0.71710\n",
      "Validation loss decreased (0.717238 --> 0.717095).  Saving model ...\n",
      "[ 24/500] train_loss: 0.91450 valid_loss: 0.71692\n",
      "Validation loss decreased (0.717095 --> 0.716915).  Saving model ...\n",
      "[ 25/500] train_loss: 0.91150 valid_loss: 0.71617\n",
      "Validation loss decreased (0.716915 --> 0.716173).  Saving model ...\n",
      "[ 26/500] train_loss: 0.91526 valid_loss: 0.71467\n",
      "Validation loss decreased (0.716173 --> 0.714668).  Saving model ...\n",
      "[ 27/500] train_loss: 0.91415 valid_loss: 0.71362\n",
      "Validation loss decreased (0.714668 --> 0.713622).  Saving model ...\n",
      "[ 28/500] train_loss: 0.90677 valid_loss: 0.71294\n",
      "Validation loss decreased (0.713622 --> 0.712938).  Saving model ...\n",
      "[ 29/500] train_loss: 0.90672 valid_loss: 0.71232\n",
      "Validation loss decreased (0.712938 --> 0.712321).  Saving model ...\n",
      "[ 30/500] train_loss: 0.91021 valid_loss: 0.71196\n",
      "Validation loss decreased (0.712321 --> 0.711958).  Saving model ...\n",
      "[ 31/500] train_loss: 0.90438 valid_loss: 0.71079\n",
      "Validation loss decreased (0.711958 --> 0.710795).  Saving model ...\n",
      "[ 32/500] train_loss: 0.91009 valid_loss: 0.71095\n",
      "[ 33/500] train_loss: 0.90533 valid_loss: 0.71021\n",
      "Validation loss decreased (0.710795 --> 0.710209).  Saving model ...\n",
      "[ 34/500] train_loss: 0.90181 valid_loss: 0.70906\n",
      "Validation loss decreased (0.710209 --> 0.709061).  Saving model ...\n",
      "[ 35/500] train_loss: 0.90636 valid_loss: 0.70936\n",
      "[ 36/500] train_loss: 0.90169 valid_loss: 0.70952\n",
      "[ 37/500] train_loss: 0.89829 valid_loss: 0.70881\n",
      "Validation loss decreased (0.709061 --> 0.708812).  Saving model ...\n",
      "[ 38/500] train_loss: 0.90115 valid_loss: 0.70823\n",
      "Validation loss decreased (0.708812 --> 0.708233).  Saving model ...\n",
      "[ 39/500] train_loss: 0.89978 valid_loss: 0.70747\n",
      "Validation loss decreased (0.708233 --> 0.707474).  Saving model ...\n",
      "[ 40/500] train_loss: 0.89854 valid_loss: 0.70693\n",
      "Validation loss decreased (0.707474 --> 0.706926).  Saving model ...\n",
      "[ 41/500] train_loss: 0.89709 valid_loss: 0.70649\n",
      "Validation loss decreased (0.706926 --> 0.706486).  Saving model ...\n",
      "[ 42/500] train_loss: 0.89696 valid_loss: 0.70593\n",
      "Validation loss decreased (0.706486 --> 0.705926).  Saving model ...\n",
      "[ 43/500] train_loss: 0.89286 valid_loss: 0.70489\n",
      "Validation loss decreased (0.705926 --> 0.704890).  Saving model ...\n",
      "[ 44/500] train_loss: 0.89960 valid_loss: 0.70463\n",
      "Validation loss decreased (0.704890 --> 0.704631).  Saving model ...\n",
      "[ 45/500] train_loss: 0.89920 valid_loss: 0.70376\n",
      "Validation loss decreased (0.704631 --> 0.703763).  Saving model ...\n",
      "[ 46/500] train_loss: 0.89528 valid_loss: 0.70265\n",
      "Validation loss decreased (0.703763 --> 0.702654).  Saving model ...\n",
      "[ 47/500] train_loss: 0.89277 valid_loss: 0.70200\n",
      "Validation loss decreased (0.702654 --> 0.702004).  Saving model ...\n",
      "[ 48/500] train_loss: 0.89505 valid_loss: 0.70169\n",
      "Validation loss decreased (0.702004 --> 0.701689).  Saving model ...\n",
      "[ 49/500] train_loss: 0.89119 valid_loss: 0.70119\n",
      "Validation loss decreased (0.701689 --> 0.701194).  Saving model ...\n",
      "[ 50/500] train_loss: 0.89185 valid_loss: 0.70033\n",
      "Validation loss decreased (0.701194 --> 0.700325).  Saving model ...\n",
      "[ 51/500] train_loss: 0.88991 valid_loss: 0.69995\n",
      "Validation loss decreased (0.700325 --> 0.699946).  Saving model ...\n",
      "[ 52/500] train_loss: 0.89295 valid_loss: 0.69912\n",
      "Validation loss decreased (0.699946 --> 0.699116).  Saving model ...\n",
      "[ 53/500] train_loss: 0.88897 valid_loss: 0.69943\n",
      "[ 54/500] train_loss: 0.88541 valid_loss: 0.69958\n",
      "[ 55/500] train_loss: 0.88935 valid_loss: 0.69921\n",
      "[ 56/500] train_loss: 0.88843 valid_loss: 0.69918\n",
      "[ 57/500] train_loss: 0.88803 valid_loss: 0.69895\n",
      "Validation loss decreased (0.699116 --> 0.698945).  Saving model ...\n",
      "[ 58/500] train_loss: 0.88746 valid_loss: 0.69861\n",
      "Validation loss decreased (0.698945 --> 0.698605).  Saving model ...\n",
      "[ 59/500] train_loss: 0.88795 valid_loss: 0.69751\n",
      "Validation loss decreased (0.698605 --> 0.697509).  Saving model ...\n",
      "[ 60/500] train_loss: 0.88785 valid_loss: 0.69769\n",
      "[ 61/500] train_loss: 0.88225 valid_loss: 0.69747\n",
      "Validation loss decreased (0.697509 --> 0.697465).  Saving model ...\n",
      "[ 62/500] train_loss: 0.87992 valid_loss: 0.69712\n",
      "Validation loss decreased (0.697465 --> 0.697115).  Saving model ...\n",
      "[ 63/500] train_loss: 0.88327 valid_loss: 0.69626\n",
      "Validation loss decreased (0.697115 --> 0.696258).  Saving model ...\n",
      "[ 64/500] train_loss: 0.88713 valid_loss: 0.69591\n",
      "Validation loss decreased (0.696258 --> 0.695913).  Saving model ...\n",
      "[ 65/500] train_loss: 0.87612 valid_loss: 0.69582\n",
      "Validation loss decreased (0.695913 --> 0.695815).  Saving model ...\n",
      "[ 66/500] train_loss: 0.88209 valid_loss: 0.69487\n",
      "Validation loss decreased (0.695815 --> 0.694866).  Saving model ...\n",
      "[ 67/500] train_loss: 0.88399 valid_loss: 0.69459\n",
      "Validation loss decreased (0.694866 --> 0.694594).  Saving model ...\n",
      "[ 68/500] train_loss: 0.88631 valid_loss: 0.69456\n",
      "Validation loss decreased (0.694594 --> 0.694561).  Saving model ...\n",
      "[ 69/500] train_loss: 0.88028 valid_loss: 0.69401\n",
      "Validation loss decreased (0.694561 --> 0.694005).  Saving model ...\n",
      "[ 70/500] train_loss: 0.88009 valid_loss: 0.69353\n",
      "Validation loss decreased (0.694005 --> 0.693534).  Saving model ...\n",
      "[ 71/500] train_loss: 0.88701 valid_loss: 0.69312\n",
      "Validation loss decreased (0.693534 --> 0.693122).  Saving model ...\n",
      "[ 72/500] train_loss: 0.88052 valid_loss: 0.69354\n",
      "[ 73/500] train_loss: 0.87686 valid_loss: 0.69295\n",
      "Validation loss decreased (0.693122 --> 0.692955).  Saving model ...\n",
      "[ 74/500] train_loss: 0.88071 valid_loss: 0.69301\n",
      "[ 75/500] train_loss: 0.88265 valid_loss: 0.69273\n",
      "Validation loss decreased (0.692955 --> 0.692731).  Saving model ...\n",
      "[ 76/500] train_loss: 0.87781 valid_loss: 0.69232\n",
      "Validation loss decreased (0.692731 --> 0.692316).  Saving model ...\n",
      "[ 77/500] train_loss: 0.88171 valid_loss: 0.69159\n",
      "Validation loss decreased (0.692316 --> 0.691586).  Saving model ...\n",
      "[ 78/500] train_loss: 0.87316 valid_loss: 0.69083\n",
      "Validation loss decreased (0.691586 --> 0.690828).  Saving model ...\n",
      "[ 79/500] train_loss: 0.87230 valid_loss: 0.69071\n",
      "Validation loss decreased (0.690828 --> 0.690705).  Saving model ...\n",
      "[ 80/500] train_loss: 0.87502 valid_loss: 0.69104\n",
      "[ 81/500] train_loss: 0.87388 valid_loss: 0.69074\n",
      "[ 82/500] train_loss: 0.87459 valid_loss: 0.69042\n",
      "Validation loss decreased (0.690705 --> 0.690424).  Saving model ...\n",
      "[ 83/500] train_loss: 0.87330 valid_loss: 0.69029\n",
      "Validation loss decreased (0.690424 --> 0.690285).  Saving model ...\n",
      "[ 84/500] train_loss: 0.87120 valid_loss: 0.68985\n",
      "Validation loss decreased (0.690285 --> 0.689854).  Saving model ...\n",
      "[ 85/500] train_loss: 0.87306 valid_loss: 0.68963\n",
      "Validation loss decreased (0.689854 --> 0.689631).  Saving model ...\n",
      "[ 86/500] train_loss: 0.86950 valid_loss: 0.68895\n",
      "Validation loss decreased (0.689631 --> 0.688947).  Saving model ...\n",
      "[ 87/500] train_loss: 0.87063 valid_loss: 0.68882\n",
      "Validation loss decreased (0.688947 --> 0.688816).  Saving model ...\n",
      "[ 88/500] train_loss: 0.87277 valid_loss: 0.68826\n",
      "Validation loss decreased (0.688816 --> 0.688262).  Saving model ...\n",
      "[ 89/500] train_loss: 0.88103 valid_loss: 0.68839\n",
      "[ 90/500] train_loss: 0.87006 valid_loss: 0.68796\n",
      "Validation loss decreased (0.688262 --> 0.687962).  Saving model ...\n",
      "[ 91/500] train_loss: 0.87186 valid_loss: 0.68767\n",
      "Validation loss decreased (0.687962 --> 0.687667).  Saving model ...\n",
      "[ 92/500] train_loss: 0.87139 valid_loss: 0.68739\n",
      "Validation loss decreased (0.687667 --> 0.687386).  Saving model ...\n",
      "[ 93/500] train_loss: 0.87250 valid_loss: 0.68735\n",
      "Validation loss decreased (0.687386 --> 0.687348).  Saving model ...\n",
      "[ 94/500] train_loss: 0.87890 valid_loss: 0.68703\n",
      "Validation loss decreased (0.687348 --> 0.687025).  Saving model ...\n",
      "[ 95/500] train_loss: 0.87095 valid_loss: 0.68640\n",
      "Validation loss decreased (0.687025 --> 0.686399).  Saving model ...\n",
      "[ 96/500] train_loss: 0.87507 valid_loss: 0.68610\n",
      "Validation loss decreased (0.686399 --> 0.686104).  Saving model ...\n",
      "[ 97/500] train_loss: 0.86869 valid_loss: 0.68593\n",
      "Validation loss decreased (0.686104 --> 0.685926).  Saving model ...\n",
      "[ 98/500] train_loss: 0.86709 valid_loss: 0.68580\n",
      "Validation loss decreased (0.685926 --> 0.685803).  Saving model ...\n",
      "[ 99/500] train_loss: 0.86762 valid_loss: 0.68629\n",
      "[100/500] train_loss: 0.86390 valid_loss: 0.70788\n",
      "[101/500] train_loss: 0.86972 valid_loss: 0.68558\n",
      "Validation loss decreased (0.685803 --> 0.685584).  Saving model ...\n",
      "[102/500] train_loss: 0.87129 valid_loss: 0.68494\n",
      "Validation loss decreased (0.685584 --> 0.684940).  Saving model ...\n",
      "[103/500] train_loss: 0.87583 valid_loss: 0.68476\n",
      "Validation loss decreased (0.684940 --> 0.684758).  Saving model ...\n",
      "[104/500] train_loss: 0.87177 valid_loss: 0.68450\n",
      "Validation loss decreased (0.684758 --> 0.684499).  Saving model ...\n",
      "[105/500] train_loss: 0.87156 valid_loss: 0.68440\n",
      "Validation loss decreased (0.684499 --> 0.684398).  Saving model ...\n",
      "[106/500] train_loss: 0.86429 valid_loss: 0.68450\n",
      "[107/500] train_loss: 0.86544 valid_loss: 0.68406\n",
      "Validation loss decreased (0.684398 --> 0.684064).  Saving model ...\n",
      "[108/500] train_loss: 0.86808 valid_loss: 0.68393\n",
      "Validation loss decreased (0.684064 --> 0.683928).  Saving model ...\n",
      "[109/500] train_loss: 0.86646 valid_loss: 0.68337\n",
      "Validation loss decreased (0.683928 --> 0.683370).  Saving model ...\n",
      "[110/500] train_loss: 0.86363 valid_loss: 0.68329\n",
      "Validation loss decreased (0.683370 --> 0.683295).  Saving model ...\n",
      "[111/500] train_loss: 0.86381 valid_loss: 0.68317\n",
      "Validation loss decreased (0.683295 --> 0.683167).  Saving model ...\n",
      "[112/500] train_loss: 0.86182 valid_loss: 0.68282\n",
      "Validation loss decreased (0.683167 --> 0.682817).  Saving model ...\n",
      "[113/500] train_loss: 0.86601 valid_loss: 0.68277\n",
      "Validation loss decreased (0.682817 --> 0.682774).  Saving model ...\n",
      "[114/500] train_loss: 0.86894 valid_loss: 0.68296\n",
      "[115/500] train_loss: 0.86227 valid_loss: 0.68275\n",
      "Validation loss decreased (0.682774 --> 0.682746).  Saving model ...\n",
      "[116/500] train_loss: 0.86793 valid_loss: 0.68279\n",
      "[117/500] train_loss: 0.85939 valid_loss: 0.68309\n",
      "[118/500] train_loss: 0.86381 valid_loss: 0.68303\n",
      "[119/500] train_loss: 0.86146 valid_loss: 0.68219\n",
      "Validation loss decreased (0.682746 --> 0.682186).  Saving model ...\n",
      "[120/500] train_loss: 0.86531 valid_loss: 0.68175\n",
      "Validation loss decreased (0.682186 --> 0.681754).  Saving model ...\n",
      "[121/500] train_loss: 0.86297 valid_loss: 0.68179\n",
      "[122/500] train_loss: 0.86443 valid_loss: 0.68192\n",
      "[123/500] train_loss: 0.86469 valid_loss: 0.68196\n",
      "[124/500] train_loss: 0.86019 valid_loss: 0.68131\n",
      "Validation loss decreased (0.681754 --> 0.681312).  Saving model ...\n",
      "[125/500] train_loss: 0.86131 valid_loss: 0.68156\n",
      "[126/500] train_loss: 0.86153 valid_loss: 0.68113\n",
      "Validation loss decreased (0.681312 --> 0.681131).  Saving model ...\n",
      "[127/500] train_loss: 0.86565 valid_loss: 0.68088\n",
      "Validation loss decreased (0.681131 --> 0.680880).  Saving model ...\n",
      "[128/500] train_loss: 0.86215 valid_loss: 0.68063\n",
      "Validation loss decreased (0.680880 --> 0.680628).  Saving model ...\n",
      "[129/500] train_loss: 0.85750 valid_loss: 0.68080\n",
      "[130/500] train_loss: 0.86102 valid_loss: 0.68066\n",
      "[131/500] train_loss: 0.85877 valid_loss: 0.68025\n",
      "Validation loss decreased (0.680628 --> 0.680255).  Saving model ...\n",
      "[132/500] train_loss: 0.86178 valid_loss: 0.68001\n",
      "Validation loss decreased (0.680255 --> 0.680010).  Saving model ...\n",
      "[133/500] train_loss: 0.85985 valid_loss: 0.68002\n",
      "[134/500] train_loss: 0.86368 valid_loss: 0.67989\n",
      "Validation loss decreased (0.680010 --> 0.679888).  Saving model ...\n",
      "[135/500] train_loss: 0.85916 valid_loss: 0.67965\n",
      "Validation loss decreased (0.679888 --> 0.679649).  Saving model ...\n",
      "[136/500] train_loss: 0.85455 valid_loss: 0.67947\n",
      "Validation loss decreased (0.679649 --> 0.679467).  Saving model ...\n",
      "[137/500] train_loss: 0.86250 valid_loss: 0.67935\n",
      "Validation loss decreased (0.679467 --> 0.679346).  Saving model ...\n",
      "[138/500] train_loss: 0.86267 valid_loss: 0.67905\n",
      "Validation loss decreased (0.679346 --> 0.679045).  Saving model ...\n",
      "[139/500] train_loss: 0.85760 valid_loss: 0.67894\n",
      "Validation loss decreased (0.679045 --> 0.678944).  Saving model ...\n",
      "[140/500] train_loss: 0.85890 valid_loss: 0.67782\n",
      "Validation loss decreased (0.678944 --> 0.677823).  Saving model ...\n",
      "[141/500] train_loss: 0.85784 valid_loss: 0.67786\n",
      "[142/500] train_loss: 0.85865 valid_loss: 0.67777\n",
      "Validation loss decreased (0.677823 --> 0.677767).  Saving model ...\n",
      "[143/500] train_loss: 0.85870 valid_loss: 0.67792\n",
      "[144/500] train_loss: 0.85444 valid_loss: 0.67781\n",
      "[145/500] train_loss: 0.85922 valid_loss: 0.67759\n",
      "Validation loss decreased (0.677767 --> 0.677593).  Saving model ...\n",
      "[146/500] train_loss: 0.85872 valid_loss: 0.67792\n",
      "[147/500] train_loss: 0.85798 valid_loss: 0.67777\n",
      "[148/500] train_loss: 0.86061 valid_loss: 0.67749\n",
      "Validation loss decreased (0.677593 --> 0.677489).  Saving model ...\n",
      "[149/500] train_loss: 0.85647 valid_loss: 0.67751\n",
      "[150/500] train_loss: 0.85395 valid_loss: 0.67741\n",
      "Validation loss decreased (0.677489 --> 0.677413).  Saving model ...\n",
      "[151/500] train_loss: 0.85873 valid_loss: 0.67725\n",
      "Validation loss decreased (0.677413 --> 0.677245).  Saving model ...\n",
      "[152/500] train_loss: 0.85127 valid_loss: 0.67703\n",
      "Validation loss decreased (0.677245 --> 0.677033).  Saving model ...\n",
      "[153/500] train_loss: 0.86019 valid_loss: 0.67706\n",
      "[154/500] train_loss: 0.85950 valid_loss: 0.67729\n",
      "[155/500] train_loss: 0.85829 valid_loss: 0.67731\n",
      "[156/500] train_loss: 0.85562 valid_loss: 0.67679\n",
      "Validation loss decreased (0.677033 --> 0.676785).  Saving model ...\n",
      "[157/500] train_loss: 0.85270 valid_loss: 0.67698\n",
      "[158/500] train_loss: 0.85272 valid_loss: 0.67676\n",
      "Validation loss decreased (0.676785 --> 0.676757).  Saving model ...\n",
      "[159/500] train_loss: 0.85328 valid_loss: 0.67681\n",
      "[160/500] train_loss: 0.85973 valid_loss: 0.67679\n",
      "[161/500] train_loss: 0.85625 valid_loss: 0.67663\n",
      "Validation loss decreased (0.676757 --> 0.676632).  Saving model ...\n",
      "[162/500] train_loss: 0.85276 valid_loss: 0.67653\n",
      "Validation loss decreased (0.676632 --> 0.676528).  Saving model ...\n",
      "[163/500] train_loss: 0.85457 valid_loss: 0.67624\n",
      "Validation loss decreased (0.676528 --> 0.676245).  Saving model ...\n",
      "[164/500] train_loss: 0.85675 valid_loss: 0.67585\n",
      "Validation loss decreased (0.676245 --> 0.675849).  Saving model ...\n",
      "[165/500] train_loss: 0.85643 valid_loss: 0.67595\n",
      "[166/500] train_loss: 0.85289 valid_loss: 0.67611\n",
      "[167/500] train_loss: 0.85370 valid_loss: 0.67574\n",
      "Validation loss decreased (0.675849 --> 0.675741).  Saving model ...\n",
      "[168/500] train_loss: 0.85229 valid_loss: 0.67545\n",
      "Validation loss decreased (0.675741 --> 0.675450).  Saving model ...\n",
      "[169/500] train_loss: 0.85566 valid_loss: 0.67547\n",
      "[170/500] train_loss: 0.85164 valid_loss: 0.67507\n",
      "Validation loss decreased (0.675450 --> 0.675070).  Saving model ...\n",
      "[171/500] train_loss: 0.85563 valid_loss: 0.67515\n",
      "[172/500] train_loss: 0.85350 valid_loss: 0.67493\n",
      "Validation loss decreased (0.675070 --> 0.674932).  Saving model ...\n",
      "[173/500] train_loss: 0.85263 valid_loss: 0.67500\n",
      "[174/500] train_loss: 0.85157 valid_loss: 0.67524\n",
      "[175/500] train_loss: 0.85095 valid_loss: 0.67519\n",
      "[176/500] train_loss: 0.85300 valid_loss: 0.67471\n",
      "Validation loss decreased (0.674932 --> 0.674715).  Saving model ...\n",
      "[177/500] train_loss: 0.85070 valid_loss: 0.67480\n",
      "[178/500] train_loss: 0.84847 valid_loss: 0.67448\n",
      "Validation loss decreased (0.674715 --> 0.674477).  Saving model ...\n",
      "[179/500] train_loss: 0.85147 valid_loss: 0.67429\n",
      "Validation loss decreased (0.674477 --> 0.674291).  Saving model ...\n",
      "[180/500] train_loss: 0.84765 valid_loss: 0.67415\n",
      "Validation loss decreased (0.674291 --> 0.674152).  Saving model ...\n",
      "[181/500] train_loss: 0.85152 valid_loss: 0.67435\n",
      "[182/500] train_loss: 0.84674 valid_loss: 0.67436\n",
      "[183/500] train_loss: 0.85122 valid_loss: 0.67457\n",
      "[184/500] train_loss: 0.85486 valid_loss: 0.67470\n",
      "[185/500] train_loss: 0.85332 valid_loss: 0.67479\n",
      "[186/500] train_loss: 0.84523 valid_loss: 0.67434\n",
      "[187/500] train_loss: 0.84715 valid_loss: 0.67431\n",
      "[188/500] train_loss: 0.84688 valid_loss: 0.67412\n",
      "Validation loss decreased (0.674152 --> 0.674124).  Saving model ...\n",
      "[189/500] train_loss: 0.84861 valid_loss: 0.67387\n",
      "Validation loss decreased (0.674124 --> 0.673874).  Saving model ...\n",
      "[190/500] train_loss: 0.85094 valid_loss: 0.67393\n",
      "[191/500] train_loss: 0.85142 valid_loss: 0.67392\n",
      "[192/500] train_loss: 0.84981 valid_loss: 0.67373\n",
      "Validation loss decreased (0.673874 --> 0.673731).  Saving model ...\n",
      "[193/500] train_loss: 0.84848 valid_loss: 0.67384\n",
      "[194/500] train_loss: 0.85361 valid_loss: 0.67354\n",
      "Validation loss decreased (0.673731 --> 0.673544).  Saving model ...\n",
      "[195/500] train_loss: 0.84689 valid_loss: 0.67324\n",
      "Validation loss decreased (0.673544 --> 0.673244).  Saving model ...\n",
      "[196/500] train_loss: 0.85182 valid_loss: 0.67330\n",
      "[197/500] train_loss: 0.84779 valid_loss: 0.67338\n",
      "[198/500] train_loss: 0.84689 valid_loss: 0.67310\n",
      "Validation loss decreased (0.673244 --> 0.673096).  Saving model ...\n",
      "[199/500] train_loss: 0.84943 valid_loss: 0.67308\n",
      "Validation loss decreased (0.673096 --> 0.673083).  Saving model ...\n",
      "[200/500] train_loss: 0.84371 valid_loss: 0.69473\n",
      "[201/500] train_loss: 0.84719 valid_loss: 0.67289\n",
      "Validation loss decreased (0.673083 --> 0.672892).  Saving model ...\n",
      "[202/500] train_loss: 0.84845 valid_loss: 0.67267\n",
      "Validation loss decreased (0.672892 --> 0.672672).  Saving model ...\n",
      "[203/500] train_loss: 0.84812 valid_loss: 0.67293\n",
      "[204/500] train_loss: 0.84896 valid_loss: 0.67290\n",
      "[205/500] train_loss: 0.84702 valid_loss: 0.67269\n",
      "[206/500] train_loss: 0.84862 valid_loss: 0.67274\n",
      "[207/500] train_loss: 0.84904 valid_loss: 0.67284\n",
      "[208/500] train_loss: 0.84622 valid_loss: 0.67258\n",
      "Validation loss decreased (0.672672 --> 0.672580).  Saving model ...\n",
      "[209/500] train_loss: 0.84438 valid_loss: 0.67285\n",
      "[210/500] train_loss: 0.84403 valid_loss: 0.67263\n",
      "[211/500] train_loss: 0.84461 valid_loss: 0.67243\n",
      "Validation loss decreased (0.672580 --> 0.672430).  Saving model ...\n",
      "[212/500] train_loss: 0.85203 valid_loss: 0.67230\n",
      "Validation loss decreased (0.672430 --> 0.672304).  Saving model ...\n",
      "[213/500] train_loss: 0.84276 valid_loss: 0.67251\n",
      "[214/500] train_loss: 0.84748 valid_loss: 0.67239\n",
      "[215/500] train_loss: 0.84441 valid_loss: 0.67247\n",
      "[216/500] train_loss: 0.84893 valid_loss: 0.67228\n",
      "Validation loss decreased (0.672304 --> 0.672281).  Saving model ...\n",
      "[217/500] train_loss: 0.84803 valid_loss: 0.67210\n",
      "Validation loss decreased (0.672281 --> 0.672096).  Saving model ...\n",
      "[218/500] train_loss: 0.84901 valid_loss: 0.67238\n",
      "[219/500] train_loss: 0.84511 valid_loss: 0.67219\n",
      "[220/500] train_loss: 0.84414 valid_loss: 0.67203\n",
      "Validation loss decreased (0.672096 --> 0.672028).  Saving model ...\n",
      "[221/500] train_loss: 0.84189 valid_loss: 0.67173\n",
      "Validation loss decreased (0.672028 --> 0.671727).  Saving model ...\n",
      "[222/500] train_loss: 0.84328 valid_loss: 0.67175\n",
      "[223/500] train_loss: 0.84621 valid_loss: 0.67180\n",
      "[224/500] train_loss: 0.84518 valid_loss: 0.67181\n",
      "[225/500] train_loss: 0.84453 valid_loss: 0.67176\n",
      "[226/500] train_loss: 0.84731 valid_loss: 0.67154\n",
      "Validation loss decreased (0.671727 --> 0.671541).  Saving model ...\n",
      "[227/500] train_loss: 0.84410 valid_loss: 0.67119\n",
      "Validation loss decreased (0.671541 --> 0.671185).  Saving model ...\n",
      "[228/500] train_loss: 0.84577 valid_loss: 0.67124\n",
      "[229/500] train_loss: 0.84141 valid_loss: 0.67151\n",
      "[230/500] train_loss: 0.84197 valid_loss: 0.67109\n",
      "Validation loss decreased (0.671185 --> 0.671089).  Saving model ...\n",
      "[231/500] train_loss: 0.84500 valid_loss: 0.67113\n",
      "[232/500] train_loss: 0.84498 valid_loss: 0.67122\n",
      "[233/500] train_loss: 0.84511 valid_loss: 0.67132\n",
      "[234/500] train_loss: 0.84911 valid_loss: 0.67141\n",
      "[235/500] train_loss: 0.84301 valid_loss: 0.67135\n",
      "[236/500] train_loss: 0.83972 valid_loss: 0.67125\n",
      "[237/500] train_loss: 0.83857 valid_loss: 0.67131\n",
      "[238/500] train_loss: 0.84403 valid_loss: 0.67140\n",
      "[239/500] train_loss: 0.84176 valid_loss: 0.67122\n",
      "[240/500] train_loss: 0.84550 valid_loss: 0.67110\n",
      "[241/500] train_loss: 0.84326 valid_loss: 0.67107\n",
      "Epoch 00241: reducing learning rate of group 0 to 1.4000e-02.\n",
      "Validation loss decreased (0.671089 --> 0.671071).  Saving model ...\n",
      "[242/500] train_loss: 0.82855 valid_loss: 0.66639\n",
      "Validation loss decreased (0.671071 --> 0.666392).  Saving model ...\n",
      "[243/500] train_loss: 0.82715 valid_loss: 0.66328\n",
      "Validation loss decreased (0.666392 --> 0.663280).  Saving model ...\n",
      "[244/500] train_loss: 0.82551 valid_loss: 0.66213\n",
      "Validation loss decreased (0.663280 --> 0.662134).  Saving model ...\n",
      "[245/500] train_loss: 0.82841 valid_loss: 0.66093\n",
      "Validation loss decreased (0.662134 --> 0.660928).  Saving model ...\n",
      "[246/500] train_loss: 0.82569 valid_loss: 0.66060\n",
      "Validation loss decreased (0.660928 --> 0.660596).  Saving model ...\n",
      "[247/500] train_loss: 0.82962 valid_loss: 0.65957\n",
      "Validation loss decreased (0.660596 --> 0.659569).  Saving model ...\n",
      "[248/500] train_loss: 0.83025 valid_loss: 0.65947\n",
      "Validation loss decreased (0.659569 --> 0.659470).  Saving model ...\n",
      "[249/500] train_loss: 0.82755 valid_loss: 0.65880\n",
      "Validation loss decreased (0.659470 --> 0.658796).  Saving model ...\n",
      "[250/500] train_loss: 0.82603 valid_loss: 0.65831\n",
      "Validation loss decreased (0.658796 --> 0.658314).  Saving model ...\n",
      "[251/500] train_loss: 0.82685 valid_loss: 0.65785\n",
      "Validation loss decreased (0.658314 --> 0.657847).  Saving model ...\n",
      "[252/500] train_loss: 0.82723 valid_loss: 0.65774\n",
      "Validation loss decreased (0.657847 --> 0.657741).  Saving model ...\n",
      "[253/500] train_loss: 0.82883 valid_loss: 0.65740\n",
      "Validation loss decreased (0.657741 --> 0.657399).  Saving model ...\n",
      "[254/500] train_loss: 0.82717 valid_loss: 0.65695\n",
      "Validation loss decreased (0.657399 --> 0.656952).  Saving model ...\n",
      "[255/500] train_loss: 0.82760 valid_loss: 0.65640\n",
      "Validation loss decreased (0.656952 --> 0.656395).  Saving model ...\n",
      "[256/500] train_loss: 0.82797 valid_loss: 0.65626\n",
      "Validation loss decreased (0.656395 --> 0.656258).  Saving model ...\n",
      "[257/500] train_loss: 0.82664 valid_loss: 0.65615\n",
      "Validation loss decreased (0.656258 --> 0.656148).  Saving model ...\n",
      "[258/500] train_loss: 0.82874 valid_loss: 0.65620\n",
      "[259/500] train_loss: 0.82603 valid_loss: 0.65637\n",
      "[260/500] train_loss: 0.82415 valid_loss: 0.65632\n",
      "[261/500] train_loss: 0.82448 valid_loss: 0.65572\n",
      "Validation loss decreased (0.656148 --> 0.655719).  Saving model ...\n",
      "[262/500] train_loss: 0.82506 valid_loss: 0.65559\n",
      "Validation loss decreased (0.655719 --> 0.655590).  Saving model ...\n",
      "[263/500] train_loss: 0.82678 valid_loss: 0.65555\n",
      "Validation loss decreased (0.655590 --> 0.655546).  Saving model ...\n",
      "[264/500] train_loss: 0.82571 valid_loss: 0.65563\n",
      "[265/500] train_loss: 0.82668 valid_loss: 0.65558\n",
      "[266/500] train_loss: 0.82370 valid_loss: 0.65544\n",
      "Validation loss decreased (0.655546 --> 0.655437).  Saving model ...\n",
      "[267/500] train_loss: 0.82590 valid_loss: 0.65521\n",
      "Validation loss decreased (0.655437 --> 0.655207).  Saving model ...\n",
      "[268/500] train_loss: 0.82575 valid_loss: 0.65499\n",
      "Validation loss decreased (0.655207 --> 0.654989).  Saving model ...\n",
      "[269/500] train_loss: 0.82503 valid_loss: 0.65474\n",
      "Validation loss decreased (0.654989 --> 0.654741).  Saving model ...\n",
      "[270/500] train_loss: 0.82894 valid_loss: 0.65475\n",
      "[271/500] train_loss: 0.82753 valid_loss: 0.65431\n",
      "Validation loss decreased (0.654741 --> 0.654307).  Saving model ...\n",
      "[272/500] train_loss: 0.82718 valid_loss: 0.65431\n",
      "[273/500] train_loss: 0.82048 valid_loss: 0.65424\n",
      "Validation loss decreased (0.654307 --> 0.654242).  Saving model ...\n",
      "[274/500] train_loss: 0.82439 valid_loss: 0.65416\n",
      "Validation loss decreased (0.654242 --> 0.654164).  Saving model ...\n",
      "[275/500] train_loss: 0.82621 valid_loss: 0.65423\n",
      "[276/500] train_loss: 0.82429 valid_loss: 0.65425\n",
      "[277/500] train_loss: 0.82486 valid_loss: 0.65451\n",
      "[278/500] train_loss: 0.82615 valid_loss: 0.65427\n",
      "[279/500] train_loss: 0.82347 valid_loss: 0.65442\n",
      "[280/500] train_loss: 0.82428 valid_loss: 0.65444\n",
      "[281/500] train_loss: 0.82708 valid_loss: 0.65426\n",
      "[282/500] train_loss: 0.82401 valid_loss: 0.65416\n",
      "Validation loss decreased (0.654164 --> 0.654159).  Saving model ...\n",
      "[283/500] train_loss: 0.82854 valid_loss: 0.65411\n",
      "Validation loss decreased (0.654159 --> 0.654114).  Saving model ...\n",
      "[284/500] train_loss: 0.82432 valid_loss: 0.65403\n",
      "Validation loss decreased (0.654114 --> 0.654026).  Saving model ...\n",
      "[285/500] train_loss: 0.82519 valid_loss: 0.65404\n",
      "[286/500] train_loss: 0.82116 valid_loss: 0.65398\n",
      "Validation loss decreased (0.654026 --> 0.653980).  Saving model ...\n",
      "[287/500] train_loss: 0.82438 valid_loss: 0.65407\n",
      "[288/500] train_loss: 0.82687 valid_loss: 0.65428\n",
      "[289/500] train_loss: 0.82237 valid_loss: 0.65413\n",
      "[290/500] train_loss: 0.82532 valid_loss: 0.65414\n",
      "[291/500] train_loss: 0.82267 valid_loss: 0.65408\n",
      "[292/500] train_loss: 0.82299 valid_loss: 0.65396\n",
      "Validation loss decreased (0.653980 --> 0.653956).  Saving model ...\n",
      "[293/500] train_loss: 0.82611 valid_loss: 0.65393\n",
      "Validation loss decreased (0.653956 --> 0.653933).  Saving model ...\n",
      "[294/500] train_loss: 0.82692 valid_loss: 0.65387\n",
      "Validation loss decreased (0.653933 --> 0.653869).  Saving model ...\n",
      "[295/500] train_loss: 0.82606 valid_loss: 0.65380\n",
      "Validation loss decreased (0.653869 --> 0.653797).  Saving model ...\n",
      "[296/500] train_loss: 0.82551 valid_loss: 0.65375\n",
      "Validation loss decreased (0.653797 --> 0.653752).  Saving model ...\n",
      "[297/500] train_loss: 0.82268 valid_loss: 0.65383\n",
      "[298/500] train_loss: 0.82340 valid_loss: 0.65369\n",
      "Validation loss decreased (0.653752 --> 0.653689).  Saving model ...\n",
      "[299/500] train_loss: 0.82488 valid_loss: 0.65364\n",
      "Validation loss decreased (0.653689 --> 0.653644).  Saving model ...\n",
      "[300/500] train_loss: 0.82314 valid_loss: 0.67487\n",
      "[301/500] train_loss: 0.82396 valid_loss: 0.65380\n",
      "[302/500] train_loss: 0.82385 valid_loss: 0.65391\n",
      "[303/500] train_loss: 0.82527 valid_loss: 0.65401\n",
      "[304/500] train_loss: 0.82372 valid_loss: 0.65404\n",
      "[305/500] train_loss: 0.82131 valid_loss: 0.65411\n",
      "[306/500] train_loss: 0.82251 valid_loss: 0.65416\n",
      "[307/500] train_loss: 0.82272 valid_loss: 0.65397\n",
      "[308/500] train_loss: 0.82554 valid_loss: 0.65402\n",
      "[309/500] train_loss: 0.82438 valid_loss: 0.65420\n",
      "Epoch 00309: reducing learning rate of group 0 to 9.8000e-03.\n",
      "[310/500] train_loss: 0.81834 valid_loss: 0.65275\n",
      "Validation loss decreased (0.653644 --> 0.652754).  Saving model ...\n",
      "[311/500] train_loss: 0.81394 valid_loss: 0.65174\n",
      "Validation loss decreased (0.652754 --> 0.651742).  Saving model ...\n",
      "[312/500] train_loss: 0.81561 valid_loss: 0.65129\n",
      "Validation loss decreased (0.651742 --> 0.651290).  Saving model ...\n",
      "[313/500] train_loss: 0.81639 valid_loss: 0.65051\n",
      "Validation loss decreased (0.651290 --> 0.650508).  Saving model ...\n",
      "[314/500] train_loss: 0.81737 valid_loss: 0.65005\n",
      "Validation loss decreased (0.650508 --> 0.650053).  Saving model ...\n",
      "[315/500] train_loss: 0.81415 valid_loss: 0.64993\n",
      "Validation loss decreased (0.650053 --> 0.649935).  Saving model ...\n",
      "[316/500] train_loss: 0.81628 valid_loss: 0.64960\n",
      "Validation loss decreased (0.649935 --> 0.649600).  Saving model ...\n",
      "[317/500] train_loss: 0.81448 valid_loss: 0.64921\n",
      "Validation loss decreased (0.649600 --> 0.649213).  Saving model ...\n",
      "[318/500] train_loss: 0.81878 valid_loss: 0.64949\n",
      "[319/500] train_loss: 0.81405 valid_loss: 0.64937\n",
      "[320/500] train_loss: 0.81583 valid_loss: 0.64931\n",
      "[321/500] train_loss: 0.81389 valid_loss: 0.64958\n",
      "[322/500] train_loss: 0.81644 valid_loss: 0.64930\n",
      "[323/500] train_loss: 0.81477 valid_loss: 0.64921\n",
      "[324/500] train_loss: 0.81663 valid_loss: 0.64933\n",
      "[325/500] train_loss: 0.81669 valid_loss: 0.64937\n",
      "[326/500] train_loss: 0.81525 valid_loss: 0.64912\n",
      "Validation loss decreased (0.649213 --> 0.649120).  Saving model ...\n",
      "[327/500] train_loss: 0.81216 valid_loss: 0.64908\n",
      "Validation loss decreased (0.649120 --> 0.649078).  Saving model ...\n",
      "[328/500] train_loss: 0.81625 valid_loss: 0.64888\n",
      "Validation loss decreased (0.649078 --> 0.648879).  Saving model ...\n",
      "[329/500] train_loss: 0.81564 valid_loss: 0.64883\n",
      "Validation loss decreased (0.648879 --> 0.648830).  Saving model ...\n",
      "[330/500] train_loss: 0.81566 valid_loss: 0.64891\n",
      "[331/500] train_loss: 0.81502 valid_loss: 0.64884\n",
      "[332/500] train_loss: 0.81847 valid_loss: 0.64884\n",
      "[333/500] train_loss: 0.81570 valid_loss: 0.64882\n",
      "Validation loss decreased (0.648830 --> 0.648822).  Saving model ...\n",
      "[334/500] train_loss: 0.81474 valid_loss: 0.64902\n",
      "[335/500] train_loss: 0.81571 valid_loss: 0.64897\n",
      "[336/500] train_loss: 0.81879 valid_loss: 0.64892\n",
      "[337/500] train_loss: 0.81498 valid_loss: 0.64883\n",
      "[338/500] train_loss: 0.81561 valid_loss: 0.64867\n",
      "Validation loss decreased (0.648822 --> 0.648671).  Saving model ...\n",
      "[339/500] train_loss: 0.81660 valid_loss: 0.64854\n",
      "Validation loss decreased (0.648671 --> 0.648543).  Saving model ...\n",
      "[340/500] train_loss: 0.81610 valid_loss: 0.64855\n",
      "[341/500] train_loss: 0.81027 valid_loss: 0.64858\n",
      "[342/500] train_loss: 0.81250 valid_loss: 0.64867\n",
      "[343/500] train_loss: 0.81024 valid_loss: 0.64879\n",
      "[344/500] train_loss: 0.81339 valid_loss: 0.64889\n",
      "[345/500] train_loss: 0.81292 valid_loss: 0.64886\n",
      "[346/500] train_loss: 0.81266 valid_loss: 0.64871\n",
      "[347/500] train_loss: 0.81613 valid_loss: 0.64878\n",
      "[348/500] train_loss: 0.81517 valid_loss: 0.64868\n",
      "[349/500] train_loss: 0.81409 valid_loss: 0.64874\n",
      "[350/500] train_loss: 0.81281 valid_loss: 0.64883\n",
      "Epoch 00350: reducing learning rate of group 0 to 6.8600e-03.\n",
      "[351/500] train_loss: 0.81137 valid_loss: 0.64776\n",
      "Validation loss decreased (0.648543 --> 0.647759).  Saving model ...\n",
      "[352/500] train_loss: 0.80955 valid_loss: 0.64739\n",
      "Validation loss decreased (0.647759 --> 0.647392).  Saving model ...\n",
      "[353/500] train_loss: 0.81057 valid_loss: 0.64732\n",
      "Validation loss decreased (0.647392 --> 0.647323).  Saving model ...\n",
      "[354/500] train_loss: 0.80988 valid_loss: 0.64725\n",
      "Validation loss decreased (0.647323 --> 0.647251).  Saving model ...\n",
      "[355/500] train_loss: 0.81245 valid_loss: 0.64708\n",
      "Validation loss decreased (0.647251 --> 0.647078).  Saving model ...\n",
      "[356/500] train_loss: 0.80725 valid_loss: 0.64716\n",
      "[357/500] train_loss: 0.81218 valid_loss: 0.64714\n",
      "[358/500] train_loss: 0.80929 valid_loss: 0.64696\n",
      "Validation loss decreased (0.647078 --> 0.646955).  Saving model ...\n",
      "[359/500] train_loss: 0.80905 valid_loss: 0.64705\n",
      "[360/500] train_loss: 0.81039 valid_loss: 0.64712\n",
      "[361/500] train_loss: 0.81110 valid_loss: 0.64725\n",
      "[362/500] train_loss: 0.81220 valid_loss: 0.64725\n",
      "[363/500] train_loss: 0.80967 valid_loss: 0.64726\n",
      "[364/500] train_loss: 0.80864 valid_loss: 0.64707\n",
      "[365/500] train_loss: 0.80933 valid_loss: 0.64706\n",
      "[366/500] train_loss: 0.80982 valid_loss: 0.64717\n",
      "[367/500] train_loss: 0.80817 valid_loss: 0.64737\n",
      "[368/500] train_loss: 0.81053 valid_loss: 0.64715\n",
      "[369/500] train_loss: 0.81050 valid_loss: 0.64685\n",
      "Validation loss decreased (0.646955 --> 0.646852).  Saving model ...\n",
      "[370/500] train_loss: 0.80791 valid_loss: 0.64695\n",
      "[371/500] train_loss: 0.81099 valid_loss: 0.64705\n",
      "[372/500] train_loss: 0.81227 valid_loss: 0.64724\n",
      "[373/500] train_loss: 0.80932 valid_loss: 0.64727\n",
      "[374/500] train_loss: 0.80707 valid_loss: 0.64719\n",
      "[375/500] train_loss: 0.81106 valid_loss: 0.64717\n",
      "[376/500] train_loss: 0.81164 valid_loss: 0.64699\n",
      "[377/500] train_loss: 0.80944 valid_loss: 0.64715\n",
      "[378/500] train_loss: 0.80969 valid_loss: 0.64720\n",
      "[379/500] train_loss: 0.81034 valid_loss: 0.64696\n",
      "[380/500] train_loss: 0.80922 valid_loss: 0.64697\n",
      "Epoch 00380: reducing learning rate of group 0 to 4.8020e-03.\n",
      "[381/500] train_loss: 0.80281 valid_loss: 0.64654\n",
      "Validation loss decreased (0.646852 --> 0.646536).  Saving model ...\n",
      "[382/500] train_loss: 0.80685 valid_loss: 0.64637\n",
      "Validation loss decreased (0.646536 --> 0.646368).  Saving model ...\n",
      "[383/500] train_loss: 0.80901 valid_loss: 0.64651\n",
      "[384/500] train_loss: 0.80974 valid_loss: 0.64657\n",
      "[385/500] train_loss: 0.80658 valid_loss: 0.64661\n",
      "[386/500] train_loss: 0.80859 valid_loss: 0.64659\n",
      "[387/500] train_loss: 0.80562 valid_loss: 0.64674\n",
      "[388/500] train_loss: 0.80516 valid_loss: 0.64663\n",
      "[389/500] train_loss: 0.80496 valid_loss: 0.64659\n",
      "[390/500] train_loss: 0.80712 valid_loss: 0.64681\n",
      "[391/500] train_loss: 0.80685 valid_loss: 0.64672\n",
      "[392/500] train_loss: 0.80696 valid_loss: 0.64674\n",
      "[393/500] train_loss: 0.80741 valid_loss: 0.64680\n",
      "Epoch 00393: reducing learning rate of group 0 to 3.3614e-03.\n",
      "[394/500] train_loss: 0.80144 valid_loss: 0.64659\n",
      "[395/500] train_loss: 0.80888 valid_loss: 0.64683\n",
      "[396/500] train_loss: 0.80762 valid_loss: 0.64841\n",
      "[397/500] train_loss: 0.80931 valid_loss: 0.64832\n",
      "[398/500] train_loss: 0.80623 valid_loss: 0.64829\n",
      "[399/500] train_loss: 0.80595 valid_loss: 0.64829\n",
      "[400/500] train_loss: 0.80830 valid_loss: 0.66922\n",
      "[401/500] train_loss: 0.80930 valid_loss: 0.64830\n",
      "[402/500] train_loss: 0.80882 valid_loss: 0.64831\n",
      "[403/500] train_loss: 0.80609 valid_loss: 0.64831\n",
      "[404/500] train_loss: 0.80836 valid_loss: 0.64825\n",
      "Epoch 00404: reducing learning rate of group 0 to 2.3530e-03.\n",
      "[405/500] train_loss: 0.80428 valid_loss: 0.64790\n",
      "[406/500] train_loss: 0.80734 valid_loss: 0.64801\n",
      "[407/500] train_loss: 0.80476 valid_loss: 0.64807\n",
      "[408/500] train_loss: 0.80883 valid_loss: 0.64810\n",
      "[409/500] train_loss: 0.80792 valid_loss: 0.64812\n",
      "[410/500] train_loss: 0.80683 valid_loss: 0.64807\n",
      "[411/500] train_loss: 0.80673 valid_loss: 0.64808\n",
      "[412/500] train_loss: 0.80372 valid_loss: 0.64796\n",
      "[413/500] train_loss: 0.80441 valid_loss: 0.64805\n",
      "[414/500] train_loss: 0.80885 valid_loss: 0.64800\n",
      "[415/500] train_loss: 0.80710 valid_loss: 0.64801\n",
      "Epoch 00415: reducing learning rate of group 0 to 1.6471e-03.\n",
      "[416/500] train_loss: 0.80353 valid_loss: 0.64712\n",
      "[417/500] train_loss: 0.80279 valid_loss: 0.64727\n",
      "[418/500] train_loss: 0.80531 valid_loss: 0.64723\n",
      "[419/500] train_loss: 0.80242 valid_loss: 0.64726\n",
      "[420/500] train_loss: 0.80305 valid_loss: 0.64723\n",
      "[421/500] train_loss: 0.80671 valid_loss: 0.64718\n",
      "[422/500] train_loss: 0.80861 valid_loss: 0.64717\n",
      "[423/500] train_loss: 0.80730 valid_loss: 0.64720\n",
      "[424/500] train_loss: 0.80307 valid_loss: 0.64714\n",
      "[425/500] train_loss: 0.80330 valid_loss: 0.64714\n",
      "[426/500] train_loss: 0.80463 valid_loss: 0.64719\n",
      "Epoch 00426: reducing learning rate of group 0 to 1.1530e-03.\n",
      "[427/500] train_loss: 0.80595 valid_loss: 0.64680\n",
      "[428/500] train_loss: 0.80596 valid_loss: 0.64682\n",
      "[429/500] train_loss: 0.80469 valid_loss: 0.64679\n",
      "[430/500] train_loss: 0.80590 valid_loss: 0.64679\n",
      "[431/500] train_loss: 0.80692 valid_loss: 0.64681\n",
      "[432/500] train_loss: 0.80222 valid_loss: 0.64683\n",
      "Early stopping\n",
      "[  1/500] train_loss: 1.27003 valid_loss: 0.95937\n",
      "Validation loss decreased (inf --> 0.959370).  Saving model ...\n",
      "[  2/500] train_loss: 1.16559 valid_loss: 0.89222\n",
      "Validation loss decreased (0.959370 --> 0.892218).  Saving model ...\n",
      "[  3/500] train_loss: 1.12591 valid_loss: 0.86210\n",
      "Validation loss decreased (0.892218 --> 0.862103).  Saving model ...\n",
      "[  4/500] train_loss: 1.10365 valid_loss: 0.84440\n",
      "Validation loss decreased (0.862103 --> 0.844400).  Saving model ...\n",
      "[  5/500] train_loss: 1.09262 valid_loss: 0.83051\n",
      "Validation loss decreased (0.844400 --> 0.830505).  Saving model ...\n",
      "[  6/500] train_loss: 1.07317 valid_loss: 0.82171\n",
      "Validation loss decreased (0.830505 --> 0.821709).  Saving model ...\n",
      "[  7/500] train_loss: 1.06808 valid_loss: 0.81508\n",
      "Validation loss decreased (0.821709 --> 0.815080).  Saving model ...\n",
      "[  8/500] train_loss: 1.06328 valid_loss: 0.80742\n",
      "Validation loss decreased (0.815080 --> 0.807420).  Saving model ...\n",
      "[  9/500] train_loss: 1.05462 valid_loss: 0.80199\n",
      "Validation loss decreased (0.807420 --> 0.801993).  Saving model ...\n",
      "[ 10/500] train_loss: 1.03822 valid_loss: 0.79647\n",
      "Validation loss decreased (0.801993 --> 0.796466).  Saving model ...\n",
      "[ 11/500] train_loss: 1.03667 valid_loss: 0.79245\n",
      "Validation loss decreased (0.796466 --> 0.792454).  Saving model ...\n",
      "[ 12/500] train_loss: 1.03232 valid_loss: 0.78989\n",
      "Validation loss decreased (0.792454 --> 0.789886).  Saving model ...\n",
      "[ 13/500] train_loss: 1.02776 valid_loss: 0.78717\n",
      "Validation loss decreased (0.789886 --> 0.787173).  Saving model ...\n",
      "[ 14/500] train_loss: 1.01751 valid_loss: 0.78524\n",
      "Validation loss decreased (0.787173 --> 0.785245).  Saving model ...\n",
      "[ 15/500] train_loss: 1.02593 valid_loss: 0.78317\n",
      "Validation loss decreased (0.785245 --> 0.783166).  Saving model ...\n",
      "[ 16/500] train_loss: 1.01038 valid_loss: 0.78129\n",
      "Validation loss decreased (0.783166 --> 0.781288).  Saving model ...\n",
      "[ 17/500] train_loss: 1.00779 valid_loss: 0.77892\n",
      "Validation loss decreased (0.781288 --> 0.778919).  Saving model ...\n",
      "[ 18/500] train_loss: 0.99929 valid_loss: 0.77755\n",
      "Validation loss decreased (0.778919 --> 0.777554).  Saving model ...\n",
      "[ 19/500] train_loss: 1.00311 valid_loss: 0.77616\n",
      "Validation loss decreased (0.777554 --> 0.776160).  Saving model ...\n",
      "[ 20/500] train_loss: 1.00298 valid_loss: 0.77471\n",
      "Validation loss decreased (0.776160 --> 0.774709).  Saving model ...\n",
      "[ 21/500] train_loss: 0.99725 valid_loss: 0.77418\n",
      "Validation loss decreased (0.774709 --> 0.774182).  Saving model ...\n",
      "[ 22/500] train_loss: 0.99997 valid_loss: 0.77266\n",
      "Validation loss decreased (0.774182 --> 0.772661).  Saving model ...\n",
      "[ 23/500] train_loss: 0.99293 valid_loss: 0.77143\n",
      "Validation loss decreased (0.772661 --> 0.771426).  Saving model ...\n",
      "[ 24/500] train_loss: 0.98893 valid_loss: 0.76932\n",
      "Validation loss decreased (0.771426 --> 0.769317).  Saving model ...\n",
      "[ 25/500] train_loss: 0.99181 valid_loss: 0.76906\n",
      "Validation loss decreased (0.769317 --> 0.769063).  Saving model ...\n",
      "[ 26/500] train_loss: 0.98507 valid_loss: 0.76743\n",
      "Validation loss decreased (0.769063 --> 0.767431).  Saving model ...\n",
      "[ 27/500] train_loss: 0.98570 valid_loss: 0.76671\n",
      "Validation loss decreased (0.767431 --> 0.766709).  Saving model ...\n",
      "[ 28/500] train_loss: 0.98356 valid_loss: 0.76467\n",
      "Validation loss decreased (0.766709 --> 0.764671).  Saving model ...\n",
      "[ 29/500] train_loss: 0.97760 valid_loss: 0.76203\n",
      "Validation loss decreased (0.764671 --> 0.762033).  Saving model ...\n",
      "[ 30/500] train_loss: 0.98501 valid_loss: 0.76045\n",
      "Validation loss decreased (0.762033 --> 0.760449).  Saving model ...\n",
      "[ 31/500] train_loss: 0.97416 valid_loss: 0.76048\n",
      "[ 32/500] train_loss: 0.98438 valid_loss: 0.76005\n",
      "Validation loss decreased (0.760449 --> 0.760050).  Saving model ...\n",
      "[ 33/500] train_loss: 0.96797 valid_loss: 0.75913\n",
      "Validation loss decreased (0.760050 --> 0.759126).  Saving model ...\n",
      "[ 34/500] train_loss: 0.97469 valid_loss: 0.75822\n",
      "Validation loss decreased (0.759126 --> 0.758218).  Saving model ...\n",
      "[ 35/500] train_loss: 0.97351 valid_loss: 0.75766\n",
      "Validation loss decreased (0.758218 --> 0.757663).  Saving model ...\n",
      "[ 36/500] train_loss: 0.97482 valid_loss: 0.75624\n",
      "Validation loss decreased (0.757663 --> 0.756237).  Saving model ...\n",
      "[ 37/500] train_loss: 0.96850 valid_loss: 0.75642\n",
      "[ 38/500] train_loss: 0.97462 valid_loss: 0.75547\n",
      "Validation loss decreased (0.756237 --> 0.755469).  Saving model ...\n",
      "[ 39/500] train_loss: 0.96700 valid_loss: 0.75445\n",
      "Validation loss decreased (0.755469 --> 0.754452).  Saving model ...\n",
      "[ 40/500] train_loss: 0.97237 valid_loss: 0.75448\n",
      "[ 41/500] train_loss: 0.96956 valid_loss: 0.75438\n",
      "Validation loss decreased (0.754452 --> 0.754380).  Saving model ...\n",
      "[ 42/500] train_loss: 0.96117 valid_loss: 0.75386\n",
      "Validation loss decreased (0.754380 --> 0.753857).  Saving model ...\n",
      "[ 43/500] train_loss: 0.96415 valid_loss: 0.75372\n",
      "Validation loss decreased (0.753857 --> 0.753720).  Saving model ...\n",
      "[ 44/500] train_loss: 0.96177 valid_loss: 0.75296\n",
      "Validation loss decreased (0.753720 --> 0.752964).  Saving model ...\n",
      "[ 45/500] train_loss: 0.96419 valid_loss: 0.75212\n",
      "Validation loss decreased (0.752964 --> 0.752120).  Saving model ...\n",
      "[ 46/500] train_loss: 0.95700 valid_loss: 0.75217\n",
      "[ 47/500] train_loss: 0.95870 valid_loss: 0.75195\n",
      "Validation loss decreased (0.752120 --> 0.751954).  Saving model ...\n",
      "[ 48/500] train_loss: 0.96187 valid_loss: 0.74921\n",
      "Validation loss decreased (0.751954 --> 0.749205).  Saving model ...\n",
      "[ 49/500] train_loss: 0.95654 valid_loss: 0.74874\n",
      "Validation loss decreased (0.749205 --> 0.748740).  Saving model ...\n",
      "[ 50/500] train_loss: 0.95516 valid_loss: 0.74830\n",
      "Validation loss decreased (0.748740 --> 0.748302).  Saving model ...\n",
      "[ 51/500] train_loss: 0.96016 valid_loss: 0.74830\n",
      "Validation loss decreased (0.748302 --> 0.748300).  Saving model ...\n",
      "[ 52/500] train_loss: 0.96290 valid_loss: 0.74793\n",
      "Validation loss decreased (0.748300 --> 0.747929).  Saving model ...\n",
      "[ 53/500] train_loss: 0.95177 valid_loss: 0.74747\n",
      "Validation loss decreased (0.747929 --> 0.747469).  Saving model ...\n",
      "[ 54/500] train_loss: 0.95747 valid_loss: 0.74635\n",
      "Validation loss decreased (0.747469 --> 0.746348).  Saving model ...\n",
      "[ 55/500] train_loss: 0.96612 valid_loss: 0.74504\n",
      "Validation loss decreased (0.746348 --> 0.745038).  Saving model ...\n",
      "[ 56/500] train_loss: 0.94571 valid_loss: 0.74520\n",
      "[ 57/500] train_loss: 0.95184 valid_loss: 0.74477\n",
      "Validation loss decreased (0.745038 --> 0.744774).  Saving model ...\n",
      "[ 58/500] train_loss: 0.94870 valid_loss: 0.74362\n",
      "Validation loss decreased (0.744774 --> 0.743616).  Saving model ...\n",
      "[ 59/500] train_loss: 0.94994 valid_loss: 0.74349\n",
      "Validation loss decreased (0.743616 --> 0.743492).  Saving model ...\n",
      "[ 60/500] train_loss: 0.94777 valid_loss: 0.74384\n",
      "[ 61/500] train_loss: 0.94673 valid_loss: 0.74359\n",
      "[ 62/500] train_loss: 0.94484 valid_loss: 0.74362\n",
      "[ 63/500] train_loss: 0.94832 valid_loss: 0.74340\n",
      "Validation loss decreased (0.743492 --> 0.743398).  Saving model ...\n",
      "[ 64/500] train_loss: 0.94688 valid_loss: 0.74239\n",
      "Validation loss decreased (0.743398 --> 0.742391).  Saving model ...\n",
      "[ 65/500] train_loss: 0.95339 valid_loss: 0.74190\n",
      "Validation loss decreased (0.742391 --> 0.741903).  Saving model ...\n",
      "[ 66/500] train_loss: 0.94919 valid_loss: 0.74197\n",
      "[ 67/500] train_loss: 0.94848 valid_loss: 0.74122\n",
      "Validation loss decreased (0.741903 --> 0.741217).  Saving model ...\n",
      "[ 68/500] train_loss: 0.94659 valid_loss: 0.74094\n",
      "Validation loss decreased (0.741217 --> 0.740939).  Saving model ...\n",
      "[ 69/500] train_loss: 0.94666 valid_loss: 0.74062\n",
      "Validation loss decreased (0.740939 --> 0.740621).  Saving model ...\n",
      "[ 70/500] train_loss: 0.93733 valid_loss: 0.73970\n",
      "Validation loss decreased (0.740621 --> 0.739698).  Saving model ...\n",
      "[ 71/500] train_loss: 0.94205 valid_loss: 0.73964\n",
      "Validation loss decreased (0.739698 --> 0.739635).  Saving model ...\n",
      "[ 72/500] train_loss: 0.95163 valid_loss: 0.73955\n",
      "Validation loss decreased (0.739635 --> 0.739554).  Saving model ...\n",
      "[ 73/500] train_loss: 0.94309 valid_loss: 0.73932\n",
      "Validation loss decreased (0.739554 --> 0.739322).  Saving model ...\n",
      "[ 74/500] train_loss: 0.93885 valid_loss: 0.73857\n",
      "Validation loss decreased (0.739322 --> 0.738570).  Saving model ...\n",
      "[ 75/500] train_loss: 0.94261 valid_loss: 0.73817\n",
      "Validation loss decreased (0.738570 --> 0.738166).  Saving model ...\n",
      "[ 76/500] train_loss: 0.94213 valid_loss: 0.73836\n",
      "[ 77/500] train_loss: 0.94406 valid_loss: 0.73817\n",
      "[ 78/500] train_loss: 0.94482 valid_loss: 0.73834\n",
      "[ 79/500] train_loss: 0.93667 valid_loss: 0.73749\n",
      "Validation loss decreased (0.738166 --> 0.737492).  Saving model ...\n",
      "[ 80/500] train_loss: 0.94422 valid_loss: 0.73773\n",
      "[ 81/500] train_loss: 0.94109 valid_loss: 0.73765\n",
      "[ 82/500] train_loss: 0.93956 valid_loss: 0.73713\n",
      "Validation loss decreased (0.737492 --> 0.737135).  Saving model ...\n",
      "[ 83/500] train_loss: 0.93924 valid_loss: 0.73633\n",
      "Validation loss decreased (0.737135 --> 0.736330).  Saving model ...\n",
      "[ 84/500] train_loss: 0.93681 valid_loss: 0.73562\n",
      "Validation loss decreased (0.736330 --> 0.735624).  Saving model ...\n",
      "[ 85/500] train_loss: 0.94111 valid_loss: 0.73465\n",
      "Validation loss decreased (0.735624 --> 0.734648).  Saving model ...\n",
      "[ 86/500] train_loss: 0.93617 valid_loss: 0.73432\n",
      "Validation loss decreased (0.734648 --> 0.734317).  Saving model ...\n",
      "[ 87/500] train_loss: 0.93639 valid_loss: 0.73420\n",
      "Validation loss decreased (0.734317 --> 0.734205).  Saving model ...\n",
      "[ 88/500] train_loss: 0.93439 valid_loss: 0.73386\n",
      "Validation loss decreased (0.734205 --> 0.733856).  Saving model ...\n",
      "[ 89/500] train_loss: 0.92978 valid_loss: 0.73394\n",
      "[ 90/500] train_loss: 0.93696 valid_loss: 0.73356\n",
      "Validation loss decreased (0.733856 --> 0.733556).  Saving model ...\n",
      "[ 91/500] train_loss: 0.93715 valid_loss: 0.73351\n",
      "Validation loss decreased (0.733556 --> 0.733506).  Saving model ...\n",
      "[ 92/500] train_loss: 0.93251 valid_loss: 0.73249\n",
      "Validation loss decreased (0.733506 --> 0.732494).  Saving model ...\n",
      "[ 93/500] train_loss: 0.92891 valid_loss: 0.73181\n",
      "Validation loss decreased (0.732494 --> 0.731808).  Saving model ...\n",
      "[ 94/500] train_loss: 0.93180 valid_loss: 0.73197\n",
      "[ 95/500] train_loss: 0.93423 valid_loss: 0.73109\n",
      "Validation loss decreased (0.731808 --> 0.731087).  Saving model ...\n",
      "[ 96/500] train_loss: 0.93201 valid_loss: 0.73103\n",
      "Validation loss decreased (0.731087 --> 0.731027).  Saving model ...\n",
      "[ 97/500] train_loss: 0.93401 valid_loss: 0.73032\n",
      "Validation loss decreased (0.731027 --> 0.730318).  Saving model ...\n",
      "[ 98/500] train_loss: 0.92677 valid_loss: 0.73054\n",
      "[ 99/500] train_loss: 0.93008 valid_loss: 0.73060\n",
      "[100/500] train_loss: 0.93623 valid_loss: 0.75369\n",
      "[101/500] train_loss: 0.93340 valid_loss: 0.73005\n",
      "Validation loss decreased (0.730318 --> 0.730051).  Saving model ...\n",
      "[102/500] train_loss: 0.93130 valid_loss: 0.73007\n",
      "[103/500] train_loss: 0.93179 valid_loss: 0.73017\n",
      "[104/500] train_loss: 0.93411 valid_loss: 0.73058\n",
      "[105/500] train_loss: 0.93356 valid_loss: 0.72981\n",
      "Validation loss decreased (0.730051 --> 0.729808).  Saving model ...\n",
      "[106/500] train_loss: 0.93163 valid_loss: 0.72920\n",
      "Validation loss decreased (0.729808 --> 0.729200).  Saving model ...\n",
      "[107/500] train_loss: 0.92545 valid_loss: 0.72963\n",
      "[108/500] train_loss: 0.92650 valid_loss: 0.72883\n",
      "Validation loss decreased (0.729200 --> 0.728828).  Saving model ...\n",
      "[109/500] train_loss: 0.92954 valid_loss: 0.72860\n",
      "Validation loss decreased (0.728828 --> 0.728597).  Saving model ...\n",
      "[110/500] train_loss: 0.92707 valid_loss: 0.72873\n",
      "[111/500] train_loss: 0.93026 valid_loss: 0.72874\n",
      "[112/500] train_loss: 0.93402 valid_loss: 0.72859\n",
      "Validation loss decreased (0.728597 --> 0.728585).  Saving model ...\n",
      "[113/500] train_loss: 0.92673 valid_loss: 0.72817\n",
      "Validation loss decreased (0.728585 --> 0.728166).  Saving model ...\n",
      "[114/500] train_loss: 0.92726 valid_loss: 0.72873\n",
      "[115/500] train_loss: 0.92915 valid_loss: 0.72868\n",
      "[116/500] train_loss: 0.93335 valid_loss: 0.72908\n",
      "[117/500] train_loss: 0.92860 valid_loss: 0.72917\n",
      "[118/500] train_loss: 0.92566 valid_loss: 0.72870\n",
      "[119/500] train_loss: 0.93014 valid_loss: 0.72844\n",
      "[120/500] train_loss: 0.92110 valid_loss: 0.72872\n",
      "[121/500] train_loss: 0.92100 valid_loss: 0.72893\n",
      "[122/500] train_loss: 0.92416 valid_loss: 0.72892\n",
      "[123/500] train_loss: 0.92343 valid_loss: 0.72845\n",
      "[124/500] train_loss: 0.92315 valid_loss: 0.72805\n",
      "Validation loss decreased (0.728166 --> 0.728052).  Saving model ...\n",
      "[125/500] train_loss: 0.92291 valid_loss: 0.72797\n",
      "Validation loss decreased (0.728052 --> 0.727966).  Saving model ...\n",
      "[126/500] train_loss: 0.92472 valid_loss: 0.72763\n",
      "Validation loss decreased (0.727966 --> 0.727629).  Saving model ...\n",
      "[127/500] train_loss: 0.92224 valid_loss: 0.72761\n",
      "Validation loss decreased (0.727629 --> 0.727615).  Saving model ...\n",
      "[128/500] train_loss: 0.92081 valid_loss: 0.72777\n",
      "[129/500] train_loss: 0.92790 valid_loss: 0.72702\n",
      "Validation loss decreased (0.727615 --> 0.727017).  Saving model ...\n",
      "[130/500] train_loss: 0.91914 valid_loss: 0.72665\n",
      "Validation loss decreased (0.727017 --> 0.726652).  Saving model ...\n",
      "[131/500] train_loss: 0.92263 valid_loss: 0.72657\n",
      "Validation loss decreased (0.726652 --> 0.726566).  Saving model ...\n",
      "[132/500] train_loss: 0.92195 valid_loss: 0.72677\n",
      "[133/500] train_loss: 0.91896 valid_loss: 0.72681\n",
      "[134/500] train_loss: 0.92680 valid_loss: 0.72669\n",
      "[135/500] train_loss: 0.92170 valid_loss: 0.72619\n",
      "Validation loss decreased (0.726566 --> 0.726191).  Saving model ...\n",
      "[136/500] train_loss: 0.91531 valid_loss: 0.72656\n",
      "[137/500] train_loss: 0.92254 valid_loss: 0.72634\n",
      "[138/500] train_loss: 0.91770 valid_loss: 0.72630\n",
      "[139/500] train_loss: 0.91309 valid_loss: 0.72585\n",
      "Validation loss decreased (0.726191 --> 0.725853).  Saving model ...\n",
      "[140/500] train_loss: 0.91714 valid_loss: 0.72506\n",
      "Validation loss decreased (0.725853 --> 0.725061).  Saving model ...\n",
      "[141/500] train_loss: 0.91668 valid_loss: 0.72546\n",
      "[142/500] train_loss: 0.91390 valid_loss: 0.72553\n",
      "[143/500] train_loss: 0.92156 valid_loss: 0.72528\n",
      "[144/500] train_loss: 0.91964 valid_loss: 0.72524\n",
      "[145/500] train_loss: 0.91498 valid_loss: 0.72503\n",
      "Validation loss decreased (0.725061 --> 0.725027).  Saving model ...\n",
      "[146/500] train_loss: 0.91823 valid_loss: 0.72537\n",
      "[147/500] train_loss: 0.91362 valid_loss: 0.72546\n",
      "[148/500] train_loss: 0.92104 valid_loss: 0.72491\n",
      "Validation loss decreased (0.725027 --> 0.724913).  Saving model ...\n",
      "[149/500] train_loss: 0.91856 valid_loss: 0.72477\n",
      "Validation loss decreased (0.724913 --> 0.724772).  Saving model ...\n",
      "[150/500] train_loss: 0.91775 valid_loss: 0.72520\n",
      "[151/500] train_loss: 0.91481 valid_loss: 0.72514\n",
      "[152/500] train_loss: 0.91851 valid_loss: 0.72526\n",
      "[153/500] train_loss: 0.91314 valid_loss: 0.72497\n",
      "[154/500] train_loss: 0.91192 valid_loss: 0.72517\n",
      "[155/500] train_loss: 0.91632 valid_loss: 0.72527\n",
      "[156/500] train_loss: 0.91619 valid_loss: 0.72493\n",
      "[157/500] train_loss: 0.91192 valid_loss: 0.72478\n",
      "[158/500] train_loss: 0.91509 valid_loss: 0.72522\n",
      "[159/500] train_loss: 0.91682 valid_loss: 0.72474\n",
      "Validation loss decreased (0.724772 --> 0.724739).  Saving model ...\n",
      "[160/500] train_loss: 0.91219 valid_loss: 0.72476\n",
      "Epoch 00160: reducing learning rate of group 0 to 1.4000e-02.\n",
      "[161/500] train_loss: 0.89699 valid_loss: 0.71415\n",
      "Validation loss decreased (0.724739 --> 0.714155).  Saving model ...\n",
      "[162/500] train_loss: 0.89159 valid_loss: 0.71163\n",
      "Validation loss decreased (0.714155 --> 0.711630).  Saving model ...\n",
      "[163/500] train_loss: 0.89999 valid_loss: 0.70908\n",
      "Validation loss decreased (0.711630 --> 0.709077).  Saving model ...\n",
      "[164/500] train_loss: 0.89680 valid_loss: 0.70802\n",
      "Validation loss decreased (0.709077 --> 0.708016).  Saving model ...\n",
      "[165/500] train_loss: 0.89456 valid_loss: 0.70646\n",
      "Validation loss decreased (0.708016 --> 0.706462).  Saving model ...\n",
      "[166/500] train_loss: 0.89497 valid_loss: 0.70623\n",
      "Validation loss decreased (0.706462 --> 0.706225).  Saving model ...\n",
      "[167/500] train_loss: 0.89617 valid_loss: 0.70643\n",
      "[168/500] train_loss: 0.89292 valid_loss: 0.70620\n",
      "Validation loss decreased (0.706225 --> 0.706197).  Saving model ...\n",
      "[169/500] train_loss: 0.89736 valid_loss: 0.70526\n",
      "Validation loss decreased (0.706197 --> 0.705256).  Saving model ...\n",
      "[170/500] train_loss: 0.89724 valid_loss: 0.70503\n",
      "Validation loss decreased (0.705256 --> 0.705035).  Saving model ...\n",
      "[171/500] train_loss: 0.89340 valid_loss: 0.70496\n",
      "Validation loss decreased (0.705035 --> 0.704964).  Saving model ...\n",
      "[172/500] train_loss: 0.90048 valid_loss: 0.70460\n",
      "Validation loss decreased (0.704964 --> 0.704603).  Saving model ...\n",
      "[173/500] train_loss: 0.89537 valid_loss: 0.70444\n",
      "Validation loss decreased (0.704603 --> 0.704436).  Saving model ...\n",
      "[174/500] train_loss: 0.89106 valid_loss: 0.70436\n",
      "Validation loss decreased (0.704436 --> 0.704361).  Saving model ...\n",
      "[175/500] train_loss: 0.89468 valid_loss: 0.70388\n",
      "Validation loss decreased (0.704361 --> 0.703876).  Saving model ...\n",
      "[176/500] train_loss: 0.89294 valid_loss: 0.70411\n",
      "[177/500] train_loss: 0.89366 valid_loss: 0.70443\n",
      "[178/500] train_loss: 0.89489 valid_loss: 0.70425\n",
      "[179/500] train_loss: 0.90043 valid_loss: 0.70460\n",
      "[180/500] train_loss: 0.88970 valid_loss: 0.70463\n",
      "[181/500] train_loss: 0.89014 valid_loss: 0.70414\n",
      "[182/500] train_loss: 0.89173 valid_loss: 0.70357\n",
      "Validation loss decreased (0.703876 --> 0.703574).  Saving model ...\n",
      "[183/500] train_loss: 0.89264 valid_loss: 0.70377\n",
      "[184/500] train_loss: 0.88824 valid_loss: 0.70412\n",
      "[185/500] train_loss: 0.89320 valid_loss: 0.70378\n",
      "[186/500] train_loss: 0.89198 valid_loss: 0.70392\n",
      "[187/500] train_loss: 0.89013 valid_loss: 0.70337\n",
      "Validation loss decreased (0.703574 --> 0.703373).  Saving model ...\n",
      "[188/500] train_loss: 0.89206 valid_loss: 0.70372\n",
      "[189/500] train_loss: 0.88796 valid_loss: 0.70364\n",
      "[190/500] train_loss: 0.88979 valid_loss: 0.70407\n",
      "[191/500] train_loss: 0.89271 valid_loss: 0.70379\n",
      "[192/500] train_loss: 0.89171 valid_loss: 0.70328\n",
      "Validation loss decreased (0.703373 --> 0.703280).  Saving model ...\n",
      "[193/500] train_loss: 0.89010 valid_loss: 0.70379\n",
      "[194/500] train_loss: 0.89032 valid_loss: 0.70363\n",
      "[195/500] train_loss: 0.89050 valid_loss: 0.70422\n",
      "[196/500] train_loss: 0.89308 valid_loss: 0.70436\n",
      "[197/500] train_loss: 0.89023 valid_loss: 0.70414\n",
      "[198/500] train_loss: 0.89013 valid_loss: 0.70428\n",
      "[199/500] train_loss: 0.89181 valid_loss: 0.70433\n",
      "[200/500] train_loss: 0.89257 valid_loss: 0.72712\n",
      "[201/500] train_loss: 0.88892 valid_loss: 0.70441\n",
      "[202/500] train_loss: 0.89099 valid_loss: 0.70430\n",
      "[203/500] train_loss: 0.88950 valid_loss: 0.70380\n",
      "Epoch 00203: reducing learning rate of group 0 to 9.8000e-03.\n",
      "[204/500] train_loss: 0.88646 valid_loss: 0.69955\n",
      "Validation loss decreased (0.703280 --> 0.699547).  Saving model ...\n",
      "[205/500] train_loss: 0.88694 valid_loss: 0.69798\n",
      "Validation loss decreased (0.699547 --> 0.697977).  Saving model ...\n",
      "[206/500] train_loss: 0.88516 valid_loss: 0.69756\n",
      "Validation loss decreased (0.697977 --> 0.697563).  Saving model ...\n",
      "[207/500] train_loss: 0.87677 valid_loss: 0.69686\n",
      "Validation loss decreased (0.697563 --> 0.696855).  Saving model ...\n",
      "[208/500] train_loss: 0.88154 valid_loss: 0.69671\n",
      "Validation loss decreased (0.696855 --> 0.696707).  Saving model ...\n",
      "[209/500] train_loss: 0.88512 valid_loss: 0.69689\n",
      "[210/500] train_loss: 0.88198 valid_loss: 0.69676\n",
      "[211/500] train_loss: 0.87818 valid_loss: 0.69672\n",
      "[212/500] train_loss: 0.88249 valid_loss: 0.69682\n",
      "[213/500] train_loss: 0.88003 valid_loss: 0.69594\n",
      "Validation loss decreased (0.696707 --> 0.695941).  Saving model ...\n",
      "[214/500] train_loss: 0.88144 valid_loss: 0.69601\n",
      "[215/500] train_loss: 0.88294 valid_loss: 0.69585\n",
      "Validation loss decreased (0.695941 --> 0.695853).  Saving model ...\n",
      "[216/500] train_loss: 0.87938 valid_loss: 0.69622\n",
      "[217/500] train_loss: 0.88094 valid_loss: 0.69622\n",
      "[218/500] train_loss: 0.87938 valid_loss: 0.69655\n",
      "[219/500] train_loss: 0.87845 valid_loss: 0.69635\n",
      "[220/500] train_loss: 0.87962 valid_loss: 0.69637\n",
      "[221/500] train_loss: 0.87898 valid_loss: 0.69644\n",
      "[222/500] train_loss: 0.87911 valid_loss: 0.69598\n",
      "[223/500] train_loss: 0.87832 valid_loss: 0.69563\n",
      "Validation loss decreased (0.695853 --> 0.695631).  Saving model ...\n",
      "[224/500] train_loss: 0.87794 valid_loss: 0.69579\n",
      "[225/500] train_loss: 0.88155 valid_loss: 0.69569\n",
      "[226/500] train_loss: 0.88121 valid_loss: 0.69603\n",
      "[227/500] train_loss: 0.87672 valid_loss: 0.69610\n",
      "[228/500] train_loss: 0.87970 valid_loss: 0.69598\n",
      "[229/500] train_loss: 0.87917 valid_loss: 0.69578\n",
      "[230/500] train_loss: 0.88013 valid_loss: 0.69575\n",
      "[231/500] train_loss: 0.88388 valid_loss: 0.69577\n",
      "[232/500] train_loss: 0.88039 valid_loss: 0.69581\n",
      "[233/500] train_loss: 0.87939 valid_loss: 0.69604\n",
      "[234/500] train_loss: 0.87699 valid_loss: 0.69604\n",
      "Epoch 00234: reducing learning rate of group 0 to 6.8600e-03.\n",
      "[235/500] train_loss: 0.87122 valid_loss: 0.69423\n",
      "Validation loss decreased (0.695631 --> 0.694229).  Saving model ...\n",
      "[236/500] train_loss: 0.87411 valid_loss: 0.69334\n",
      "Validation loss decreased (0.694229 --> 0.693343).  Saving model ...\n",
      "[237/500] train_loss: 0.87231 valid_loss: 0.69304\n",
      "Validation loss decreased (0.693343 --> 0.693037).  Saving model ...\n",
      "[238/500] train_loss: 0.87880 valid_loss: 0.69329\n",
      "[239/500] train_loss: 0.87414 valid_loss: 0.69341\n",
      "[240/500] train_loss: 0.87713 valid_loss: 0.69369\n",
      "[241/500] train_loss: 0.87862 valid_loss: 0.69352\n",
      "[242/500] train_loss: 0.87266 valid_loss: 0.69330\n",
      "[243/500] train_loss: 0.87742 valid_loss: 0.69374\n",
      "[244/500] train_loss: 0.87520 valid_loss: 0.69341\n",
      "[245/500] train_loss: 0.87810 valid_loss: 0.69358\n",
      "[246/500] train_loss: 0.88011 valid_loss: 0.69331\n",
      "[247/500] train_loss: 0.87676 valid_loss: 0.69329\n",
      "[248/500] train_loss: 0.86994 valid_loss: 0.69308\n",
      "Epoch 00248: reducing learning rate of group 0 to 4.8020e-03.\n",
      "[249/500] train_loss: 0.87185 valid_loss: 0.69259\n",
      "Validation loss decreased (0.693037 --> 0.692595).  Saving model ...\n",
      "[250/500] train_loss: 0.87142 valid_loss: 0.69234\n",
      "Validation loss decreased (0.692595 --> 0.692339).  Saving model ...\n",
      "[251/500] train_loss: 0.86741 valid_loss: 0.69236\n",
      "[252/500] train_loss: 0.86862 valid_loss: 0.69251\n",
      "[253/500] train_loss: 0.87081 valid_loss: 0.69254\n",
      "[254/500] train_loss: 0.87216 valid_loss: 0.69243\n",
      "[255/500] train_loss: 0.86868 valid_loss: 0.69258\n",
      "[256/500] train_loss: 0.87377 valid_loss: 0.69228\n",
      "Validation loss decreased (0.692339 --> 0.692281).  Saving model ...\n",
      "[257/500] train_loss: 0.87238 valid_loss: 0.69221\n",
      "Validation loss decreased (0.692281 --> 0.692208).  Saving model ...\n",
      "[258/500] train_loss: 0.87321 valid_loss: 0.69233\n",
      "[259/500] train_loss: 0.87454 valid_loss: 0.69223\n",
      "[260/500] train_loss: 0.87438 valid_loss: 0.69248\n",
      "[261/500] train_loss: 0.87438 valid_loss: 0.69226\n",
      "[262/500] train_loss: 0.86914 valid_loss: 0.69214\n",
      "Validation loss decreased (0.692208 --> 0.692144).  Saving model ...\n",
      "[263/500] train_loss: 0.87397 valid_loss: 0.69200\n",
      "Validation loss decreased (0.692144 --> 0.692003).  Saving model ...\n",
      "[264/500] train_loss: 0.86869 valid_loss: 0.69200\n",
      "Validation loss decreased (0.692003 --> 0.691999).  Saving model ...\n",
      "[265/500] train_loss: 0.87201 valid_loss: 0.69207\n",
      "[266/500] train_loss: 0.87003 valid_loss: 0.69203\n",
      "[267/500] train_loss: 0.87140 valid_loss: 0.69166\n",
      "Validation loss decreased (0.691999 --> 0.691664).  Saving model ...\n",
      "[268/500] train_loss: 0.87288 valid_loss: 0.69212\n",
      "[269/500] train_loss: 0.87425 valid_loss: 0.69202\n",
      "[270/500] train_loss: 0.87253 valid_loss: 0.69187\n",
      "[271/500] train_loss: 0.87148 valid_loss: 0.69168\n",
      "[272/500] train_loss: 0.87712 valid_loss: 0.69173\n",
      "[273/500] train_loss: 0.87321 valid_loss: 0.69188\n",
      "[274/500] train_loss: 0.87116 valid_loss: 0.69181\n",
      "[275/500] train_loss: 0.87247 valid_loss: 0.69152\n",
      "Validation loss decreased (0.691664 --> 0.691517).  Saving model ...\n",
      "[276/500] train_loss: 0.87127 valid_loss: 0.69160\n",
      "[277/500] train_loss: 0.87404 valid_loss: 0.69155\n",
      "[278/500] train_loss: 0.86951 valid_loss: 0.69156\n",
      "[279/500] train_loss: 0.87395 valid_loss: 0.69162\n",
      "[280/500] train_loss: 0.86874 valid_loss: 0.69176\n",
      "[281/500] train_loss: 0.86941 valid_loss: 0.69161\n",
      "[282/500] train_loss: 0.87417 valid_loss: 0.69164\n",
      "[283/500] train_loss: 0.86466 valid_loss: 0.69177\n",
      "[284/500] train_loss: 0.87000 valid_loss: 0.69183\n",
      "[285/500] train_loss: 0.87042 valid_loss: 0.69189\n",
      "[286/500] train_loss: 0.87030 valid_loss: 0.69177\n",
      "Epoch 00286: reducing learning rate of group 0 to 3.3614e-03.\n",
      "[287/500] train_loss: 0.86720 valid_loss: 0.69139\n",
      "Validation loss decreased (0.691517 --> 0.691392).  Saving model ...\n",
      "[288/500] train_loss: 0.87265 valid_loss: 0.69250\n",
      "[289/500] train_loss: 0.86859 valid_loss: 0.69218\n",
      "[290/500] train_loss: 0.86927 valid_loss: 0.69195\n",
      "[291/500] train_loss: 0.87065 valid_loss: 0.69185\n",
      "[292/500] train_loss: 0.87455 valid_loss: 0.69176\n",
      "[293/500] train_loss: 0.86793 valid_loss: 0.69177\n",
      "[294/500] train_loss: 0.86981 valid_loss: 0.69166\n",
      "[295/500] train_loss: 0.87020 valid_loss: 0.69156\n",
      "[296/500] train_loss: 0.86843 valid_loss: 0.69152\n",
      "[297/500] train_loss: 0.87039 valid_loss: 0.69151\n",
      "[298/500] train_loss: 0.87092 valid_loss: 0.69153\n",
      "Epoch 00298: reducing learning rate of group 0 to 2.3530e-03.\n",
      "[299/500] train_loss: 0.86841 valid_loss: 0.69152\n",
      "[300/500] train_loss: 0.86792 valid_loss: 0.71389\n",
      "[301/500] train_loss: 0.86812 valid_loss: 0.69163\n",
      "[302/500] train_loss: 0.86361 valid_loss: 0.69162\n",
      "[303/500] train_loss: 0.86673 valid_loss: 0.69164\n",
      "[304/500] train_loss: 0.86309 valid_loss: 0.69158\n",
      "[305/500] train_loss: 0.86385 valid_loss: 0.69161\n",
      "[306/500] train_loss: 0.86340 valid_loss: 0.69159\n",
      "[307/500] train_loss: 0.86686 valid_loss: 0.69163\n",
      "[308/500] train_loss: 0.86938 valid_loss: 0.69162\n",
      "[309/500] train_loss: 0.86852 valid_loss: 0.69160\n",
      "Epoch 00309: reducing learning rate of group 0 to 1.6471e-03.\n",
      "[310/500] train_loss: 0.86533 valid_loss: 0.69165\n",
      "[311/500] train_loss: 0.86957 valid_loss: 0.69165\n",
      "[312/500] train_loss: 0.86679 valid_loss: 0.69166\n",
      "[313/500] train_loss: 0.86802 valid_loss: 0.69167\n",
      "[314/500] train_loss: 0.86640 valid_loss: 0.69166\n",
      "[315/500] train_loss: 0.86691 valid_loss: 0.69165\n",
      "[316/500] train_loss: 0.86696 valid_loss: 0.69170\n",
      "[317/500] train_loss: 0.86756 valid_loss: 0.69179\n",
      "[318/500] train_loss: 0.86702 valid_loss: 0.69182\n",
      "[319/500] train_loss: 0.86450 valid_loss: 0.69181\n",
      "[320/500] train_loss: 0.86870 valid_loss: 0.69183\n",
      "Epoch 00320: reducing learning rate of group 0 to 1.1530e-03.\n",
      "[321/500] train_loss: 0.86845 valid_loss: 0.69155\n",
      "[322/500] train_loss: 0.86695 valid_loss: 0.69162\n",
      "[323/500] train_loss: 0.86773 valid_loss: 0.69163\n",
      "[324/500] train_loss: 0.86277 valid_loss: 0.69166\n",
      "[325/500] train_loss: 0.86542 valid_loss: 0.69172\n",
      "[326/500] train_loss: 0.86848 valid_loss: 0.69174\n",
      "[327/500] train_loss: 0.86482 valid_loss: 0.69178\n",
      "[328/500] train_loss: 0.86852 valid_loss: 0.69179\n",
      "[329/500] train_loss: 0.86435 valid_loss: 0.69188\n",
      "[330/500] train_loss: 0.86767 valid_loss: 0.69177\n",
      "[331/500] train_loss: 0.86695 valid_loss: 0.69183\n",
      "Epoch 00331: reducing learning rate of group 0 to 8.0707e-04.\n",
      "[332/500] train_loss: 0.86446 valid_loss: 0.69160\n",
      "[333/500] train_loss: 0.86735 valid_loss: 0.69166\n",
      "[334/500] train_loss: 0.86726 valid_loss: 0.69164\n",
      "[335/500] train_loss: 0.86753 valid_loss: 0.69170\n",
      "[336/500] train_loss: 0.86468 valid_loss: 0.69170\n",
      "[337/500] train_loss: 0.86530 valid_loss: 0.69170\n",
      "Early stopping\n",
      "[  1/500] train_loss: 1.47647 valid_loss: 1.07921\n",
      "Validation loss decreased (inf --> 1.079212).  Saving model ...\n",
      "[  2/500] train_loss: 1.18404 valid_loss: 0.99521\n",
      "Validation loss decreased (1.079212 --> 0.995213).  Saving model ...\n",
      "[  3/500] train_loss: 1.11102 valid_loss: 0.95099\n",
      "Validation loss decreased (0.995213 --> 0.950988).  Saving model ...\n",
      "[  4/500] train_loss: 1.07554 valid_loss: 0.92203\n",
      "Validation loss decreased (0.950988 --> 0.922030).  Saving model ...\n",
      "[  5/500] train_loss: 1.04072 valid_loss: 0.90247\n",
      "Validation loss decreased (0.922030 --> 0.902469).  Saving model ...\n",
      "[  6/500] train_loss: 1.02794 valid_loss: 0.88939\n",
      "Validation loss decreased (0.902469 --> 0.889394).  Saving model ...\n",
      "[  7/500] train_loss: 1.00889 valid_loss: 0.87855\n",
      "Validation loss decreased (0.889394 --> 0.878547).  Saving model ...\n",
      "[  8/500] train_loss: 0.99981 valid_loss: 0.87125\n",
      "Validation loss decreased (0.878547 --> 0.871246).  Saving model ...\n",
      "[  9/500] train_loss: 0.98201 valid_loss: 0.86448\n",
      "Validation loss decreased (0.871246 --> 0.864480).  Saving model ...\n",
      "[ 10/500] train_loss: 0.97876 valid_loss: 0.85935\n",
      "Validation loss decreased (0.864480 --> 0.859354).  Saving model ...\n",
      "[ 11/500] train_loss: 0.97502 valid_loss: 0.85440\n",
      "Validation loss decreased (0.859354 --> 0.854402).  Saving model ...\n",
      "[ 12/500] train_loss: 0.96185 valid_loss: 0.85107\n",
      "Validation loss decreased (0.854402 --> 0.851074).  Saving model ...\n",
      "[ 13/500] train_loss: 0.95512 valid_loss: 0.84796\n",
      "Validation loss decreased (0.851074 --> 0.847962).  Saving model ...\n",
      "[ 14/500] train_loss: 0.95104 valid_loss: 0.84492\n",
      "Validation loss decreased (0.847962 --> 0.844918).  Saving model ...\n",
      "[ 15/500] train_loss: 0.95010 valid_loss: 0.84183\n",
      "Validation loss decreased (0.844918 --> 0.841826).  Saving model ...\n",
      "[ 16/500] train_loss: 0.94415 valid_loss: 0.83933\n",
      "Validation loss decreased (0.841826 --> 0.839331).  Saving model ...\n",
      "[ 17/500] train_loss: 0.93902 valid_loss: 0.83730\n",
      "Validation loss decreased (0.839331 --> 0.837298).  Saving model ...\n",
      "[ 18/500] train_loss: 0.93883 valid_loss: 0.83502\n",
      "Validation loss decreased (0.837298 --> 0.835024).  Saving model ...\n",
      "[ 19/500] train_loss: 0.93304 valid_loss: 0.83304\n",
      "Validation loss decreased (0.835024 --> 0.833043).  Saving model ...\n",
      "[ 20/500] train_loss: 0.92502 valid_loss: 0.83108\n",
      "Validation loss decreased (0.833043 --> 0.831085).  Saving model ...\n",
      "[ 21/500] train_loss: 0.92568 valid_loss: 0.82960\n",
      "Validation loss decreased (0.831085 --> 0.829600).  Saving model ...\n",
      "[ 22/500] train_loss: 0.91795 valid_loss: 0.82784\n",
      "Validation loss decreased (0.829600 --> 0.827844).  Saving model ...\n",
      "[ 23/500] train_loss: 0.91697 valid_loss: 0.82703\n",
      "Validation loss decreased (0.827844 --> 0.827033).  Saving model ...\n",
      "[ 24/500] train_loss: 0.91501 valid_loss: 0.82618\n",
      "Validation loss decreased (0.827033 --> 0.826183).  Saving model ...\n",
      "[ 25/500] train_loss: 0.91169 valid_loss: 0.82494\n",
      "Validation loss decreased (0.826183 --> 0.824941).  Saving model ...\n",
      "[ 26/500] train_loss: 0.91789 valid_loss: 0.82353\n",
      "Validation loss decreased (0.824941 --> 0.823530).  Saving model ...\n",
      "[ 27/500] train_loss: 0.91031 valid_loss: 0.82278\n",
      "Validation loss decreased (0.823530 --> 0.822776).  Saving model ...\n",
      "[ 28/500] train_loss: 0.90729 valid_loss: 0.82176\n",
      "Validation loss decreased (0.822776 --> 0.821760).  Saving model ...\n",
      "[ 29/500] train_loss: 0.90564 valid_loss: 0.82147\n",
      "Validation loss decreased (0.821760 --> 0.821466).  Saving model ...\n",
      "[ 30/500] train_loss: 0.90904 valid_loss: 0.82150\n",
      "[ 31/500] train_loss: 0.90606 valid_loss: 0.82113\n",
      "Validation loss decreased (0.821466 --> 0.821127).  Saving model ...\n",
      "[ 32/500] train_loss: 0.89887 valid_loss: 0.82041\n",
      "Validation loss decreased (0.821127 --> 0.820408).  Saving model ...\n",
      "[ 33/500] train_loss: 0.90072 valid_loss: 0.82007\n",
      "Validation loss decreased (0.820408 --> 0.820067).  Saving model ...\n",
      "[ 34/500] train_loss: 0.88639 valid_loss: 0.81918\n",
      "Validation loss decreased (0.820067 --> 0.819180).  Saving model ...\n",
      "[ 35/500] train_loss: 0.89314 valid_loss: 0.81867\n",
      "Validation loss decreased (0.819180 --> 0.818673).  Saving model ...\n",
      "[ 36/500] train_loss: 0.88948 valid_loss: 0.81838\n",
      "Validation loss decreased (0.818673 --> 0.818378).  Saving model ...\n",
      "[ 37/500] train_loss: 0.89440 valid_loss: 0.81817\n",
      "Validation loss decreased (0.818378 --> 0.818173).  Saving model ...\n",
      "[ 38/500] train_loss: 0.89460 valid_loss: 0.81749\n",
      "Validation loss decreased (0.818173 --> 0.817487).  Saving model ...\n",
      "[ 39/500] train_loss: 0.89368 valid_loss: 0.81721\n",
      "Validation loss decreased (0.817487 --> 0.817208).  Saving model ...\n",
      "[ 40/500] train_loss: 0.88982 valid_loss: 0.81692\n",
      "Validation loss decreased (0.817208 --> 0.816924).  Saving model ...\n",
      "[ 41/500] train_loss: 0.88974 valid_loss: 0.81680\n",
      "Validation loss decreased (0.816924 --> 0.816804).  Saving model ...\n",
      "[ 42/500] train_loss: 0.88053 valid_loss: 0.81665\n",
      "Validation loss decreased (0.816804 --> 0.816650).  Saving model ...\n",
      "[ 43/500] train_loss: 0.88543 valid_loss: 0.81637\n",
      "Validation loss decreased (0.816650 --> 0.816373).  Saving model ...\n",
      "[ 44/500] train_loss: 0.87928 valid_loss: 0.81617\n",
      "Validation loss decreased (0.816373 --> 0.816173).  Saving model ...\n",
      "[ 45/500] train_loss: 0.88803 valid_loss: 0.81602\n",
      "Validation loss decreased (0.816173 --> 0.816020).  Saving model ...\n",
      "[ 46/500] train_loss: 0.88236 valid_loss: 0.81578\n",
      "Validation loss decreased (0.816020 --> 0.815776).  Saving model ...\n",
      "[ 47/500] train_loss: 0.88366 valid_loss: 0.81562\n",
      "Validation loss decreased (0.815776 --> 0.815623).  Saving model ...\n",
      "[ 48/500] train_loss: 0.88514 valid_loss: 0.81599\n",
      "[ 49/500] train_loss: 0.88028 valid_loss: 0.81604\n",
      "[ 50/500] train_loss: 0.87730 valid_loss: 0.81584\n",
      "[ 51/500] train_loss: 0.88075 valid_loss: 0.81624\n",
      "[ 52/500] train_loss: 0.88196 valid_loss: 0.81655\n",
      "[ 53/500] train_loss: 0.87757 valid_loss: 0.81655\n",
      "[ 54/500] train_loss: 0.87449 valid_loss: 0.81575\n",
      "[ 55/500] train_loss: 0.87837 valid_loss: 0.81565\n",
      "[ 56/500] train_loss: 0.87753 valid_loss: 0.81591\n",
      "[ 57/500] train_loss: 0.88042 valid_loss: 0.81540\n",
      "Validation loss decreased (0.815623 --> 0.815398).  Saving model ...\n",
      "[ 58/500] train_loss: 0.87703 valid_loss: 0.81515\n",
      "Validation loss decreased (0.815398 --> 0.815147).  Saving model ...\n",
      "[ 59/500] train_loss: 0.87563 valid_loss: 0.81450\n",
      "Validation loss decreased (0.815147 --> 0.814499).  Saving model ...\n",
      "[ 60/500] train_loss: 0.87436 valid_loss: 0.81460\n",
      "[ 61/500] train_loss: 0.87920 valid_loss: 0.81496\n",
      "[ 62/500] train_loss: 0.87472 valid_loss: 0.81462\n",
      "[ 63/500] train_loss: 0.86689 valid_loss: 0.81453\n",
      "[ 64/500] train_loss: 0.87050 valid_loss: 0.81412\n",
      "Validation loss decreased (0.814499 --> 0.814124).  Saving model ...\n",
      "[ 65/500] train_loss: 0.86898 valid_loss: 0.81400\n",
      "Validation loss decreased (0.814124 --> 0.814003).  Saving model ...\n",
      "[ 66/500] train_loss: 0.86800 valid_loss: 0.81317\n",
      "Validation loss decreased (0.814003 --> 0.813171).  Saving model ...\n",
      "[ 67/500] train_loss: 0.87182 valid_loss: 0.81316\n",
      "Validation loss decreased (0.813171 --> 0.813158).  Saving model ...\n",
      "[ 68/500] train_loss: 0.86269 valid_loss: 0.81317\n",
      "[ 69/500] train_loss: 0.86582 valid_loss: 0.81271\n",
      "Validation loss decreased (0.813158 --> 0.812712).  Saving model ...\n",
      "[ 70/500] train_loss: 0.87238 valid_loss: 0.81283\n",
      "[ 71/500] train_loss: 0.86443 valid_loss: 0.81297\n",
      "[ 72/500] train_loss: 0.86912 valid_loss: 0.81227\n",
      "Validation loss decreased (0.812712 --> 0.812273).  Saving model ...\n",
      "[ 73/500] train_loss: 0.86446 valid_loss: 0.81241\n",
      "[ 74/500] train_loss: 0.86484 valid_loss: 0.81212\n",
      "Validation loss decreased (0.812273 --> 0.812117).  Saving model ...\n",
      "[ 75/500] train_loss: 0.86634 valid_loss: 0.81228\n",
      "[ 76/500] train_loss: 0.86488 valid_loss: 0.81193\n",
      "Validation loss decreased (0.812117 --> 0.811929).  Saving model ...\n",
      "[ 77/500] train_loss: 0.86211 valid_loss: 0.81198\n",
      "[ 78/500] train_loss: 0.86530 valid_loss: 0.81200\n",
      "[ 79/500] train_loss: 0.86158 valid_loss: 0.81207\n",
      "[ 80/500] train_loss: 0.86214 valid_loss: 0.81223\n",
      "[ 81/500] train_loss: 0.86870 valid_loss: 0.81204\n",
      "[ 82/500] train_loss: 0.85881 valid_loss: 0.81142\n",
      "Validation loss decreased (0.811929 --> 0.811419).  Saving model ...\n",
      "[ 83/500] train_loss: 0.86509 valid_loss: 0.81067\n",
      "Validation loss decreased (0.811419 --> 0.810674).  Saving model ...\n",
      "[ 84/500] train_loss: 0.86546 valid_loss: 0.81064\n",
      "Validation loss decreased (0.810674 --> 0.810641).  Saving model ...\n",
      "[ 85/500] train_loss: 0.85698 valid_loss: 0.81018\n",
      "Validation loss decreased (0.810641 --> 0.810184).  Saving model ...\n",
      "[ 86/500] train_loss: 0.85712 valid_loss: 0.81011\n",
      "Validation loss decreased (0.810184 --> 0.810108).  Saving model ...\n",
      "[ 87/500] train_loss: 0.85986 valid_loss: 0.80966\n",
      "Validation loss decreased (0.810108 --> 0.809662).  Saving model ...\n",
      "[ 88/500] train_loss: 0.85894 valid_loss: 0.80917\n",
      "Validation loss decreased (0.809662 --> 0.809167).  Saving model ...\n",
      "[ 89/500] train_loss: 0.86613 valid_loss: 0.80948\n",
      "[ 90/500] train_loss: 0.85852 valid_loss: 0.80945\n",
      "[ 91/500] train_loss: 0.85718 valid_loss: 0.80893\n",
      "Validation loss decreased (0.809167 --> 0.808931).  Saving model ...\n",
      "[ 92/500] train_loss: 0.86227 valid_loss: 0.80895\n",
      "[ 93/500] train_loss: 0.85436 valid_loss: 0.80879\n",
      "Validation loss decreased (0.808931 --> 0.808793).  Saving model ...\n",
      "[ 94/500] train_loss: 0.86308 valid_loss: 0.80877\n",
      "Validation loss decreased (0.808793 --> 0.808766).  Saving model ...\n",
      "[ 95/500] train_loss: 0.86035 valid_loss: 0.80873\n",
      "Validation loss decreased (0.808766 --> 0.808729).  Saving model ...\n",
      "[ 96/500] train_loss: 0.85658 valid_loss: 0.80844\n",
      "Validation loss decreased (0.808729 --> 0.808436).  Saving model ...\n",
      "[ 97/500] train_loss: 0.85125 valid_loss: 0.80839\n",
      "Validation loss decreased (0.808436 --> 0.808389).  Saving model ...\n",
      "[ 98/500] train_loss: 0.85763 valid_loss: 0.80801\n",
      "Validation loss decreased (0.808389 --> 0.808009).  Saving model ...\n",
      "[ 99/500] train_loss: 0.85138 valid_loss: 0.80826\n",
      "[100/500] train_loss: 0.85297 valid_loss: 0.83373\n",
      "[101/500] train_loss: 0.85803 valid_loss: 0.80782\n",
      "Validation loss decreased (0.808009 --> 0.807821).  Saving model ...\n",
      "[102/500] train_loss: 0.85523 valid_loss: 0.80727\n",
      "Validation loss decreased (0.807821 --> 0.807271).  Saving model ...\n",
      "[103/500] train_loss: 0.85115 valid_loss: 0.80727\n",
      "Validation loss decreased (0.807271 --> 0.807268).  Saving model ...\n",
      "[104/500] train_loss: 0.85361 valid_loss: 0.80694\n",
      "Validation loss decreased (0.807268 --> 0.806935).  Saving model ...\n",
      "[105/500] train_loss: 0.85341 valid_loss: 0.80674\n",
      "Validation loss decreased (0.806935 --> 0.806740).  Saving model ...\n",
      "[106/500] train_loss: 0.85150 valid_loss: 0.80660\n",
      "Validation loss decreased (0.806740 --> 0.806602).  Saving model ...\n",
      "[107/500] train_loss: 0.85186 valid_loss: 0.80666\n",
      "[108/500] train_loss: 0.84856 valid_loss: 0.80677\n",
      "[109/500] train_loss: 0.85085 valid_loss: 0.80659\n",
      "Validation loss decreased (0.806602 --> 0.806593).  Saving model ...\n",
      "[110/500] train_loss: 0.84717 valid_loss: 0.80658\n",
      "Validation loss decreased (0.806593 --> 0.806584).  Saving model ...\n",
      "[111/500] train_loss: 0.84702 valid_loss: 0.80638\n",
      "Validation loss decreased (0.806584 --> 0.806385).  Saving model ...\n",
      "[112/500] train_loss: 0.84590 valid_loss: 0.80658\n",
      "[113/500] train_loss: 0.85233 valid_loss: 0.80652\n",
      "[114/500] train_loss: 0.84796 valid_loss: 0.80618\n",
      "Validation loss decreased (0.806385 --> 0.806175).  Saving model ...\n",
      "[115/500] train_loss: 0.84895 valid_loss: 0.80594\n",
      "Validation loss decreased (0.806175 --> 0.805941).  Saving model ...\n",
      "[116/500] train_loss: 0.84986 valid_loss: 0.80557\n",
      "Validation loss decreased (0.805941 --> 0.805573).  Saving model ...\n",
      "[117/500] train_loss: 0.84850 valid_loss: 0.80529\n",
      "Validation loss decreased (0.805573 --> 0.805289).  Saving model ...\n",
      "[118/500] train_loss: 0.85020 valid_loss: 0.80518\n",
      "Validation loss decreased (0.805289 --> 0.805179).  Saving model ...\n",
      "[119/500] train_loss: 0.84517 valid_loss: 0.80555\n",
      "[120/500] train_loss: 0.84336 valid_loss: 0.80510\n",
      "Validation loss decreased (0.805179 --> 0.805102).  Saving model ...\n",
      "[121/500] train_loss: 0.84512 valid_loss: 0.80514\n",
      "[122/500] train_loss: 0.84748 valid_loss: 0.80477\n",
      "Validation loss decreased (0.805102 --> 0.804767).  Saving model ...\n",
      "[123/500] train_loss: 0.84981 valid_loss: 0.80464\n",
      "Validation loss decreased (0.804767 --> 0.804641).  Saving model ...\n",
      "[124/500] train_loss: 0.84505 valid_loss: 0.80498\n",
      "[125/500] train_loss: 0.84033 valid_loss: 0.80444\n",
      "Validation loss decreased (0.804641 --> 0.804443).  Saving model ...\n",
      "[126/500] train_loss: 0.84369 valid_loss: 0.80400\n",
      "Validation loss decreased (0.804443 --> 0.803997).  Saving model ...\n",
      "[127/500] train_loss: 0.84958 valid_loss: 0.80374\n",
      "Validation loss decreased (0.803997 --> 0.803739).  Saving model ...\n",
      "[128/500] train_loss: 0.84550 valid_loss: 0.80350\n",
      "Validation loss decreased (0.803739 --> 0.803504).  Saving model ...\n",
      "[129/500] train_loss: 0.84059 valid_loss: 0.80416\n",
      "[130/500] train_loss: 0.84323 valid_loss: 0.80385\n",
      "[131/500] train_loss: 0.84656 valid_loss: 0.80376\n",
      "[132/500] train_loss: 0.84456 valid_loss: 0.80404\n",
      "[133/500] train_loss: 0.84225 valid_loss: 0.80404\n",
      "[134/500] train_loss: 0.84499 valid_loss: 0.80389\n",
      "[135/500] train_loss: 0.84688 valid_loss: 0.80360\n",
      "[136/500] train_loss: 0.84116 valid_loss: 0.80352\n",
      "[137/500] train_loss: 0.84596 valid_loss: 0.80314\n",
      "Validation loss decreased (0.803504 --> 0.803141).  Saving model ...\n",
      "[138/500] train_loss: 0.84365 valid_loss: 0.80314\n",
      "Validation loss decreased (0.803141 --> 0.803135).  Saving model ...\n",
      "[139/500] train_loss: 0.84189 valid_loss: 0.80266\n",
      "Validation loss decreased (0.803135 --> 0.802657).  Saving model ...\n",
      "[140/500] train_loss: 0.84235 valid_loss: 0.80301\n",
      "[141/500] train_loss: 0.83794 valid_loss: 0.80272\n",
      "[142/500] train_loss: 0.83523 valid_loss: 0.80271\n",
      "[143/500] train_loss: 0.84124 valid_loss: 0.80211\n",
      "Validation loss decreased (0.802657 --> 0.802108).  Saving model ...\n",
      "[144/500] train_loss: 0.84287 valid_loss: 0.80210\n",
      "Validation loss decreased (0.802108 --> 0.802103).  Saving model ...\n",
      "[145/500] train_loss: 0.83340 valid_loss: 0.80206\n",
      "Validation loss decreased (0.802103 --> 0.802058).  Saving model ...\n",
      "[146/500] train_loss: 0.83720 valid_loss: 0.80208\n",
      "[147/500] train_loss: 0.83843 valid_loss: 0.80208\n",
      "[148/500] train_loss: 0.83743 valid_loss: 0.80196\n",
      "Validation loss decreased (0.802058 --> 0.801961).  Saving model ...\n",
      "[149/500] train_loss: 0.84215 valid_loss: 0.80194\n",
      "Validation loss decreased (0.801961 --> 0.801936).  Saving model ...\n",
      "[150/500] train_loss: 0.83867 valid_loss: 0.80212\n",
      "[151/500] train_loss: 0.83834 valid_loss: 0.80209\n",
      "[152/500] train_loss: 0.84254 valid_loss: 0.80208\n",
      "[153/500] train_loss: 0.84250 valid_loss: 0.80170\n",
      "Validation loss decreased (0.801936 --> 0.801701).  Saving model ...\n",
      "[154/500] train_loss: 0.84022 valid_loss: 0.80189\n",
      "[155/500] train_loss: 0.84045 valid_loss: 0.80169\n",
      "Validation loss decreased (0.801701 --> 0.801688).  Saving model ...\n",
      "[156/500] train_loss: 0.83347 valid_loss: 0.80212\n",
      "[157/500] train_loss: 0.83558 valid_loss: 0.80193\n",
      "[158/500] train_loss: 0.83803 valid_loss: 0.80178\n",
      "[159/500] train_loss: 0.83715 valid_loss: 0.80182\n",
      "[160/500] train_loss: 0.84121 valid_loss: 0.80139\n",
      "Validation loss decreased (0.801688 --> 0.801393).  Saving model ...\n",
      "[161/500] train_loss: 0.82985 valid_loss: 0.80076\n",
      "Validation loss decreased (0.801393 --> 0.800763).  Saving model ...\n",
      "[162/500] train_loss: 0.83235 valid_loss: 0.80064\n",
      "Validation loss decreased (0.800763 --> 0.800639).  Saving model ...\n",
      "[163/500] train_loss: 0.83705 valid_loss: 0.80083\n",
      "[164/500] train_loss: 0.83434 valid_loss: 0.80047\n",
      "Validation loss decreased (0.800639 --> 0.800468).  Saving model ...\n",
      "[165/500] train_loss: 0.83458 valid_loss: 0.80054\n",
      "[166/500] train_loss: 0.83470 valid_loss: 0.80080\n",
      "[167/500] train_loss: 0.83428 valid_loss: 0.80072\n",
      "[168/500] train_loss: 0.83337 valid_loss: 0.80036\n",
      "Validation loss decreased (0.800468 --> 0.800360).  Saving model ...\n",
      "[169/500] train_loss: 0.84233 valid_loss: 0.80011\n",
      "Validation loss decreased (0.800360 --> 0.800112).  Saving model ...\n",
      "[170/500] train_loss: 0.83307 valid_loss: 0.79993\n",
      "Validation loss decreased (0.800112 --> 0.799930).  Saving model ...\n",
      "[171/500] train_loss: 0.82797 valid_loss: 0.79924\n",
      "Validation loss decreased (0.799930 --> 0.799242).  Saving model ...\n",
      "[172/500] train_loss: 0.83253 valid_loss: 0.79906\n",
      "Validation loss decreased (0.799242 --> 0.799056).  Saving model ...\n",
      "[173/500] train_loss: 0.83155 valid_loss: 0.79956\n",
      "[174/500] train_loss: 0.83362 valid_loss: 0.79941\n",
      "[175/500] train_loss: 0.83582 valid_loss: 0.79918\n",
      "[176/500] train_loss: 0.82984 valid_loss: 0.79927\n",
      "[177/500] train_loss: 0.83237 valid_loss: 0.79926\n",
      "[178/500] train_loss: 0.83767 valid_loss: 0.79978\n",
      "[179/500] train_loss: 0.83330 valid_loss: 0.79961\n",
      "[180/500] train_loss: 0.83260 valid_loss: 0.79909\n",
      "[181/500] train_loss: 0.83175 valid_loss: 0.79897\n",
      "Validation loss decreased (0.799056 --> 0.798973).  Saving model ...\n",
      "[182/500] train_loss: 0.83010 valid_loss: 0.79861\n",
      "Validation loss decreased (0.798973 --> 0.798614).  Saving model ...\n",
      "[183/500] train_loss: 0.83655 valid_loss: 0.79856\n",
      "Validation loss decreased (0.798614 --> 0.798558).  Saving model ...\n",
      "[184/500] train_loss: 0.83131 valid_loss: 0.79826\n",
      "Validation loss decreased (0.798558 --> 0.798262).  Saving model ...\n",
      "[185/500] train_loss: 0.82764 valid_loss: 0.79813\n",
      "Validation loss decreased (0.798262 --> 0.798128).  Saving model ...\n",
      "[186/500] train_loss: 0.82735 valid_loss: 0.79801\n",
      "Validation loss decreased (0.798128 --> 0.798010).  Saving model ...\n",
      "[187/500] train_loss: 0.82772 valid_loss: 0.79804\n",
      "[188/500] train_loss: 0.83039 valid_loss: 0.79821\n",
      "[189/500] train_loss: 0.83137 valid_loss: 0.79803\n",
      "[190/500] train_loss: 0.83177 valid_loss: 0.79821\n",
      "[191/500] train_loss: 0.83148 valid_loss: 0.79829\n",
      "[192/500] train_loss: 0.82608 valid_loss: 0.79832\n",
      "[193/500] train_loss: 0.83152 valid_loss: 0.79864\n",
      "[194/500] train_loss: 0.82437 valid_loss: 0.79867\n",
      "[195/500] train_loss: 0.82312 valid_loss: 0.79849\n",
      "[196/500] train_loss: 0.82692 valid_loss: 0.79822\n",
      "[197/500] train_loss: 0.82862 valid_loss: 0.79805\n",
      "Epoch 00197: reducing learning rate of group 0 to 1.4000e-02.\n",
      "[198/500] train_loss: 0.80910 valid_loss: 0.79193\n",
      "Validation loss decreased (0.798010 --> 0.791934).  Saving model ...\n",
      "[199/500] train_loss: 0.80604 valid_loss: 0.78869\n",
      "Validation loss decreased (0.791934 --> 0.788688).  Saving model ...\n",
      "[200/500] train_loss: 0.80653 valid_loss: 0.81154\n",
      "[201/500] train_loss: 0.80303 valid_loss: 0.78411\n",
      "Validation loss decreased (0.788688 --> 0.784115).  Saving model ...\n",
      "[202/500] train_loss: 0.80799 valid_loss: 0.78362\n",
      "Validation loss decreased (0.784115 --> 0.783620).  Saving model ...\n",
      "[203/500] train_loss: 0.81366 valid_loss: 0.78358\n",
      "Validation loss decreased (0.783620 --> 0.783576).  Saving model ...\n",
      "[204/500] train_loss: 0.80764 valid_loss: 0.78270\n",
      "Validation loss decreased (0.783576 --> 0.782699).  Saving model ...\n",
      "[205/500] train_loss: 0.80551 valid_loss: 0.78248\n",
      "Validation loss decreased (0.782699 --> 0.782482).  Saving model ...\n",
      "[206/500] train_loss: 0.80860 valid_loss: 0.78208\n",
      "Validation loss decreased (0.782482 --> 0.782081).  Saving model ...\n",
      "[207/500] train_loss: 0.80537 valid_loss: 0.78173\n",
      "Validation loss decreased (0.782081 --> 0.781727).  Saving model ...\n",
      "[208/500] train_loss: 0.80527 valid_loss: 0.78096\n",
      "Validation loss decreased (0.781727 --> 0.780957).  Saving model ...\n",
      "[209/500] train_loss: 0.80856 valid_loss: 0.78081\n",
      "Validation loss decreased (0.780957 --> 0.780814).  Saving model ...\n",
      "[210/500] train_loss: 0.80414 valid_loss: 0.78069\n",
      "Validation loss decreased (0.780814 --> 0.780692).  Saving model ...\n",
      "[211/500] train_loss: 0.80335 valid_loss: 0.78016\n",
      "Validation loss decreased (0.780692 --> 0.780157).  Saving model ...\n",
      "[212/500] train_loss: 0.80621 valid_loss: 0.78040\n",
      "[213/500] train_loss: 0.80520 valid_loss: 0.78053\n",
      "[214/500] train_loss: 0.80598 valid_loss: 0.78031\n",
      "[215/500] train_loss: 0.81357 valid_loss: 0.78040\n",
      "[216/500] train_loss: 0.80490 valid_loss: 0.78006\n",
      "Validation loss decreased (0.780157 --> 0.780056).  Saving model ...\n",
      "[217/500] train_loss: 0.80922 valid_loss: 0.77983\n",
      "Validation loss decreased (0.780056 --> 0.779831).  Saving model ...\n",
      "[218/500] train_loss: 0.80572 valid_loss: 0.77927\n",
      "Validation loss decreased (0.779831 --> 0.779268).  Saving model ...\n",
      "[219/500] train_loss: 0.80446 valid_loss: 0.77936\n",
      "[220/500] train_loss: 0.80837 valid_loss: 0.77975\n",
      "[221/500] train_loss: 0.80590 valid_loss: 0.77992\n",
      "[222/500] train_loss: 0.80369 valid_loss: 0.77962\n",
      "[223/500] train_loss: 0.80461 valid_loss: 0.77924\n",
      "Validation loss decreased (0.779268 --> 0.779237).  Saving model ...\n",
      "[224/500] train_loss: 0.80425 valid_loss: 0.77874\n",
      "Validation loss decreased (0.779237 --> 0.778744).  Saving model ...\n",
      "[225/500] train_loss: 0.80556 valid_loss: 0.77916\n",
      "[226/500] train_loss: 0.80391 valid_loss: 0.77921\n",
      "[227/500] train_loss: 0.80350 valid_loss: 0.77940\n",
      "[228/500] train_loss: 0.80492 valid_loss: 0.77916\n",
      "[229/500] train_loss: 0.80628 valid_loss: 0.77939\n",
      "[230/500] train_loss: 0.80265 valid_loss: 0.77907\n",
      "[231/500] train_loss: 0.80324 valid_loss: 0.77896\n",
      "[232/500] train_loss: 0.80865 valid_loss: 0.77915\n",
      "[233/500] train_loss: 0.80166 valid_loss: 0.77923\n",
      "[234/500] train_loss: 0.80600 valid_loss: 0.77905\n",
      "[235/500] train_loss: 0.80380 valid_loss: 0.77879\n",
      "Epoch 00235: reducing learning rate of group 0 to 9.8000e-03.\n",
      "[236/500] train_loss: 0.79635 valid_loss: 0.77275\n",
      "Validation loss decreased (0.778744 --> 0.772749).  Saving model ...\n",
      "[237/500] train_loss: 0.79582 valid_loss: 0.77216\n",
      "Validation loss decreased (0.772749 --> 0.772162).  Saving model ...\n",
      "[238/500] train_loss: 0.79661 valid_loss: 0.77184\n",
      "Validation loss decreased (0.772162 --> 0.771839).  Saving model ...\n",
      "[239/500] train_loss: 0.79585 valid_loss: 0.77147\n",
      "Validation loss decreased (0.771839 --> 0.771472).  Saving model ...\n",
      "[240/500] train_loss: 0.79328 valid_loss: 0.77171\n",
      "[241/500] train_loss: 0.79890 valid_loss: 0.77164\n",
      "[242/500] train_loss: 0.79035 valid_loss: 0.77117\n",
      "Validation loss decreased (0.771472 --> 0.771171).  Saving model ...\n",
      "[243/500] train_loss: 0.79256 valid_loss: 0.77143\n",
      "[244/500] train_loss: 0.79451 valid_loss: 0.77082\n",
      "Validation loss decreased (0.771171 --> 0.770821).  Saving model ...\n",
      "[245/500] train_loss: 0.79880 valid_loss: 0.77089\n",
      "[246/500] train_loss: 0.79492 valid_loss: 0.77086\n",
      "[247/500] train_loss: 0.79351 valid_loss: 0.77094\n",
      "[248/500] train_loss: 0.79447 valid_loss: 0.77095\n",
      "[249/500] train_loss: 0.78795 valid_loss: 0.77042\n",
      "Validation loss decreased (0.770821 --> 0.770422).  Saving model ...\n",
      "[250/500] train_loss: 0.79477 valid_loss: 0.77090\n",
      "[251/500] train_loss: 0.79418 valid_loss: 0.77095\n",
      "[252/500] train_loss: 0.79158 valid_loss: 0.77103\n",
      "[253/500] train_loss: 0.79263 valid_loss: 0.77064\n",
      "[254/500] train_loss: 0.79435 valid_loss: 0.77060\n",
      "[255/500] train_loss: 0.79381 valid_loss: 0.77059\n",
      "[256/500] train_loss: 0.79009 valid_loss: 0.77052\n",
      "[257/500] train_loss: 0.79530 valid_loss: 0.77073\n",
      "[258/500] train_loss: 0.79562 valid_loss: 0.77047\n",
      "[259/500] train_loss: 0.79609 valid_loss: 0.77016\n",
      "Validation loss decreased (0.770422 --> 0.770164).  Saving model ...\n",
      "[260/500] train_loss: 0.79238 valid_loss: 0.76963\n",
      "Validation loss decreased (0.770164 --> 0.769626).  Saving model ...\n",
      "[261/500] train_loss: 0.79203 valid_loss: 0.77007\n",
      "[262/500] train_loss: 0.78717 valid_loss: 0.77021\n",
      "[263/500] train_loss: 0.79146 valid_loss: 0.76989\n",
      "[264/500] train_loss: 0.78834 valid_loss: 0.76917\n",
      "Validation loss decreased (0.769626 --> 0.769166).  Saving model ...\n",
      "[265/500] train_loss: 0.79143 valid_loss: 0.76966\n",
      "[266/500] train_loss: 0.79127 valid_loss: 0.76972\n",
      "[267/500] train_loss: 0.79120 valid_loss: 0.76957\n",
      "[268/500] train_loss: 0.79285 valid_loss: 0.76954\n",
      "[269/500] train_loss: 0.79262 valid_loss: 0.76896\n",
      "Validation loss decreased (0.769166 --> 0.768962).  Saving model ...\n",
      "[270/500] train_loss: 0.79174 valid_loss: 0.76909\n",
      "[271/500] train_loss: 0.79058 valid_loss: 0.76924\n",
      "[272/500] train_loss: 0.79223 valid_loss: 0.76910\n",
      "[273/500] train_loss: 0.79039 valid_loss: 0.76900\n",
      "[274/500] train_loss: 0.79508 valid_loss: 0.76884\n",
      "Validation loss decreased (0.768962 --> 0.768835).  Saving model ...\n",
      "[275/500] train_loss: 0.79175 valid_loss: 0.76933\n",
      "[276/500] train_loss: 0.79198 valid_loss: 0.76951\n",
      "[277/500] train_loss: 0.79550 valid_loss: 0.76966\n",
      "[278/500] train_loss: 0.79080 valid_loss: 0.76964\n",
      "[279/500] train_loss: 0.79055 valid_loss: 0.76858\n",
      "Validation loss decreased (0.768835 --> 0.768581).  Saving model ...\n",
      "[280/500] train_loss: 0.79092 valid_loss: 0.76893\n",
      "[281/500] train_loss: 0.79169 valid_loss: 0.76863\n",
      "[282/500] train_loss: 0.79633 valid_loss: 0.76810\n",
      "Validation loss decreased (0.768581 --> 0.768102).  Saving model ...\n",
      "[283/500] train_loss: 0.79440 valid_loss: 0.76859\n",
      "[284/500] train_loss: 0.79461 valid_loss: 0.76906\n",
      "[285/500] train_loss: 0.79525 valid_loss: 0.76919\n",
      "[286/500] train_loss: 0.79311 valid_loss: 0.76881\n",
      "[287/500] train_loss: 0.78669 valid_loss: 0.76873\n",
      "[288/500] train_loss: 0.79347 valid_loss: 0.76798\n",
      "Validation loss decreased (0.768102 --> 0.767978).  Saving model ...\n",
      "[289/500] train_loss: 0.79033 valid_loss: 0.76863\n",
      "[290/500] train_loss: 0.79334 valid_loss: 0.76855\n",
      "[291/500] train_loss: 0.79028 valid_loss: 0.76890\n",
      "[292/500] train_loss: 0.79129 valid_loss: 0.76925\n",
      "[293/500] train_loss: 0.78949 valid_loss: 0.76923\n",
      "[294/500] train_loss: 0.79202 valid_loss: 0.76945\n",
      "[295/500] train_loss: 0.78612 valid_loss: 0.76919\n",
      "[296/500] train_loss: 0.78806 valid_loss: 0.76931\n",
      "[297/500] train_loss: 0.79196 valid_loss: 0.76872\n",
      "[298/500] train_loss: 0.78789 valid_loss: 0.76901\n",
      "[299/500] train_loss: 0.78995 valid_loss: 0.76825\n",
      "Epoch 00299: reducing learning rate of group 0 to 6.8600e-03.\n",
      "[300/500] train_loss: 0.78115 valid_loss: 0.79002\n",
      "[301/500] train_loss: 0.78572 valid_loss: 0.76433\n",
      "Validation loss decreased (0.767978 --> 0.764331).  Saving model ...\n",
      "[302/500] train_loss: 0.78271 valid_loss: 0.76480\n",
      "[303/500] train_loss: 0.78489 valid_loss: 0.76538\n",
      "[304/500] train_loss: 0.78522 valid_loss: 0.76520\n",
      "[305/500] train_loss: 0.78730 valid_loss: 0.76588\n",
      "[306/500] train_loss: 0.78894 valid_loss: 0.76550\n",
      "[307/500] train_loss: 0.78385 valid_loss: 0.76574\n",
      "[308/500] train_loss: 0.77893 valid_loss: 0.76517\n",
      "[309/500] train_loss: 0.78508 valid_loss: 0.76534\n",
      "[310/500] train_loss: 0.78717 valid_loss: 0.76457\n",
      "[311/500] train_loss: 0.78282 valid_loss: 0.76479\n",
      "[312/500] train_loss: 0.78792 valid_loss: 0.76526\n",
      "Epoch 00312: reducing learning rate of group 0 to 4.8020e-03.\n",
      "[313/500] train_loss: 0.78235 valid_loss: 0.76372\n",
      "Validation loss decreased (0.764331 --> 0.763724).  Saving model ...\n",
      "[314/500] train_loss: 0.78257 valid_loss: 0.76379\n",
      "[315/500] train_loss: 0.78214 valid_loss: 0.76352\n",
      "Validation loss decreased (0.763724 --> 0.763522).  Saving model ...\n",
      "[316/500] train_loss: 0.78038 valid_loss: 0.76367\n",
      "[317/500] train_loss: 0.78381 valid_loss: 0.76362\n",
      "[318/500] train_loss: 0.78113 valid_loss: 0.76301\n",
      "Validation loss decreased (0.763522 --> 0.763006).  Saving model ...\n",
      "[319/500] train_loss: 0.78131 valid_loss: 0.76385\n",
      "[320/500] train_loss: 0.78150 valid_loss: 0.76353\n",
      "[321/500] train_loss: 0.78209 valid_loss: 0.76338\n",
      "[322/500] train_loss: 0.78235 valid_loss: 0.76362\n",
      "[323/500] train_loss: 0.78271 valid_loss: 0.76373\n",
      "[324/500] train_loss: 0.77942 valid_loss: 0.76381\n",
      "[325/500] train_loss: 0.77949 valid_loss: 0.76281\n",
      "Validation loss decreased (0.763006 --> 0.762813).  Saving model ...\n",
      "[326/500] train_loss: 0.78320 valid_loss: 0.76381\n",
      "[327/500] train_loss: 0.78322 valid_loss: 0.76356\n",
      "[328/500] train_loss: 0.77924 valid_loss: 0.76258\n",
      "Validation loss decreased (0.762813 --> 0.762578).  Saving model ...\n",
      "[329/500] train_loss: 0.78285 valid_loss: 0.76358\n",
      "[330/500] train_loss: 0.78280 valid_loss: 0.76355\n",
      "[331/500] train_loss: 0.78221 valid_loss: 0.76380\n",
      "[332/500] train_loss: 0.77969 valid_loss: 0.76347\n",
      "[333/500] train_loss: 0.77991 valid_loss: 0.76341\n",
      "[334/500] train_loss: 0.78172 valid_loss: 0.76364\n",
      "[335/500] train_loss: 0.78077 valid_loss: 0.76411\n",
      "[336/500] train_loss: 0.78120 valid_loss: 0.76309\n",
      "[337/500] train_loss: 0.78088 valid_loss: 0.76390\n",
      "[338/500] train_loss: 0.78058 valid_loss: 0.76419\n",
      "[339/500] train_loss: 0.78261 valid_loss: 0.76386\n",
      "Epoch 00339: reducing learning rate of group 0 to 3.3614e-03.\n",
      "[340/500] train_loss: 0.78125 valid_loss: 0.76290\n",
      "[341/500] train_loss: 0.77636 valid_loss: 0.76280\n",
      "[342/500] train_loss: 0.78139 valid_loss: 0.76276\n",
      "[343/500] train_loss: 0.77736 valid_loss: 0.76315\n",
      "[344/500] train_loss: 0.77879 valid_loss: 0.76303\n",
      "[345/500] train_loss: 0.77950 valid_loss: 0.76293\n",
      "[346/500] train_loss: 0.77679 valid_loss: 0.76294\n",
      "[347/500] train_loss: 0.77901 valid_loss: 0.76285\n",
      "[348/500] train_loss: 0.77814 valid_loss: 0.76287\n",
      "[349/500] train_loss: 0.78031 valid_loss: 0.76298\n",
      "[350/500] train_loss: 0.77643 valid_loss: 0.76180\n",
      "Validation loss decreased (0.762578 --> 0.761804).  Saving model ...\n",
      "[351/500] train_loss: 0.77745 valid_loss: 0.76250\n",
      "[352/500] train_loss: 0.77722 valid_loss: 0.76312\n",
      "[353/500] train_loss: 0.77883 valid_loss: 0.76212\n",
      "[354/500] train_loss: 0.78177 valid_loss: 0.76200\n",
      "[355/500] train_loss: 0.78151 valid_loss: 0.76168\n",
      "Validation loss decreased (0.761804 --> 0.761677).  Saving model ...\n",
      "[356/500] train_loss: 0.77912 valid_loss: 0.76237\n",
      "[357/500] train_loss: 0.77851 valid_loss: 0.76206\n",
      "[358/500] train_loss: 0.78002 valid_loss: 0.76213\n",
      "[359/500] train_loss: 0.77738 valid_loss: 0.76191\n",
      "[360/500] train_loss: 0.77652 valid_loss: 0.76186\n",
      "[361/500] train_loss: 0.77808 valid_loss: 0.76301\n",
      "[362/500] train_loss: 0.77730 valid_loss: 0.76215\n",
      "[363/500] train_loss: 0.77930 valid_loss: 0.76171\n",
      "[364/500] train_loss: 0.78245 valid_loss: 0.76228\n",
      "[365/500] train_loss: 0.77699 valid_loss: 0.76230\n",
      "[366/500] train_loss: 0.77963 valid_loss: 0.76216\n",
      "Epoch 00366: reducing learning rate of group 0 to 2.3530e-03.\n",
      "[367/500] train_loss: 0.77721 valid_loss: 0.76163\n",
      "Validation loss decreased (0.761677 --> 0.761628).  Saving model ...\n",
      "[368/500] train_loss: 0.77800 valid_loss: 0.76173\n",
      "[369/500] train_loss: 0.77661 valid_loss: 0.76157\n",
      "Validation loss decreased (0.761628 --> 0.761568).  Saving model ...\n",
      "[370/500] train_loss: 0.77801 valid_loss: 0.76215\n",
      "[371/500] train_loss: 0.78062 valid_loss: 0.76179\n",
      "[372/500] train_loss: 0.77601 valid_loss: 0.76144\n",
      "Validation loss decreased (0.761568 --> 0.761444).  Saving model ...\n",
      "[373/500] train_loss: 0.77886 valid_loss: 0.76174\n",
      "[374/500] train_loss: 0.77871 valid_loss: 0.76169\n",
      "[375/500] train_loss: 0.77375 valid_loss: 0.76192\n",
      "[376/500] train_loss: 0.77350 valid_loss: 0.76182\n",
      "[377/500] train_loss: 0.77547 valid_loss: 0.76136\n",
      "Validation loss decreased (0.761444 --> 0.761364).  Saving model ...\n",
      "[378/500] train_loss: 0.77718 valid_loss: 0.76137\n",
      "[379/500] train_loss: 0.77807 valid_loss: 0.76171\n",
      "[380/500] train_loss: 0.77101 valid_loss: 0.76138\n",
      "[381/500] train_loss: 0.78093 valid_loss: 0.76183\n",
      "[382/500] train_loss: 0.77587 valid_loss: 0.76153\n",
      "[383/500] train_loss: 0.77790 valid_loss: 0.76224\n",
      "[384/500] train_loss: 0.77751 valid_loss: 0.76180\n",
      "[385/500] train_loss: 0.77587 valid_loss: 0.76151\n",
      "[386/500] train_loss: 0.77643 valid_loss: 0.76182\n",
      "[387/500] train_loss: 0.77816 valid_loss: 0.76152\n",
      "[388/500] train_loss: 0.77705 valid_loss: 0.76181\n",
      "Epoch 00388: reducing learning rate of group 0 to 1.6471e-03.\n",
      "[389/500] train_loss: 0.77846 valid_loss: 0.76140\n",
      "[390/500] train_loss: 0.77693 valid_loss: 0.76128\n",
      "Validation loss decreased (0.761364 --> 0.761279).  Saving model ...\n",
      "[391/500] train_loss: 0.77615 valid_loss: 0.76150\n",
      "[392/500] train_loss: 0.77663 valid_loss: 0.76124\n",
      "Validation loss decreased (0.761279 --> 0.761235).  Saving model ...\n",
      "[393/500] train_loss: 0.77700 valid_loss: 0.76146\n",
      "[394/500] train_loss: 0.77386 valid_loss: 0.76110\n",
      "Validation loss decreased (0.761235 --> 0.761101).  Saving model ...\n",
      "[395/500] train_loss: 0.77349 valid_loss: 0.76104\n",
      "Validation loss decreased (0.761101 --> 0.761044).  Saving model ...\n",
      "[396/500] train_loss: 0.77323 valid_loss: 0.76111\n",
      "[397/500] train_loss: 0.77573 valid_loss: 0.76121\n",
      "[398/500] train_loss: 0.77478 valid_loss: 0.76112\n",
      "[399/500] train_loss: 0.77657 valid_loss: 0.76108\n",
      "[400/500] train_loss: 0.77848 valid_loss: 0.78580\n",
      "[401/500] train_loss: 0.77635 valid_loss: 0.76090\n",
      "Validation loss decreased (0.761044 --> 0.760904).  Saving model ...\n",
      "[402/500] train_loss: 0.77576 valid_loss: 0.76096\n",
      "[403/500] train_loss: 0.77449 valid_loss: 0.76097\n",
      "[404/500] train_loss: 0.77903 valid_loss: 0.76080\n",
      "Validation loss decreased (0.760904 --> 0.760801).  Saving model ...\n",
      "[405/500] train_loss: 0.77473 valid_loss: 0.76101\n",
      "[406/500] train_loss: 0.77612 valid_loss: 0.76082\n",
      "[407/500] train_loss: 0.77542 valid_loss: 0.76049\n",
      "Validation loss decreased (0.760801 --> 0.760488).  Saving model ...\n",
      "[408/500] train_loss: 0.77614 valid_loss: 0.76065\n",
      "[409/500] train_loss: 0.77892 valid_loss: 0.76058\n",
      "[410/500] train_loss: 0.77828 valid_loss: 0.76053\n",
      "[411/500] train_loss: 0.77807 valid_loss: 0.76051\n",
      "[412/500] train_loss: 0.77777 valid_loss: 0.76047\n",
      "Validation loss decreased (0.760488 --> 0.760474).  Saving model ...\n",
      "[413/500] train_loss: 0.77654 valid_loss: 0.76049\n",
      "[414/500] train_loss: 0.77876 valid_loss: 0.76060\n",
      "[415/500] train_loss: 0.77712 valid_loss: 0.76097\n",
      "[416/500] train_loss: 0.77743 valid_loss: 0.76075\n",
      "[417/500] train_loss: 0.77903 valid_loss: 0.76066\n",
      "[418/500] train_loss: 0.77220 valid_loss: 0.76084\n",
      "Epoch 00418: reducing learning rate of group 0 to 1.1530e-03.\n",
      "[419/500] train_loss: 0.78027 valid_loss: 0.76051\n",
      "[420/500] train_loss: 0.77639 valid_loss: 0.76071\n",
      "[421/500] train_loss: 0.77292 valid_loss: 0.76070\n",
      "[422/500] train_loss: 0.77655 valid_loss: 0.76061\n",
      "[423/500] train_loss: 0.77464 valid_loss: 0.76055\n",
      "[424/500] train_loss: 0.77609 valid_loss: 0.76052\n",
      "[425/500] train_loss: 0.77449 valid_loss: 0.76045\n",
      "Validation loss decreased (0.760474 --> 0.760450).  Saving model ...\n",
      "[426/500] train_loss: 0.77737 valid_loss: 0.76060\n",
      "[427/500] train_loss: 0.77484 valid_loss: 0.76035\n",
      "Validation loss decreased (0.760450 --> 0.760351).  Saving model ...\n",
      "[428/500] train_loss: 0.77094 valid_loss: 0.76042\n",
      "[429/500] train_loss: 0.77288 valid_loss: 0.76063\n",
      "[430/500] train_loss: 0.77393 valid_loss: 0.76046\n",
      "[431/500] train_loss: 0.77454 valid_loss: 0.76057\n",
      "[432/500] train_loss: 0.77565 valid_loss: 0.76046\n",
      "[433/500] train_loss: 0.77579 valid_loss: 0.76035\n",
      "[434/500] train_loss: 0.77529 valid_loss: 0.76032\n",
      "Validation loss decreased (0.760351 --> 0.760323).  Saving model ...\n",
      "[435/500] train_loss: 0.77651 valid_loss: 0.76049\n",
      "[436/500] train_loss: 0.77538 valid_loss: 0.76029\n",
      "Validation loss decreased (0.760323 --> 0.760293).  Saving model ...\n",
      "[437/500] train_loss: 0.77262 valid_loss: 0.76046\n",
      "[438/500] train_loss: 0.77747 valid_loss: 0.76036\n",
      "Epoch 00438: reducing learning rate of group 0 to 8.0707e-04.\n",
      "[439/500] train_loss: 0.77559 valid_loss: 0.76025\n",
      "Validation loss decreased (0.760293 --> 0.760247).  Saving model ...\n",
      "[440/500] train_loss: 0.77411 valid_loss: 0.76026\n",
      "[441/500] train_loss: 0.77713 valid_loss: 0.76030\n",
      "[442/500] train_loss: 0.77791 valid_loss: 0.76033\n",
      "[443/500] train_loss: 0.77590 valid_loss: 0.76033\n",
      "[444/500] train_loss: 0.77422 valid_loss: 0.76035\n",
      "[445/500] train_loss: 0.77468 valid_loss: 0.76040\n",
      "[446/500] train_loss: 0.77998 valid_loss: 0.76031\n",
      "[447/500] train_loss: 0.77457 valid_loss: 0.76026\n",
      "[448/500] train_loss: 0.77358 valid_loss: 0.76037\n",
      "[449/500] train_loss: 0.77688 valid_loss: 0.76030\n",
      "[450/500] train_loss: 0.77590 valid_loss: 0.76022\n",
      "Epoch 00450: reducing learning rate of group 0 to 5.6495e-04.\n",
      "Validation loss decreased (0.760247 --> 0.760218).  Saving model ...\n",
      "[451/500] train_loss: 0.77317 valid_loss: 0.76018\n",
      "Validation loss decreased (0.760218 --> 0.760175).  Saving model ...\n",
      "[452/500] train_loss: 0.77065 valid_loss: 0.76019\n",
      "[453/500] train_loss: 0.77693 valid_loss: 0.76018\n",
      "[454/500] train_loss: 0.77648 valid_loss: 0.76015\n",
      "Validation loss decreased (0.760175 --> 0.760146).  Saving model ...\n",
      "[455/500] train_loss: 0.77686 valid_loss: 0.76021\n",
      "[456/500] train_loss: 0.77676 valid_loss: 0.76018\n",
      "[457/500] train_loss: 0.77478 valid_loss: 0.76018\n",
      "[458/500] train_loss: 0.77510 valid_loss: 0.76021\n",
      "[459/500] train_loss: 0.77447 valid_loss: 0.76015\n",
      "[460/500] train_loss: 0.76973 valid_loss: 0.76010\n",
      "Validation loss decreased (0.760146 --> 0.760104).  Saving model ...\n",
      "[461/500] train_loss: 0.77435 valid_loss: 0.76010\n",
      "Validation loss decreased (0.760104 --> 0.760096).  Saving model ...\n",
      "[462/500] train_loss: 0.77535 valid_loss: 0.76008\n",
      "Validation loss decreased (0.760096 --> 0.760078).  Saving model ...\n",
      "[463/500] train_loss: 0.77561 valid_loss: 0.76009\n",
      "[464/500] train_loss: 0.77586 valid_loss: 0.76014\n",
      "[465/500] train_loss: 0.77253 valid_loss: 0.76010\n",
      "Epoch 00465: reducing learning rate of group 0 to 3.9547e-04.\n",
      "[466/500] train_loss: 0.77420 valid_loss: 0.76011\n",
      "[467/500] train_loss: 0.77461 valid_loss: 0.76012\n",
      "[468/500] train_loss: 0.77669 valid_loss: 0.76013\n",
      "[469/500] train_loss: 0.77173 valid_loss: 0.76011\n",
      "[470/500] train_loss: 0.77615 valid_loss: 0.76011\n",
      "[471/500] train_loss: 0.77586 valid_loss: 0.76008\n",
      "Validation loss decreased (0.760078 --> 0.760077).  Saving model ...\n",
      "[472/500] train_loss: 0.77298 valid_loss: 0.76004\n",
      "Validation loss decreased (0.760077 --> 0.760038).  Saving model ...\n",
      "[473/500] train_loss: 0.77248 valid_loss: 0.76002\n",
      "Validation loss decreased (0.760038 --> 0.760020).  Saving model ...\n",
      "[474/500] train_loss: 0.77389 valid_loss: 0.76002\n",
      "Validation loss decreased (0.760020 --> 0.760017).  Saving model ...\n",
      "[475/500] train_loss: 0.77312 valid_loss: 0.76003\n",
      "[476/500] train_loss: 0.77262 valid_loss: 0.76004\n",
      "[477/500] train_loss: 0.77641 valid_loss: 0.76003\n",
      "[478/500] train_loss: 0.77526 valid_loss: 0.76003\n",
      "[479/500] train_loss: 0.77336 valid_loss: 0.76000\n",
      "Validation loss decreased (0.760017 --> 0.760001).  Saving model ...\n",
      "[480/500] train_loss: 0.77166 valid_loss: 0.75999\n",
      "Validation loss decreased (0.760001 --> 0.759989).  Saving model ...\n",
      "[481/500] train_loss: 0.77527 valid_loss: 0.76003\n",
      "[482/500] train_loss: 0.77707 valid_loss: 0.76004\n",
      "[483/500] train_loss: 0.77366 valid_loss: 0.76003\n",
      "Epoch 00483: reducing learning rate of group 0 to 2.7683e-04.\n",
      "[484/500] train_loss: 0.77342 valid_loss: 0.76000\n",
      "[485/500] train_loss: 0.77391 valid_loss: 0.75999\n",
      "[486/500] train_loss: 0.77793 valid_loss: 0.75998\n",
      "Validation loss decreased (0.759989 --> 0.759975).  Saving model ...\n",
      "[487/500] train_loss: 0.77177 valid_loss: 0.75996\n",
      "Validation loss decreased (0.759975 --> 0.759957).  Saving model ...\n",
      "[488/500] train_loss: 0.77626 valid_loss: 0.75993\n",
      "Validation loss decreased (0.759957 --> 0.759930).  Saving model ...\n",
      "[489/500] train_loss: 0.77360 valid_loss: 0.75992\n",
      "Validation loss decreased (0.759930 --> 0.759919).  Saving model ...\n",
      "[490/500] train_loss: 0.77243 valid_loss: 0.75994\n",
      "[491/500] train_loss: 0.77571 valid_loss: 0.75993\n",
      "[492/500] train_loss: 0.77404 valid_loss: 0.75993\n",
      "[493/500] train_loss: 0.77361 valid_loss: 0.75990\n",
      "Validation loss decreased (0.759919 --> 0.759900).  Saving model ...\n",
      "[494/500] train_loss: 0.77707 valid_loss: 0.75989\n",
      "Validation loss decreased (0.759900 --> 0.759888).  Saving model ...\n",
      "[495/500] train_loss: 0.77538 valid_loss: 0.75988\n",
      "Validation loss decreased (0.759888 --> 0.759876).  Saving model ...\n",
      "[496/500] train_loss: 0.77083 valid_loss: 0.75989\n",
      "[497/500] train_loss: 0.76979 valid_loss: 0.75989\n",
      "[498/500] train_loss: 0.77307 valid_loss: 0.75988\n",
      "[499/500] train_loss: 0.77424 valid_loss: 0.75985\n",
      "Validation loss decreased (0.759876 --> 0.759850).  Saving model ...\n",
      "[500/500] train_loss: 0.77030 valid_loss: 0.78435\n",
      "[  1/500] train_loss: 1.90408 valid_loss: 0.63728\n",
      "Validation loss decreased (inf --> 0.637276).  Saving model ...\n",
      "[  2/500] train_loss: 0.79921 valid_loss: 0.57105\n",
      "Validation loss decreased (0.637276 --> 0.571051).  Saving model ...\n",
      "[  3/500] train_loss: 0.74146 valid_loss: 0.57106\n",
      "[  4/500] train_loss: 0.72152 valid_loss: 0.55825\n",
      "Validation loss decreased (0.571051 --> 0.558253).  Saving model ...\n",
      "[  5/500] train_loss: 0.71198 valid_loss: 0.55030\n",
      "Validation loss decreased (0.558253 --> 0.550299).  Saving model ...\n",
      "[  6/500] train_loss: 0.70482 valid_loss: 0.54429\n",
      "Validation loss decreased (0.550299 --> 0.544292).  Saving model ...\n",
      "[  7/500] train_loss: 0.69637 valid_loss: 0.53989\n",
      "Validation loss decreased (0.544292 --> 0.539893).  Saving model ...\n",
      "[  8/500] train_loss: 0.69120 valid_loss: 0.53598\n",
      "Validation loss decreased (0.539893 --> 0.535982).  Saving model ...\n",
      "[  9/500] train_loss: 0.68304 valid_loss: 0.53283\n",
      "Validation loss decreased (0.535982 --> 0.532826).  Saving model ...\n",
      "[ 10/500] train_loss: 0.68284 valid_loss: 0.53042\n",
      "Validation loss decreased (0.532826 --> 0.530422).  Saving model ...\n",
      "[ 11/500] train_loss: 0.67775 valid_loss: 0.52719\n",
      "Validation loss decreased (0.530422 --> 0.527189).  Saving model ...\n",
      "[ 12/500] train_loss: 0.67391 valid_loss: 0.52505\n",
      "Validation loss decreased (0.527189 --> 0.525047).  Saving model ...\n",
      "[ 13/500] train_loss: 0.67139 valid_loss: 0.52265\n",
      "Validation loss decreased (0.525047 --> 0.522655).  Saving model ...\n",
      "[ 14/500] train_loss: 0.67318 valid_loss: 0.52072\n",
      "Validation loss decreased (0.522655 --> 0.520724).  Saving model ...\n",
      "[ 15/500] train_loss: 0.66045 valid_loss: 0.51905\n",
      "Validation loss decreased (0.520724 --> 0.519054).  Saving model ...\n",
      "[ 16/500] train_loss: 0.65656 valid_loss: 0.51705\n",
      "Validation loss decreased (0.519054 --> 0.517051).  Saving model ...\n",
      "[ 17/500] train_loss: 0.65421 valid_loss: 0.51510\n",
      "Validation loss decreased (0.517051 --> 0.515101).  Saving model ...\n",
      "[ 18/500] train_loss: 0.65383 valid_loss: 0.51335\n",
      "Validation loss decreased (0.515101 --> 0.513353).  Saving model ...\n",
      "[ 19/500] train_loss: 0.64196 valid_loss: 0.51176\n",
      "Validation loss decreased (0.513353 --> 0.511763).  Saving model ...\n",
      "[ 20/500] train_loss: 0.64505 valid_loss: 0.51067\n",
      "Validation loss decreased (0.511763 --> 0.510667).  Saving model ...\n",
      "[ 21/500] train_loss: 0.64915 valid_loss: 0.50890\n",
      "Validation loss decreased (0.510667 --> 0.508896).  Saving model ...\n",
      "[ 22/500] train_loss: 0.64123 valid_loss: 0.50716\n",
      "Validation loss decreased (0.508896 --> 0.507164).  Saving model ...\n",
      "[ 23/500] train_loss: 0.63928 valid_loss: 0.50606\n",
      "Validation loss decreased (0.507164 --> 0.506059).  Saving model ...\n",
      "[ 24/500] train_loss: 0.63361 valid_loss: 0.50444\n",
      "Validation loss decreased (0.506059 --> 0.504445).  Saving model ...\n",
      "[ 25/500] train_loss: 0.63489 valid_loss: 0.50293\n",
      "Validation loss decreased (0.504445 --> 0.502932).  Saving model ...\n",
      "[ 26/500] train_loss: 0.63950 valid_loss: 0.50166\n",
      "Validation loss decreased (0.502932 --> 0.501656).  Saving model ...\n",
      "[ 27/500] train_loss: 0.63671 valid_loss: 0.50065\n",
      "Validation loss decreased (0.501656 --> 0.500651).  Saving model ...\n",
      "[ 28/500] train_loss: 0.63785 valid_loss: 0.49901\n",
      "Validation loss decreased (0.500651 --> 0.499008).  Saving model ...\n",
      "[ 29/500] train_loss: 0.62924 valid_loss: 0.49853\n",
      "Validation loss decreased (0.499008 --> 0.498528).  Saving model ...\n",
      "[ 30/500] train_loss: 0.62761 valid_loss: 0.49745\n",
      "Validation loss decreased (0.498528 --> 0.497451).  Saving model ...\n",
      "[ 31/500] train_loss: 0.62655 valid_loss: 0.49631\n",
      "Validation loss decreased (0.497451 --> 0.496314).  Saving model ...\n",
      "[ 32/500] train_loss: 0.62793 valid_loss: 0.49533\n",
      "Validation loss decreased (0.496314 --> 0.495334).  Saving model ...\n",
      "[ 33/500] train_loss: 0.62621 valid_loss: 0.49496\n",
      "Validation loss decreased (0.495334 --> 0.494958).  Saving model ...\n",
      "[ 34/500] train_loss: 0.62527 valid_loss: 0.49423\n",
      "Validation loss decreased (0.494958 --> 0.494227).  Saving model ...\n",
      "[ 35/500] train_loss: 0.62169 valid_loss: 0.49302\n",
      "Validation loss decreased (0.494227 --> 0.493023).  Saving model ...\n",
      "[ 36/500] train_loss: 0.62201 valid_loss: 0.49194\n",
      "Validation loss decreased (0.493023 --> 0.491942).  Saving model ...\n",
      "[ 37/500] train_loss: 0.62292 valid_loss: 0.49082\n",
      "Validation loss decreased (0.491942 --> 0.490822).  Saving model ...\n",
      "[ 38/500] train_loss: 0.61952 valid_loss: 0.48980\n",
      "Validation loss decreased (0.490822 --> 0.489801).  Saving model ...\n",
      "[ 39/500] train_loss: 0.62292 valid_loss: 0.48854\n",
      "Validation loss decreased (0.489801 --> 0.488536).  Saving model ...\n",
      "[ 40/500] train_loss: 0.62208 valid_loss: 0.48741\n",
      "Validation loss decreased (0.488536 --> 0.487411).  Saving model ...\n",
      "[ 41/500] train_loss: 0.61805 valid_loss: 0.48666\n",
      "Validation loss decreased (0.487411 --> 0.486664).  Saving model ...\n",
      "[ 42/500] train_loss: 0.62045 valid_loss: 0.48573\n",
      "Validation loss decreased (0.486664 --> 0.485729).  Saving model ...\n",
      "[ 43/500] train_loss: 0.61815 valid_loss: 0.48461\n",
      "Validation loss decreased (0.485729 --> 0.484614).  Saving model ...\n",
      "[ 44/500] train_loss: 0.60732 valid_loss: 0.48347\n",
      "Validation loss decreased (0.484614 --> 0.483468).  Saving model ...\n",
      "[ 45/500] train_loss: 0.61277 valid_loss: 0.48292\n",
      "Validation loss decreased (0.483468 --> 0.482917).  Saving model ...\n",
      "[ 46/500] train_loss: 0.61117 valid_loss: 0.48223\n",
      "Validation loss decreased (0.482917 --> 0.482235).  Saving model ...\n",
      "[ 47/500] train_loss: 0.61689 valid_loss: 0.48163\n",
      "Validation loss decreased (0.482235 --> 0.481629).  Saving model ...\n",
      "[ 48/500] train_loss: 0.61135 valid_loss: 0.48047\n",
      "Validation loss decreased (0.481629 --> 0.480468).  Saving model ...\n",
      "[ 49/500] train_loss: 0.60894 valid_loss: 0.47969\n",
      "Validation loss decreased (0.480468 --> 0.479686).  Saving model ...\n",
      "[ 50/500] train_loss: 0.60372 valid_loss: 0.47897\n",
      "Validation loss decreased (0.479686 --> 0.478974).  Saving model ...\n",
      "[ 51/500] train_loss: 0.60912 valid_loss: 0.47796\n",
      "Validation loss decreased (0.478974 --> 0.477958).  Saving model ...\n",
      "[ 52/500] train_loss: 0.61276 valid_loss: 0.47746\n",
      "Validation loss decreased (0.477958 --> 0.477462).  Saving model ...\n",
      "[ 53/500] train_loss: 0.60656 valid_loss: 0.47664\n",
      "Validation loss decreased (0.477462 --> 0.476642).  Saving model ...\n",
      "[ 54/500] train_loss: 0.60720 valid_loss: 0.47592\n",
      "Validation loss decreased (0.476642 --> 0.475918).  Saving model ...\n",
      "[ 55/500] train_loss: 0.60856 valid_loss: 0.47567\n",
      "Validation loss decreased (0.475918 --> 0.475672).  Saving model ...\n",
      "[ 56/500] train_loss: 0.60023 valid_loss: 0.47501\n",
      "Validation loss decreased (0.475672 --> 0.475013).  Saving model ...\n",
      "[ 57/500] train_loss: 0.60419 valid_loss: 0.47353\n",
      "Validation loss decreased (0.475013 --> 0.473526).  Saving model ...\n",
      "[ 58/500] train_loss: 0.60453 valid_loss: 0.47278\n",
      "Validation loss decreased (0.473526 --> 0.472784).  Saving model ...\n",
      "[ 59/500] train_loss: 0.59690 valid_loss: 0.47232\n",
      "Validation loss decreased (0.472784 --> 0.472325).  Saving model ...\n",
      "[ 60/500] train_loss: 0.59774 valid_loss: 0.47150\n",
      "Validation loss decreased (0.472325 --> 0.471501).  Saving model ...\n",
      "[ 61/500] train_loss: 0.59960 valid_loss: 0.47128\n",
      "Validation loss decreased (0.471501 --> 0.471279).  Saving model ...\n",
      "[ 62/500] train_loss: 0.59983 valid_loss: 0.47053\n",
      "Validation loss decreased (0.471279 --> 0.470527).  Saving model ...\n",
      "[ 63/500] train_loss: 0.59666 valid_loss: 0.47067\n",
      "[ 64/500] train_loss: 0.59513 valid_loss: 0.47009\n",
      "Validation loss decreased (0.470527 --> 0.470094).  Saving model ...\n",
      "[ 65/500] train_loss: 0.59671 valid_loss: 0.46894\n",
      "Validation loss decreased (0.470094 --> 0.468938).  Saving model ...\n",
      "[ 66/500] train_loss: 0.59784 valid_loss: 0.46782\n",
      "Validation loss decreased (0.468938 --> 0.467819).  Saving model ...\n",
      "[ 67/500] train_loss: 0.59481 valid_loss: 0.46712\n",
      "Validation loss decreased (0.467819 --> 0.467119).  Saving model ...\n",
      "[ 68/500] train_loss: 0.59336 valid_loss: 0.46692\n",
      "Validation loss decreased (0.467119 --> 0.466916).  Saving model ...\n",
      "[ 69/500] train_loss: 0.59916 valid_loss: 0.46692\n",
      "[ 70/500] train_loss: 0.59457 valid_loss: 0.46528\n",
      "Validation loss decreased (0.466916 --> 0.465284).  Saving model ...\n",
      "[ 71/500] train_loss: 0.59594 valid_loss: 0.46529\n",
      "[ 72/500] train_loss: 0.58928 valid_loss: 0.46472\n",
      "Validation loss decreased (0.465284 --> 0.464716).  Saving model ...\n",
      "[ 73/500] train_loss: 0.59540 valid_loss: 0.46402\n",
      "Validation loss decreased (0.464716 --> 0.464022).  Saving model ...\n",
      "[ 74/500] train_loss: 0.59255 valid_loss: 0.46358\n",
      "Validation loss decreased (0.464022 --> 0.463583).  Saving model ...\n",
      "[ 75/500] train_loss: 0.59041 valid_loss: 0.46306\n",
      "Validation loss decreased (0.463583 --> 0.463063).  Saving model ...\n",
      "[ 76/500] train_loss: 0.59154 valid_loss: 0.46221\n",
      "Validation loss decreased (0.463063 --> 0.462209).  Saving model ...\n",
      "[ 77/500] train_loss: 0.59213 valid_loss: 0.46192\n",
      "Validation loss decreased (0.462209 --> 0.461917).  Saving model ...\n",
      "[ 78/500] train_loss: 0.59127 valid_loss: 0.46187\n",
      "Validation loss decreased (0.461917 --> 0.461874).  Saving model ...\n",
      "[ 79/500] train_loss: 0.58908 valid_loss: 0.46119\n",
      "Validation loss decreased (0.461874 --> 0.461188).  Saving model ...\n",
      "[ 80/500] train_loss: 0.58962 valid_loss: 0.46145\n",
      "[ 81/500] train_loss: 0.59045 valid_loss: 0.46120\n",
      "[ 82/500] train_loss: 0.58555 valid_loss: 0.46095\n",
      "Validation loss decreased (0.461188 --> 0.460953).  Saving model ...\n",
      "[ 83/500] train_loss: 0.58800 valid_loss: 0.46048\n",
      "Validation loss decreased (0.460953 --> 0.460483).  Saving model ...\n",
      "[ 84/500] train_loss: 0.58282 valid_loss: 0.46001\n",
      "Validation loss decreased (0.460483 --> 0.460009).  Saving model ...\n",
      "[ 85/500] train_loss: 0.59041 valid_loss: 0.45977\n",
      "Validation loss decreased (0.460009 --> 0.459770).  Saving model ...\n",
      "[ 86/500] train_loss: 0.58684 valid_loss: 0.45975\n",
      "Validation loss decreased (0.459770 --> 0.459752).  Saving model ...\n",
      "[ 87/500] train_loss: 0.58728 valid_loss: 0.45934\n",
      "Validation loss decreased (0.459752 --> 0.459340).  Saving model ...\n",
      "[ 88/500] train_loss: 0.58272 valid_loss: 0.45959\n",
      "[ 89/500] train_loss: 0.58479 valid_loss: 0.45877\n",
      "Validation loss decreased (0.459340 --> 0.458768).  Saving model ...\n",
      "[ 90/500] train_loss: 0.58718 valid_loss: 0.45837\n",
      "Validation loss decreased (0.458768 --> 0.458372).  Saving model ...\n",
      "[ 91/500] train_loss: 0.58147 valid_loss: 0.45820\n",
      "Validation loss decreased (0.458372 --> 0.458196).  Saving model ...\n",
      "[ 92/500] train_loss: 0.58560 valid_loss: 0.45762\n",
      "Validation loss decreased (0.458196 --> 0.457618).  Saving model ...\n",
      "[ 93/500] train_loss: 0.58376 valid_loss: 0.45730\n",
      "Validation loss decreased (0.457618 --> 0.457295).  Saving model ...\n",
      "[ 94/500] train_loss: 0.58264 valid_loss: 0.45697\n",
      "Validation loss decreased (0.457295 --> 0.456973).  Saving model ...\n",
      "[ 95/500] train_loss: 0.57956 valid_loss: 0.45732\n",
      "[ 96/500] train_loss: 0.58146 valid_loss: 0.45656\n",
      "Validation loss decreased (0.456973 --> 0.456560).  Saving model ...\n",
      "[ 97/500] train_loss: 0.58017 valid_loss: 0.45600\n",
      "Validation loss decreased (0.456560 --> 0.455999).  Saving model ...\n",
      "[ 98/500] train_loss: 0.58599 valid_loss: 0.45553\n",
      "Validation loss decreased (0.455999 --> 0.455528).  Saving model ...\n",
      "[ 99/500] train_loss: 0.57927 valid_loss: 0.45562\n",
      "[100/500] train_loss: 0.57708 valid_loss: 0.46972\n",
      "[101/500] train_loss: 0.57940 valid_loss: 0.45492\n",
      "Validation loss decreased (0.455528 --> 0.454916).  Saving model ...\n",
      "[102/500] train_loss: 0.58124 valid_loss: 0.45422\n",
      "Validation loss decreased (0.454916 --> 0.454222).  Saving model ...\n",
      "[103/500] train_loss: 0.57472 valid_loss: 0.45459\n",
      "[104/500] train_loss: 0.58072 valid_loss: 0.45423\n",
      "[105/500] train_loss: 0.58217 valid_loss: 0.45460\n",
      "[106/500] train_loss: 0.57590 valid_loss: 0.45485\n",
      "[107/500] train_loss: 0.57881 valid_loss: 0.45450\n",
      "[108/500] train_loss: 0.57191 valid_loss: 0.45441\n",
      "[109/500] train_loss: 0.57375 valid_loss: 0.45417\n",
      "Validation loss decreased (0.454222 --> 0.454172).  Saving model ...\n",
      "[110/500] train_loss: 0.57505 valid_loss: 0.45397\n",
      "Validation loss decreased (0.454172 --> 0.453973).  Saving model ...\n",
      "[111/500] train_loss: 0.57341 valid_loss: 0.45331\n",
      "Validation loss decreased (0.453973 --> 0.453312).  Saving model ...\n",
      "[112/500] train_loss: 0.57532 valid_loss: 0.45309\n",
      "Validation loss decreased (0.453312 --> 0.453094).  Saving model ...\n",
      "[113/500] train_loss: 0.57512 valid_loss: 0.45333\n",
      "[114/500] train_loss: 0.57143 valid_loss: 0.45321\n",
      "[115/500] train_loss: 0.57311 valid_loss: 0.45358\n",
      "[116/500] train_loss: 0.57636 valid_loss: 0.45340\n",
      "[117/500] train_loss: 0.57338 valid_loss: 0.45299\n",
      "Validation loss decreased (0.453094 --> 0.452991).  Saving model ...\n",
      "[118/500] train_loss: 0.57514 valid_loss: 0.45269\n",
      "Validation loss decreased (0.452991 --> 0.452689).  Saving model ...\n",
      "[119/500] train_loss: 0.56883 valid_loss: 0.45221\n",
      "Validation loss decreased (0.452689 --> 0.452205).  Saving model ...\n",
      "[120/500] train_loss: 0.56895 valid_loss: 0.45221\n",
      "[121/500] train_loss: 0.57035 valid_loss: 0.45208\n",
      "Validation loss decreased (0.452205 --> 0.452081).  Saving model ...\n",
      "[122/500] train_loss: 0.56926 valid_loss: 0.45239\n",
      "[123/500] train_loss: 0.56977 valid_loss: 0.45218\n",
      "[124/500] train_loss: 0.57474 valid_loss: 0.45215\n",
      "[125/500] train_loss: 0.57100 valid_loss: 0.45208\n",
      "Validation loss decreased (0.452081 --> 0.452080).  Saving model ...\n",
      "[126/500] train_loss: 0.57056 valid_loss: 0.45088\n",
      "Validation loss decreased (0.452080 --> 0.450878).  Saving model ...\n",
      "[127/500] train_loss: 0.57008 valid_loss: 0.45111\n",
      "[128/500] train_loss: 0.56909 valid_loss: 0.45132\n",
      "[129/500] train_loss: 0.56841 valid_loss: 0.45093\n",
      "[130/500] train_loss: 0.56879 valid_loss: 0.45114\n",
      "[131/500] train_loss: 0.56563 valid_loss: 0.45087\n",
      "Validation loss decreased (0.450878 --> 0.450868).  Saving model ...\n",
      "[132/500] train_loss: 0.56661 valid_loss: 0.45037\n",
      "Validation loss decreased (0.450868 --> 0.450370).  Saving model ...\n",
      "[133/500] train_loss: 0.56837 valid_loss: 0.44929\n",
      "Validation loss decreased (0.450370 --> 0.449288).  Saving model ...\n",
      "[134/500] train_loss: 0.56690 valid_loss: 0.45000\n",
      "[135/500] train_loss: 0.56662 valid_loss: 0.44985\n",
      "[136/500] train_loss: 0.56601 valid_loss: 0.45039\n",
      "[137/500] train_loss: 0.56453 valid_loss: 0.45045\n",
      "[138/500] train_loss: 0.56989 valid_loss: 0.45042\n",
      "[139/500] train_loss: 0.56580 valid_loss: 0.45027\n",
      "[140/500] train_loss: 0.56547 valid_loss: 0.45050\n",
      "[141/500] train_loss: 0.56379 valid_loss: 0.45032\n",
      "[142/500] train_loss: 0.56749 valid_loss: 0.45072\n",
      "[143/500] train_loss: 0.56114 valid_loss: 0.45043\n",
      "[144/500] train_loss: 0.56640 valid_loss: 0.45061\n",
      "Epoch 00144: reducing learning rate of group 0 to 1.4000e-02.\n",
      "[145/500] train_loss: 0.54703 valid_loss: 0.42585\n",
      "Validation loss decreased (0.449288 --> 0.425855).  Saving model ...\n",
      "[146/500] train_loss: 0.54797 valid_loss: 0.42930\n",
      "[147/500] train_loss: 0.55096 valid_loss: 0.42477\n",
      "Validation loss decreased (0.425855 --> 0.424767).  Saving model ...\n",
      "[148/500] train_loss: 0.54642 valid_loss: 0.42673\n",
      "[149/500] train_loss: 0.54866 valid_loss: 0.42675\n",
      "[150/500] train_loss: 0.55019 valid_loss: 0.42612\n",
      "[151/500] train_loss: 0.55095 valid_loss: 0.42675\n",
      "[152/500] train_loss: 0.54717 valid_loss: 0.42569\n",
      "[153/500] train_loss: 0.54743 valid_loss: 0.42339\n",
      "Validation loss decreased (0.424767 --> 0.423394).  Saving model ...\n",
      "[154/500] train_loss: 0.54567 valid_loss: 0.42308\n",
      "Validation loss decreased (0.423394 --> 0.423082).  Saving model ...\n",
      "[155/500] train_loss: 0.54617 valid_loss: 0.42192\n",
      "Validation loss decreased (0.423082 --> 0.421925).  Saving model ...\n",
      "[156/500] train_loss: 0.54653 valid_loss: 0.42257\n",
      "[157/500] train_loss: 0.54293 valid_loss: 0.41974\n",
      "Validation loss decreased (0.421925 --> 0.419738).  Saving model ...\n",
      "[158/500] train_loss: 0.54465 valid_loss: 0.42216\n",
      "[159/500] train_loss: 0.54787 valid_loss: 0.42224\n",
      "[160/500] train_loss: 0.54374 valid_loss: 0.42164\n",
      "[161/500] train_loss: 0.54246 valid_loss: 0.42127\n",
      "[162/500] train_loss: 0.54408 valid_loss: 0.42258\n",
      "[163/500] train_loss: 0.54433 valid_loss: 0.42366\n",
      "[164/500] train_loss: 0.54555 valid_loss: 0.42247\n",
      "[165/500] train_loss: 0.54414 valid_loss: 0.42139\n",
      "[166/500] train_loss: 0.54570 valid_loss: 0.42127\n",
      "[167/500] train_loss: 0.54274 valid_loss: 0.42116\n",
      "[168/500] train_loss: 0.54186 valid_loss: 0.42047\n",
      "Epoch 00168: reducing learning rate of group 0 to 9.8000e-03.\n",
      "[169/500] train_loss: 0.53396 valid_loss: 0.38014\n",
      "Validation loss decreased (0.419738 --> 0.380138).  Saving model ...\n",
      "[170/500] train_loss: 0.54690 valid_loss: 0.37682\n",
      "Validation loss decreased (0.380138 --> 0.376815).  Saving model ...\n",
      "[171/500] train_loss: 0.54808 valid_loss: 0.37501\n",
      "Validation loss decreased (0.376815 --> 0.375008).  Saving model ...\n",
      "[172/500] train_loss: 0.54615 valid_loss: 0.37394\n",
      "Validation loss decreased (0.375008 --> 0.373939).  Saving model ...\n",
      "[173/500] train_loss: 0.54473 valid_loss: 0.37338\n",
      "Validation loss decreased (0.373939 --> 0.373381).  Saving model ...\n",
      "[174/500] train_loss: 0.54698 valid_loss: 0.37315\n",
      "Validation loss decreased (0.373381 --> 0.373153).  Saving model ...\n",
      "[175/500] train_loss: 0.54507 valid_loss: 0.37313\n",
      "Validation loss decreased (0.373153 --> 0.373129).  Saving model ...\n",
      "[176/500] train_loss: 0.54887 valid_loss: 0.37299\n",
      "Validation loss decreased (0.373129 --> 0.372989).  Saving model ...\n",
      "[177/500] train_loss: 0.54448 valid_loss: 0.37320\n",
      "[178/500] train_loss: 0.54749 valid_loss: 0.37326\n",
      "[179/500] train_loss: 0.54929 valid_loss: 0.37350\n",
      "[180/500] train_loss: 0.54730 valid_loss: 0.37355\n",
      "[181/500] train_loss: 0.54608 valid_loss: 0.37383\n",
      "[182/500] train_loss: 0.54504 valid_loss: 0.37394\n",
      "[183/500] train_loss: 0.54725 valid_loss: 0.37416\n",
      "[184/500] train_loss: 0.54397 valid_loss: 0.37419\n",
      "[185/500] train_loss: 0.54709 valid_loss: 0.37434\n",
      "[186/500] train_loss: 0.54488 valid_loss: 0.37437\n",
      "[187/500] train_loss: 0.54335 valid_loss: 0.37439\n",
      "Epoch 00187: reducing learning rate of group 0 to 6.8600e-03.\n",
      "[188/500] train_loss: 0.54120 valid_loss: 0.37367\n",
      "[189/500] train_loss: 0.53925 valid_loss: 0.37330\n",
      "[190/500] train_loss: 0.53775 valid_loss: 0.37321\n",
      "[191/500] train_loss: 0.53923 valid_loss: 0.37311\n",
      "[192/500] train_loss: 0.53662 valid_loss: 0.37302\n",
      "[193/500] train_loss: 0.54161 valid_loss: 0.37292\n",
      "Validation loss decreased (0.372989 --> 0.372923).  Saving model ...\n",
      "[194/500] train_loss: 0.53854 valid_loss: 0.37301\n",
      "[195/500] train_loss: 0.53808 valid_loss: 0.37295\n",
      "[196/500] train_loss: 0.53627 valid_loss: 0.37306\n",
      "[197/500] train_loss: 0.53888 valid_loss: 0.37297\n",
      "[198/500] train_loss: 0.53599 valid_loss: 0.37310\n",
      "[199/500] train_loss: 0.53369 valid_loss: 0.37318\n",
      "[200/500] train_loss: 0.53641 valid_loss: 0.38505\n",
      "[201/500] train_loss: 0.53779 valid_loss: 0.37304\n",
      "[202/500] train_loss: 0.53490 valid_loss: 0.37296\n",
      "[203/500] train_loss: 0.53637 valid_loss: 0.37302\n",
      "[204/500] train_loss: 0.53617 valid_loss: 0.37295\n",
      "Epoch 00204: reducing learning rate of group 0 to 4.8020e-03.\n",
      "[205/500] train_loss: 0.53178 valid_loss: 0.37293\n",
      "[206/500] train_loss: 0.53385 valid_loss: 0.37286\n",
      "Validation loss decreased (0.372923 --> 0.372861).  Saving model ...\n",
      "[207/500] train_loss: 0.53263 valid_loss: 0.37267\n",
      "Validation loss decreased (0.372861 --> 0.372672).  Saving model ...\n",
      "[208/500] train_loss: 0.53574 valid_loss: 0.37268\n",
      "[209/500] train_loss: 0.53461 valid_loss: 0.37261\n",
      "Validation loss decreased (0.372672 --> 0.372614).  Saving model ...\n",
      "[210/500] train_loss: 0.53396 valid_loss: 0.37257\n",
      "Validation loss decreased (0.372614 --> 0.372572).  Saving model ...\n",
      "[211/500] train_loss: 0.53315 valid_loss: 0.37278\n",
      "[212/500] train_loss: 0.53644 valid_loss: 0.37267\n",
      "[213/500] train_loss: 0.53543 valid_loss: 0.37262\n",
      "[214/500] train_loss: 0.53034 valid_loss: 0.37238\n",
      "Validation loss decreased (0.372572 --> 0.372377).  Saving model ...\n",
      "[215/500] train_loss: 0.53381 valid_loss: 0.37254\n",
      "[216/500] train_loss: 0.53258 valid_loss: 0.37250\n",
      "[217/500] train_loss: 0.53608 valid_loss: 0.37279\n",
      "[218/500] train_loss: 0.53393 valid_loss: 0.37249\n",
      "[219/500] train_loss: 0.53563 valid_loss: 0.37223\n",
      "Validation loss decreased (0.372377 --> 0.372229).  Saving model ...\n",
      "[220/500] train_loss: 0.53531 valid_loss: 0.37241\n",
      "[221/500] train_loss: 0.53433 valid_loss: 0.37244\n",
      "[222/500] train_loss: 0.53385 valid_loss: 0.37223\n",
      "[223/500] train_loss: 0.53507 valid_loss: 0.37268\n",
      "[224/500] train_loss: 0.53486 valid_loss: 0.37214\n",
      "Validation loss decreased (0.372229 --> 0.372145).  Saving model ...\n",
      "[225/500] train_loss: 0.52896 valid_loss: 0.37208\n",
      "Validation loss decreased (0.372145 --> 0.372078).  Saving model ...\n",
      "[226/500] train_loss: 0.53365 valid_loss: 0.37215\n",
      "[227/500] train_loss: 0.53396 valid_loss: 0.37224\n",
      "[228/500] train_loss: 0.53458 valid_loss: 0.37218\n",
      "[229/500] train_loss: 0.53200 valid_loss: 0.37252\n",
      "[230/500] train_loss: 0.53096 valid_loss: 0.37236\n",
      "[231/500] train_loss: 0.53413 valid_loss: 0.37222\n",
      "[232/500] train_loss: 0.53580 valid_loss: 0.37220\n",
      "[233/500] train_loss: 0.53677 valid_loss: 0.37240\n",
      "[234/500] train_loss: 0.52934 valid_loss: 0.37244\n",
      "[235/500] train_loss: 0.53363 valid_loss: 0.37212\n",
      "[236/500] train_loss: 0.53282 valid_loss: 0.37257\n",
      "Epoch 00236: reducing learning rate of group 0 to 3.3614e-03.\n",
      "[237/500] train_loss: 0.53140 valid_loss: 0.37354\n",
      "[238/500] train_loss: 0.53223 valid_loss: 0.37637\n",
      "[239/500] train_loss: 0.53119 valid_loss: 0.37614\n",
      "[240/500] train_loss: 0.53399 valid_loss: 0.37593\n",
      "[241/500] train_loss: 0.53300 valid_loss: 0.37598\n",
      "[242/500] train_loss: 0.53046 valid_loss: 0.37557\n",
      "[243/500] train_loss: 0.53290 valid_loss: 0.37561\n",
      "[244/500] train_loss: 0.53264 valid_loss: 0.37563\n",
      "[245/500] train_loss: 0.53127 valid_loss: 0.37525\n",
      "[246/500] train_loss: 0.53169 valid_loss: 0.37544\n",
      "[247/500] train_loss: 0.53130 valid_loss: 0.37531\n",
      "Epoch 00247: reducing learning rate of group 0 to 2.3530e-03.\n",
      "[248/500] train_loss: 0.52953 valid_loss: 0.37502\n",
      "[249/500] train_loss: 0.52727 valid_loss: 0.37466\n",
      "[250/500] train_loss: 0.52708 valid_loss: 0.37419\n",
      "[251/500] train_loss: 0.53255 valid_loss: 0.37541\n",
      "[252/500] train_loss: 0.52926 valid_loss: 0.37449\n",
      "[253/500] train_loss: 0.52938 valid_loss: 0.37510\n",
      "[254/500] train_loss: 0.52978 valid_loss: 0.37483\n",
      "[255/500] train_loss: 0.52766 valid_loss: 0.37486\n",
      "[256/500] train_loss: 0.53208 valid_loss: 0.37436\n",
      "[257/500] train_loss: 0.52979 valid_loss: 0.37431\n",
      "[258/500] train_loss: 0.53105 valid_loss: 0.37486\n",
      "Epoch 00258: reducing learning rate of group 0 to 1.6471e-03.\n",
      "[259/500] train_loss: 0.52810 valid_loss: 0.37412\n",
      "[260/500] train_loss: 0.53018 valid_loss: 0.37419\n",
      "[261/500] train_loss: 0.52906 valid_loss: 0.37447\n",
      "[262/500] train_loss: 0.53016 valid_loss: 0.37422\n",
      "[263/500] train_loss: 0.52754 valid_loss: 0.37283\n",
      "[264/500] train_loss: 0.52796 valid_loss: 0.37443\n",
      "[265/500] train_loss: 0.53105 valid_loss: 0.37430\n",
      "[266/500] train_loss: 0.52840 valid_loss: 0.37459\n",
      "[267/500] train_loss: 0.52843 valid_loss: 0.37299\n",
      "[268/500] train_loss: 0.52875 valid_loss: 0.37490\n",
      "[269/500] train_loss: 0.52999 valid_loss: 0.37337\n",
      "Epoch 00269: reducing learning rate of group 0 to 1.1530e-03.\n",
      "[270/500] train_loss: 0.53071 valid_loss: 0.37431\n",
      "[271/500] train_loss: 0.52870 valid_loss: 0.37387\n",
      "[272/500] train_loss: 0.52774 valid_loss: 0.37430\n",
      "[273/500] train_loss: 0.52941 valid_loss: 0.37443\n",
      "[274/500] train_loss: 0.52753 valid_loss: 0.37394\n",
      "[275/500] train_loss: 0.53218 valid_loss: 0.37409\n",
      "Early stopping\n",
      "[  1/500] train_loss: 0.97324 valid_loss: 0.88195\n",
      "Validation loss decreased (inf --> 0.881951).  Saving model ...\n",
      "[  2/500] train_loss: 0.84461 valid_loss: 0.77936\n",
      "Validation loss decreased (0.881951 --> 0.779357).  Saving model ...\n",
      "[  3/500] train_loss: 0.77720 valid_loss: 0.70730\n",
      "Validation loss decreased (0.779357 --> 0.707296).  Saving model ...\n",
      "[  4/500] train_loss: 0.73780 valid_loss: 0.66454\n",
      "Validation loss decreased (0.707296 --> 0.664540).  Saving model ...\n",
      "[  5/500] train_loss: 0.72072 valid_loss: 0.64428\n",
      "Validation loss decreased (0.664540 --> 0.644280).  Saving model ...\n",
      "[  6/500] train_loss: 0.70497 valid_loss: 0.62963\n",
      "Validation loss decreased (0.644280 --> 0.629629).  Saving model ...\n",
      "[  7/500] train_loss: 0.69550 valid_loss: 0.62046\n",
      "Validation loss decreased (0.629629 --> 0.620463).  Saving model ...\n",
      "[  8/500] train_loss: 0.67884 valid_loss: 0.61432\n",
      "Validation loss decreased (0.620463 --> 0.614321).  Saving model ...\n",
      "[  9/500] train_loss: 0.67625 valid_loss: 0.60475\n",
      "Validation loss decreased (0.614321 --> 0.604745).  Saving model ...\n",
      "[ 10/500] train_loss: 0.66972 valid_loss: 0.60049\n",
      "Validation loss decreased (0.604745 --> 0.600494).  Saving model ...\n",
      "[ 11/500] train_loss: 0.65709 valid_loss: 0.59735\n",
      "Validation loss decreased (0.600494 --> 0.597355).  Saving model ...\n",
      "[ 12/500] train_loss: 0.65377 valid_loss: 0.59321\n",
      "Validation loss decreased (0.597355 --> 0.593210).  Saving model ...\n",
      "[ 13/500] train_loss: 0.64540 valid_loss: 0.58889\n",
      "Validation loss decreased (0.593210 --> 0.588895).  Saving model ...\n",
      "[ 14/500] train_loss: 0.64614 valid_loss: 0.58778\n",
      "Validation loss decreased (0.588895 --> 0.587783).  Saving model ...\n",
      "[ 15/500] train_loss: 0.63766 valid_loss: 0.58249\n",
      "Validation loss decreased (0.587783 --> 0.582492).  Saving model ...\n",
      "[ 16/500] train_loss: 0.63814 valid_loss: 0.57822\n",
      "Validation loss decreased (0.582492 --> 0.578219).  Saving model ...\n",
      "[ 17/500] train_loss: 0.63273 valid_loss: 0.57556\n",
      "Validation loss decreased (0.578219 --> 0.575557).  Saving model ...\n",
      "[ 18/500] train_loss: 0.62887 valid_loss: 0.57633\n",
      "[ 19/500] train_loss: 0.62842 valid_loss: 0.57385\n",
      "Validation loss decreased (0.575557 --> 0.573853).  Saving model ...\n",
      "[ 20/500] train_loss: 0.62626 valid_loss: 0.57357\n",
      "Validation loss decreased (0.573853 --> 0.573570).  Saving model ...\n",
      "[ 21/500] train_loss: 0.62776 valid_loss: 0.57304\n",
      "Validation loss decreased (0.573570 --> 0.573037).  Saving model ...\n",
      "[ 22/500] train_loss: 0.62346 valid_loss: 0.57399\n",
      "[ 23/500] train_loss: 0.61861 valid_loss: 0.57253\n",
      "Validation loss decreased (0.573037 --> 0.572528).  Saving model ...\n",
      "[ 24/500] train_loss: 0.62047 valid_loss: 0.57386\n",
      "[ 25/500] train_loss: 0.61705 valid_loss: 0.57290\n",
      "[ 26/500] train_loss: 0.61979 valid_loss: 0.57139\n",
      "Validation loss decreased (0.572528 --> 0.571390).  Saving model ...\n",
      "[ 27/500] train_loss: 0.61528 valid_loss: 0.56945\n",
      "Validation loss decreased (0.571390 --> 0.569450).  Saving model ...\n",
      "[ 28/500] train_loss: 0.61029 valid_loss: 0.56703\n",
      "Validation loss decreased (0.569450 --> 0.567029).  Saving model ...\n",
      "[ 29/500] train_loss: 0.61433 valid_loss: 0.56440\n",
      "Validation loss decreased (0.567029 --> 0.564397).  Saving model ...\n",
      "[ 30/500] train_loss: 0.61198 valid_loss: 0.56523\n",
      "[ 31/500] train_loss: 0.61185 valid_loss: 0.56504\n",
      "[ 32/500] train_loss: 0.60991 valid_loss: 0.56247\n",
      "Validation loss decreased (0.564397 --> 0.562472).  Saving model ...\n",
      "[ 33/500] train_loss: 0.60859 valid_loss: 0.56442\n",
      "[ 34/500] train_loss: 0.60971 valid_loss: 0.56486\n",
      "[ 35/500] train_loss: 0.60506 valid_loss: 0.56453\n",
      "[ 36/500] train_loss: 0.60917 valid_loss: 0.56277\n",
      "[ 37/500] train_loss: 0.60223 valid_loss: 0.56242\n",
      "Validation loss decreased (0.562472 --> 0.562420).  Saving model ...\n",
      "[ 38/500] train_loss: 0.60746 valid_loss: 0.56380\n",
      "[ 39/500] train_loss: 0.60316 valid_loss: 0.55968\n",
      "Validation loss decreased (0.562420 --> 0.559683).  Saving model ...\n",
      "[ 40/500] train_loss: 0.60405 valid_loss: 0.56174\n",
      "[ 41/500] train_loss: 0.60116 valid_loss: 0.56294\n",
      "[ 42/500] train_loss: 0.60544 valid_loss: 0.56052\n",
      "[ 43/500] train_loss: 0.60218 valid_loss: 0.55893\n",
      "Validation loss decreased (0.559683 --> 0.558928).  Saving model ...\n",
      "[ 44/500] train_loss: 0.60010 valid_loss: 0.55886\n",
      "Validation loss decreased (0.558928 --> 0.558860).  Saving model ...\n",
      "[ 45/500] train_loss: 0.59812 valid_loss: 0.55942\n",
      "[ 46/500] train_loss: 0.59878 valid_loss: 0.56216\n",
      "[ 47/500] train_loss: 0.59634 valid_loss: 0.56078\n",
      "[ 48/500] train_loss: 0.59732 valid_loss: 0.55896\n",
      "[ 49/500] train_loss: 0.60109 valid_loss: 0.56007\n",
      "[ 50/500] train_loss: 0.59610 valid_loss: 0.55743\n",
      "Validation loss decreased (0.558860 --> 0.557434).  Saving model ...\n",
      "[ 51/500] train_loss: 0.59626 valid_loss: 0.55766\n",
      "[ 52/500] train_loss: 0.60042 valid_loss: 0.55609\n",
      "Validation loss decreased (0.557434 --> 0.556089).  Saving model ...\n",
      "[ 53/500] train_loss: 0.59603 valid_loss: 0.55600\n",
      "Validation loss decreased (0.556089 --> 0.555995).  Saving model ...\n",
      "[ 54/500] train_loss: 0.59521 valid_loss: 0.55663\n",
      "[ 55/500] train_loss: 0.59812 valid_loss: 0.55835\n",
      "[ 56/500] train_loss: 0.59262 valid_loss: 0.55672\n",
      "[ 57/500] train_loss: 0.59396 valid_loss: 0.55424\n",
      "Validation loss decreased (0.555995 --> 0.554243).  Saving model ...\n",
      "[ 58/500] train_loss: 0.58874 valid_loss: 0.55852\n",
      "[ 59/500] train_loss: 0.59178 valid_loss: 0.55230\n",
      "Validation loss decreased (0.554243 --> 0.552303).  Saving model ...\n",
      "[ 60/500] train_loss: 0.58966 valid_loss: 0.55185\n",
      "Validation loss decreased (0.552303 --> 0.551851).  Saving model ...\n",
      "[ 61/500] train_loss: 0.59145 valid_loss: 0.55491\n",
      "[ 62/500] train_loss: 0.58989 valid_loss: 0.55579\n",
      "[ 63/500] train_loss: 0.59018 valid_loss: 0.55561\n",
      "[ 64/500] train_loss: 0.58683 valid_loss: 0.55348\n",
      "[ 65/500] train_loss: 0.58859 valid_loss: 0.55378\n",
      "[ 66/500] train_loss: 0.58910 valid_loss: 0.54837\n",
      "Validation loss decreased (0.551851 --> 0.548367).  Saving model ...\n",
      "[ 67/500] train_loss: 0.58812 valid_loss: 0.55155\n",
      "[ 68/500] train_loss: 0.58808 valid_loss: 0.55312\n",
      "[ 69/500] train_loss: 0.58957 valid_loss: 0.55181\n",
      "[ 70/500] train_loss: 0.58602 valid_loss: 0.55119\n",
      "[ 71/500] train_loss: 0.58798 valid_loss: 0.55227\n",
      "[ 72/500] train_loss: 0.58495 valid_loss: 0.55199\n",
      "[ 73/500] train_loss: 0.58581 valid_loss: 0.55326\n",
      "[ 74/500] train_loss: 0.58219 valid_loss: 0.55350\n",
      "[ 75/500] train_loss: 0.58823 valid_loss: 0.55131\n",
      "[ 76/500] train_loss: 0.58553 valid_loss: 0.54922\n",
      "[ 77/500] train_loss: 0.58486 valid_loss: 0.55055\n",
      "Epoch 00077: reducing learning rate of group 0 to 1.4000e-02.\n",
      "[ 78/500] train_loss: 0.56953 valid_loss: 0.54905\n",
      "[ 79/500] train_loss: 0.59573 valid_loss: 0.51377\n",
      "Validation loss decreased (0.548367 --> 0.513774).  Saving model ...\n",
      "[ 80/500] train_loss: 0.58164 valid_loss: 0.51931\n",
      "[ 81/500] train_loss: 0.57773 valid_loss: 0.52265\n",
      "[ 82/500] train_loss: 0.57799 valid_loss: 0.52590\n",
      "[ 83/500] train_loss: 0.58271 valid_loss: 0.52770\n",
      "[ 84/500] train_loss: 0.58117 valid_loss: 0.52848\n",
      "[ 85/500] train_loss: 0.57902 valid_loss: 0.52981\n",
      "[ 86/500] train_loss: 0.57770 valid_loss: 0.53033\n",
      "[ 87/500] train_loss: 0.57685 valid_loss: 0.53023\n",
      "[ 88/500] train_loss: 0.57544 valid_loss: 0.52969\n",
      "[ 89/500] train_loss: 0.57729 valid_loss: 0.53007\n",
      "[ 90/500] train_loss: 0.57689 valid_loss: 0.53058\n",
      "Epoch 00090: reducing learning rate of group 0 to 9.8000e-03.\n",
      "[ 91/500] train_loss: 0.55418 valid_loss: 0.51024\n",
      "Validation loss decreased (0.513774 --> 0.510244).  Saving model ...\n",
      "[ 92/500] train_loss: 0.56015 valid_loss: 0.51142\n",
      "[ 93/500] train_loss: 0.56251 valid_loss: 0.51250\n",
      "[ 94/500] train_loss: 0.55878 valid_loss: 0.51345\n",
      "[ 95/500] train_loss: 0.55872 valid_loss: 0.51414\n",
      "[ 96/500] train_loss: 0.55759 valid_loss: 0.51384\n",
      "[ 97/500] train_loss: 0.56282 valid_loss: 0.51458\n",
      "[ 98/500] train_loss: 0.55727 valid_loss: 0.51385\n",
      "[ 99/500] train_loss: 0.55628 valid_loss: 0.51280\n",
      "[100/500] train_loss: 0.55335 valid_loss: 0.52947\n",
      "[101/500] train_loss: 0.55861 valid_loss: 0.51328\n",
      "[102/500] train_loss: 0.55924 valid_loss: 0.51329\n",
      "Epoch 00102: reducing learning rate of group 0 to 6.8600e-03.\n",
      "[103/500] train_loss: 0.55048 valid_loss: 0.51985\n",
      "[104/500] train_loss: 0.54987 valid_loss: 0.51493\n",
      "[105/500] train_loss: 0.54883 valid_loss: 0.51401\n",
      "[106/500] train_loss: 0.55006 valid_loss: 0.51889\n",
      "[107/500] train_loss: 0.54520 valid_loss: 0.51937\n",
      "[108/500] train_loss: 0.54744 valid_loss: 0.52144\n",
      "[109/500] train_loss: 0.54490 valid_loss: 0.52062\n",
      "[110/500] train_loss: 0.54783 valid_loss: 0.52219\n",
      "[111/500] train_loss: 0.54804 valid_loss: 0.51912\n",
      "[112/500] train_loss: 0.54715 valid_loss: 0.51882\n",
      "[113/500] train_loss: 0.54491 valid_loss: 0.51840\n",
      "Epoch 00113: reducing learning rate of group 0 to 4.8020e-03.\n",
      "[114/500] train_loss: 0.54606 valid_loss: 0.50676\n",
      "Validation loss decreased (0.510244 --> 0.506760).  Saving model ...\n",
      "[115/500] train_loss: 0.54413 valid_loss: 0.50795\n",
      "[116/500] train_loss: 0.54394 valid_loss: 0.50892\n",
      "[117/500] train_loss: 0.54601 valid_loss: 0.51040\n",
      "[118/500] train_loss: 0.54213 valid_loss: 0.51049\n",
      "[119/500] train_loss: 0.54548 valid_loss: 0.51018\n",
      "[120/500] train_loss: 0.54048 valid_loss: 0.51039\n",
      "[121/500] train_loss: 0.54030 valid_loss: 0.50991\n",
      "[122/500] train_loss: 0.54033 valid_loss: 0.50899\n",
      "[123/500] train_loss: 0.54480 valid_loss: 0.50858\n",
      "[124/500] train_loss: 0.54095 valid_loss: 0.50848\n",
      "[125/500] train_loss: 0.54337 valid_loss: 0.50864\n",
      "Epoch 00125: reducing learning rate of group 0 to 3.3614e-03.\n",
      "[126/500] train_loss: 0.54342 valid_loss: 0.49841\n",
      "Validation loss decreased (0.506760 --> 0.498405).  Saving model ...\n",
      "[127/500] train_loss: 0.53988 valid_loss: 0.50132\n",
      "[128/500] train_loss: 0.54325 valid_loss: 0.50133\n",
      "[129/500] train_loss: 0.53875 valid_loss: 0.50191\n",
      "[130/500] train_loss: 0.53942 valid_loss: 0.50170\n",
      "[131/500] train_loss: 0.53875 valid_loss: 0.50212\n",
      "[132/500] train_loss: 0.54064 valid_loss: 0.50271\n",
      "[133/500] train_loss: 0.54005 valid_loss: 0.50168\n",
      "[134/500] train_loss: 0.53846 valid_loss: 0.50153\n",
      "[135/500] train_loss: 0.54213 valid_loss: 0.50147\n",
      "[136/500] train_loss: 0.53923 valid_loss: 0.50145\n",
      "[137/500] train_loss: 0.53928 valid_loss: 0.50233\n",
      "Epoch 00137: reducing learning rate of group 0 to 2.3530e-03.\n",
      "[138/500] train_loss: 0.54389 valid_loss: 0.49586\n",
      "Validation loss decreased (0.498405 --> 0.495865).  Saving model ...\n",
      "[139/500] train_loss: 0.53888 valid_loss: 0.49748\n",
      "[140/500] train_loss: 0.53867 valid_loss: 0.49875\n",
      "[141/500] train_loss: 0.54027 valid_loss: 0.49853\n",
      "[142/500] train_loss: 0.54074 valid_loss: 0.49880\n",
      "[143/500] train_loss: 0.53718 valid_loss: 0.49858\n",
      "[144/500] train_loss: 0.53893 valid_loss: 0.49831\n",
      "[145/500] train_loss: 0.53727 valid_loss: 0.49840\n",
      "[146/500] train_loss: 0.53850 valid_loss: 0.49801\n",
      "[147/500] train_loss: 0.54039 valid_loss: 0.49792\n",
      "[148/500] train_loss: 0.54036 valid_loss: 0.49769\n",
      "[149/500] train_loss: 0.53891 valid_loss: 0.49778\n",
      "Epoch 00149: reducing learning rate of group 0 to 1.6471e-03.\n",
      "[150/500] train_loss: 0.53723 valid_loss: 0.49510\n",
      "Validation loss decreased (0.495865 --> 0.495095).  Saving model ...\n",
      "[151/500] train_loss: 0.53617 valid_loss: 0.49531\n",
      "[152/500] train_loss: 0.54309 valid_loss: 0.49538\n",
      "[153/500] train_loss: 0.53891 valid_loss: 0.49560\n",
      "[154/500] train_loss: 0.54146 valid_loss: 0.49554\n",
      "[155/500] train_loss: 0.53668 valid_loss: 0.49544\n",
      "[156/500] train_loss: 0.53951 valid_loss: 0.49528\n",
      "[157/500] train_loss: 0.53797 valid_loss: 0.49531\n",
      "[158/500] train_loss: 0.53788 valid_loss: 0.49533\n",
      "[159/500] train_loss: 0.53856 valid_loss: 0.49532\n",
      "[160/500] train_loss: 0.53714 valid_loss: 0.49558\n",
      "[161/500] train_loss: 0.54043 valid_loss: 0.49545\n",
      "Epoch 00161: reducing learning rate of group 0 to 1.1530e-03.\n",
      "[162/500] train_loss: 0.54111 valid_loss: 0.49527\n",
      "[163/500] train_loss: 0.53830 valid_loss: 0.49525\n",
      "[164/500] train_loss: 0.53756 valid_loss: 0.49521\n",
      "[165/500] train_loss: 0.53650 valid_loss: 0.49515\n",
      "[166/500] train_loss: 0.54024 valid_loss: 0.49521\n",
      "[167/500] train_loss: 0.54125 valid_loss: 0.49521\n",
      "[168/500] train_loss: 0.54140 valid_loss: 0.49521\n",
      "[169/500] train_loss: 0.54326 valid_loss: 0.49514\n",
      "[170/500] train_loss: 0.54236 valid_loss: 0.49513\n",
      "[171/500] train_loss: 0.53833 valid_loss: 0.49510\n",
      "[172/500] train_loss: 0.53764 valid_loss: 0.49506\n",
      "Epoch 00172: reducing learning rate of group 0 to 8.0707e-04.\n",
      "Validation loss decreased (0.495095 --> 0.495061).  Saving model ...\n",
      "[173/500] train_loss: 0.54012 valid_loss: 0.49493\n",
      "Validation loss decreased (0.495061 --> 0.494934).  Saving model ...\n",
      "[174/500] train_loss: 0.54062 valid_loss: 0.49493\n",
      "Validation loss decreased (0.494934 --> 0.494926).  Saving model ...\n",
      "[175/500] train_loss: 0.53868 valid_loss: 0.49488\n",
      "Validation loss decreased (0.494926 --> 0.494884).  Saving model ...\n",
      "[176/500] train_loss: 0.53929 valid_loss: 0.49493\n",
      "[177/500] train_loss: 0.53878 valid_loss: 0.49493\n",
      "[178/500] train_loss: 0.53831 valid_loss: 0.49492\n",
      "[179/500] train_loss: 0.53689 valid_loss: 0.49488\n",
      "Validation loss decreased (0.494884 --> 0.494876).  Saving model ...\n",
      "[180/500] train_loss: 0.53741 valid_loss: 0.49488\n",
      "[181/500] train_loss: 0.53715 valid_loss: 0.49487\n",
      "Validation loss decreased (0.494876 --> 0.494865).  Saving model ...\n",
      "[182/500] train_loss: 0.53652 valid_loss: 0.49484\n",
      "Validation loss decreased (0.494865 --> 0.494837).  Saving model ...\n",
      "[183/500] train_loss: 0.53894 valid_loss: 0.49490\n",
      "[184/500] train_loss: 0.53661 valid_loss: 0.49479\n",
      "Validation loss decreased (0.494837 --> 0.494785).  Saving model ...\n",
      "[185/500] train_loss: 0.53862 valid_loss: 0.49482\n",
      "[186/500] train_loss: 0.53946 valid_loss: 0.49479\n",
      "[187/500] train_loss: 0.53983 valid_loss: 0.49477\n",
      "Validation loss decreased (0.494785 --> 0.494766).  Saving model ...\n",
      "[188/500] train_loss: 0.53574 valid_loss: 0.49469\n",
      "Validation loss decreased (0.494766 --> 0.494692).  Saving model ...\n",
      "[189/500] train_loss: 0.53991 valid_loss: 0.49472\n",
      "[190/500] train_loss: 0.53769 valid_loss: 0.49482\n",
      "[191/500] train_loss: 0.53941 valid_loss: 0.49482\n",
      "[192/500] train_loss: 0.53809 valid_loss: 0.49480\n",
      "[193/500] train_loss: 0.53770 valid_loss: 0.49475\n",
      "[194/500] train_loss: 0.53739 valid_loss: 0.49473\n",
      "[195/500] train_loss: 0.54282 valid_loss: 0.49469\n",
      "[196/500] train_loss: 0.53535 valid_loss: 0.49468\n",
      "Validation loss decreased (0.494692 --> 0.494677).  Saving model ...\n",
      "[197/500] train_loss: 0.53712 valid_loss: 0.49470\n",
      "[198/500] train_loss: 0.53821 valid_loss: 0.49460\n",
      "Validation loss decreased (0.494677 --> 0.494603).  Saving model ...\n",
      "[199/500] train_loss: 0.53793 valid_loss: 0.49461\n",
      "[200/500] train_loss: 0.53856 valid_loss: 0.51059\n",
      "[201/500] train_loss: 0.53889 valid_loss: 0.49466\n",
      "[202/500] train_loss: 0.53750 valid_loss: 0.49464\n",
      "[203/500] train_loss: 0.54211 valid_loss: 0.49460\n",
      "Validation loss decreased (0.494603 --> 0.494596).  Saving model ...\n",
      "[204/500] train_loss: 0.54007 valid_loss: 0.49467\n",
      "[205/500] train_loss: 0.54011 valid_loss: 0.49462\n",
      "[206/500] train_loss: 0.53801 valid_loss: 0.49464\n",
      "[207/500] train_loss: 0.53801 valid_loss: 0.49462\n",
      "[208/500] train_loss: 0.53828 valid_loss: 0.49451\n",
      "Validation loss decreased (0.494596 --> 0.494510).  Saving model ...\n",
      "[209/500] train_loss: 0.53693 valid_loss: 0.49451\n",
      "[210/500] train_loss: 0.54157 valid_loss: 0.49445\n",
      "Validation loss decreased (0.494510 --> 0.494450).  Saving model ...\n",
      "[211/500] train_loss: 0.53316 valid_loss: 0.49451\n",
      "[212/500] train_loss: 0.53875 valid_loss: 0.49444\n",
      "Validation loss decreased (0.494450 --> 0.494438).  Saving model ...\n",
      "[213/500] train_loss: 0.53876 valid_loss: 0.49448\n",
      "[214/500] train_loss: 0.53817 valid_loss: 0.49444\n",
      "Validation loss decreased (0.494438 --> 0.494437).  Saving model ...\n",
      "[215/500] train_loss: 0.53935 valid_loss: 0.49451\n",
      "[216/500] train_loss: 0.53894 valid_loss: 0.49457\n",
      "[217/500] train_loss: 0.53602 valid_loss: 0.49451\n",
      "[218/500] train_loss: 0.53705 valid_loss: 0.49445\n",
      "[219/500] train_loss: 0.53803 valid_loss: 0.49441\n",
      "Validation loss decreased (0.494437 --> 0.494414).  Saving model ...\n",
      "[220/500] train_loss: 0.53788 valid_loss: 0.49444\n",
      "[221/500] train_loss: 0.53780 valid_loss: 0.49442\n",
      "Epoch 00221: reducing learning rate of group 0 to 5.6495e-04.\n",
      "[222/500] train_loss: 0.54108 valid_loss: 0.49433\n",
      "Validation loss decreased (0.494414 --> 0.494331).  Saving model ...\n",
      "[223/500] train_loss: 0.53801 valid_loss: 0.49433\n",
      "Validation loss decreased (0.494331 --> 0.494327).  Saving model ...\n",
      "[224/500] train_loss: 0.53764 valid_loss: 0.49429\n",
      "Validation loss decreased (0.494327 --> 0.494288).  Saving model ...\n",
      "[225/500] train_loss: 0.54208 valid_loss: 0.49428\n",
      "Validation loss decreased (0.494288 --> 0.494277).  Saving model ...\n",
      "[226/500] train_loss: 0.53920 valid_loss: 0.49426\n",
      "Validation loss decreased (0.494277 --> 0.494264).  Saving model ...\n",
      "[227/500] train_loss: 0.53653 valid_loss: 0.49420\n",
      "Validation loss decreased (0.494264 --> 0.494199).  Saving model ...\n",
      "[228/500] train_loss: 0.53983 valid_loss: 0.49424\n",
      "[229/500] train_loss: 0.53651 valid_loss: 0.49430\n",
      "[230/500] train_loss: 0.54122 valid_loss: 0.49430\n",
      "[231/500] train_loss: 0.53660 valid_loss: 0.49429\n",
      "[232/500] train_loss: 0.53627 valid_loss: 0.49433\n",
      "[233/500] train_loss: 0.53460 valid_loss: 0.49432\n",
      "[234/500] train_loss: 0.53874 valid_loss: 0.49429\n",
      "[235/500] train_loss: 0.53724 valid_loss: 0.49422\n",
      "[236/500] train_loss: 0.53643 valid_loss: 0.49425\n",
      "[237/500] train_loss: 0.53600 valid_loss: 0.49424\n",
      "[238/500] train_loss: 0.53666 valid_loss: 0.49422\n",
      "Epoch 00238: reducing learning rate of group 0 to 3.9547e-04.\n",
      "[239/500] train_loss: 0.53859 valid_loss: 0.49426\n",
      "[240/500] train_loss: 0.53828 valid_loss: 0.49425\n",
      "[241/500] train_loss: 0.53873 valid_loss: 0.49425\n",
      "[242/500] train_loss: 0.53690 valid_loss: 0.49426\n",
      "[243/500] train_loss: 0.53720 valid_loss: 0.49425\n",
      "[244/500] train_loss: 0.53779 valid_loss: 0.49419\n",
      "Validation loss decreased (0.494199 --> 0.494190).  Saving model ...\n",
      "[245/500] train_loss: 0.53863 valid_loss: 0.49421\n",
      "[246/500] train_loss: 0.53702 valid_loss: 0.49422\n",
      "[247/500] train_loss: 0.54157 valid_loss: 0.49423\n",
      "[248/500] train_loss: 0.53722 valid_loss: 0.49421\n",
      "[249/500] train_loss: 0.53893 valid_loss: 0.49420\n",
      "Epoch 00249: reducing learning rate of group 0 to 2.7683e-04.\n",
      "[250/500] train_loss: 0.53534 valid_loss: 0.49418\n",
      "Validation loss decreased (0.494190 --> 0.494184).  Saving model ...\n",
      "[251/500] train_loss: 0.53432 valid_loss: 0.49419\n",
      "[252/500] train_loss: 0.53640 valid_loss: 0.49415\n",
      "Validation loss decreased (0.494184 --> 0.494153).  Saving model ...\n",
      "[253/500] train_loss: 0.53799 valid_loss: 0.49416\n",
      "[254/500] train_loss: 0.53609 valid_loss: 0.49418\n",
      "[255/500] train_loss: 0.53595 valid_loss: 0.49419\n",
      "[256/500] train_loss: 0.53959 valid_loss: 0.49417\n",
      "[257/500] train_loss: 0.53657 valid_loss: 0.49416\n",
      "[258/500] train_loss: 0.53668 valid_loss: 0.49411\n",
      "Validation loss decreased (0.494153 --> 0.494115).  Saving model ...\n",
      "[259/500] train_loss: 0.53785 valid_loss: 0.49413\n",
      "[260/500] train_loss: 0.53881 valid_loss: 0.49413\n",
      "[261/500] train_loss: 0.53755 valid_loss: 0.49414\n",
      "[262/500] train_loss: 0.53597 valid_loss: 0.49411\n",
      "Validation loss decreased (0.494115 --> 0.494113).  Saving model ...\n",
      "[263/500] train_loss: 0.53675 valid_loss: 0.49412\n",
      "[264/500] train_loss: 0.53349 valid_loss: 0.49411\n",
      "[265/500] train_loss: 0.53604 valid_loss: 0.49410\n",
      "Validation loss decreased (0.494113 --> 0.494104).  Saving model ...\n",
      "[266/500] train_loss: 0.53987 valid_loss: 0.49412\n",
      "[267/500] train_loss: 0.53842 valid_loss: 0.49411\n",
      "[268/500] train_loss: 0.53884 valid_loss: 0.49412\n",
      "[269/500] train_loss: 0.53597 valid_loss: 0.49413\n",
      "Epoch 00269: reducing learning rate of group 0 to 1.9378e-04.\n",
      "[270/500] train_loss: 0.53767 valid_loss: 0.49411\n",
      "[271/500] train_loss: 0.53801 valid_loss: 0.49411\n",
      "[272/500] train_loss: 0.53842 valid_loss: 0.49409\n",
      "Validation loss decreased (0.494104 --> 0.494092).  Saving model ...\n",
      "[273/500] train_loss: 0.53775 valid_loss: 0.49409\n",
      "Validation loss decreased (0.494092 --> 0.494089).  Saving model ...\n",
      "[274/500] train_loss: 0.53684 valid_loss: 0.49408\n",
      "Validation loss decreased (0.494089 --> 0.494081).  Saving model ...\n",
      "[275/500] train_loss: 0.53679 valid_loss: 0.49408\n",
      "Validation loss decreased (0.494081 --> 0.494077).  Saving model ...\n",
      "[276/500] train_loss: 0.53622 valid_loss: 0.49407\n",
      "Validation loss decreased (0.494077 --> 0.494066).  Saving model ...\n",
      "[277/500] train_loss: 0.53516 valid_loss: 0.49405\n",
      "Validation loss decreased (0.494066 --> 0.494048).  Saving model ...\n",
      "[278/500] train_loss: 0.53689 valid_loss: 0.49406\n",
      "[279/500] train_loss: 0.53748 valid_loss: 0.49407\n",
      "[280/500] train_loss: 0.53792 valid_loss: 0.49405\n",
      "[281/500] train_loss: 0.53644 valid_loss: 0.49405\n",
      "[282/500] train_loss: 0.53761 valid_loss: 0.49403\n",
      "Validation loss decreased (0.494048 --> 0.494034).  Saving model ...\n",
      "[283/500] train_loss: 0.53755 valid_loss: 0.49404\n",
      "[284/500] train_loss: 0.53617 valid_loss: 0.49405\n",
      "[285/500] train_loss: 0.53711 valid_loss: 0.49405\n",
      "[286/500] train_loss: 0.53219 valid_loss: 0.49406\n",
      "[287/500] train_loss: 0.53535 valid_loss: 0.49404\n",
      "[288/500] train_loss: 0.53808 valid_loss: 0.49402\n",
      "Epoch 00288: reducing learning rate of group 0 to 1.3564e-04.\n",
      "Validation loss decreased (0.494034 --> 0.494020).  Saving model ...\n",
      "[289/500] train_loss: 0.53917 valid_loss: 0.49402\n",
      "Validation loss decreased (0.494020 --> 0.494018).  Saving model ...\n",
      "[290/500] train_loss: 0.53599 valid_loss: 0.49402\n",
      "Validation loss decreased (0.494018 --> 0.494017).  Saving model ...\n",
      "[291/500] train_loss: 0.53730 valid_loss: 0.49401\n",
      "Validation loss decreased (0.494017 --> 0.494005).  Saving model ...\n",
      "[292/500] train_loss: 0.53388 valid_loss: 0.49400\n",
      "Validation loss decreased (0.494005 --> 0.494001).  Saving model ...\n",
      "[293/500] train_loss: 0.53892 valid_loss: 0.49400\n",
      "[294/500] train_loss: 0.53634 valid_loss: 0.49399\n",
      "Validation loss decreased (0.494001 --> 0.493993).  Saving model ...\n",
      "[295/500] train_loss: 0.53840 valid_loss: 0.49398\n",
      "Validation loss decreased (0.493993 --> 0.493982).  Saving model ...\n",
      "[296/500] train_loss: 0.53712 valid_loss: 0.49397\n",
      "Validation loss decreased (0.493982 --> 0.493975).  Saving model ...\n",
      "[297/500] train_loss: 0.53514 valid_loss: 0.49398\n",
      "[298/500] train_loss: 0.53714 valid_loss: 0.49399\n",
      "[299/500] train_loss: 0.53490 valid_loss: 0.49399\n",
      "[300/500] train_loss: 0.53720 valid_loss: 0.50992\n",
      "[301/500] train_loss: 0.53626 valid_loss: 0.49397\n",
      "Validation loss decreased (0.493975 --> 0.493969).  Saving model ...\n",
      "[302/500] train_loss: 0.53532 valid_loss: 0.49398\n",
      "[303/500] train_loss: 0.53536 valid_loss: 0.49398\n",
      "[304/500] train_loss: 0.53829 valid_loss: 0.49398\n",
      "[305/500] train_loss: 0.53653 valid_loss: 0.49397\n",
      "Epoch 00305: reducing learning rate of group 0 to 9.4951e-05.\n",
      "[306/500] train_loss: 0.53763 valid_loss: 0.49396\n",
      "Validation loss decreased (0.493969 --> 0.493964).  Saving model ...\n",
      "[307/500] train_loss: 0.53740 valid_loss: 0.49397\n",
      "[308/500] train_loss: 0.53823 valid_loss: 0.49395\n",
      "Validation loss decreased (0.493964 --> 0.493952).  Saving model ...\n",
      "[309/500] train_loss: 0.53768 valid_loss: 0.49395\n",
      "Validation loss decreased (0.493952 --> 0.493952).  Saving model ...\n",
      "[310/500] train_loss: 0.53608 valid_loss: 0.49395\n",
      "Validation loss decreased (0.493952 --> 0.493949).  Saving model ...\n",
      "[311/500] train_loss: 0.53800 valid_loss: 0.49394\n",
      "Validation loss decreased (0.493949 --> 0.493943).  Saving model ...\n",
      "[312/500] train_loss: 0.53693 valid_loss: 0.49394\n",
      "Validation loss decreased (0.493943 --> 0.493941).  Saving model ...\n",
      "[313/500] train_loss: 0.53848 valid_loss: 0.49393\n",
      "Validation loss decreased (0.493941 --> 0.493925).  Saving model ...\n",
      "[314/500] train_loss: 0.54063 valid_loss: 0.49392\n",
      "Validation loss decreased (0.493925 --> 0.493921).  Saving model ...\n",
      "[315/500] train_loss: 0.53591 valid_loss: 0.49392\n",
      "[316/500] train_loss: 0.53637 valid_loss: 0.49392\n",
      "[317/500] train_loss: 0.53856 valid_loss: 0.49392\n",
      "Validation loss decreased (0.493921 --> 0.493917).  Saving model ...\n",
      "[318/500] train_loss: 0.53352 valid_loss: 0.49391\n",
      "Validation loss decreased (0.493917 --> 0.493906).  Saving model ...\n",
      "[319/500] train_loss: 0.53808 valid_loss: 0.49391\n",
      "[320/500] train_loss: 0.53701 valid_loss: 0.49390\n",
      "Validation loss decreased (0.493906 --> 0.493900).  Saving model ...\n",
      "[321/500] train_loss: 0.53924 valid_loss: 0.49389\n",
      "Validation loss decreased (0.493900 --> 0.493886).  Saving model ...\n",
      "[322/500] train_loss: 0.53298 valid_loss: 0.49389\n",
      "[323/500] train_loss: 0.53641 valid_loss: 0.49389\n",
      "[324/500] train_loss: 0.53480 valid_loss: 0.49388\n",
      "Validation loss decreased (0.493886 --> 0.493885).  Saving model ...\n",
      "[325/500] train_loss: 0.53547 valid_loss: 0.49388\n",
      "[326/500] train_loss: 0.53479 valid_loss: 0.49388\n",
      "Validation loss decreased (0.493885 --> 0.493877).  Saving model ...\n",
      "[327/500] train_loss: 0.53412 valid_loss: 0.49388\n",
      "[328/500] train_loss: 0.53321 valid_loss: 0.49388\n",
      "Validation loss decreased (0.493877 --> 0.493875).  Saving model ...\n",
      "[329/500] train_loss: 0.53639 valid_loss: 0.49387\n",
      "Validation loss decreased (0.493875 --> 0.493874).  Saving model ...\n",
      "[330/500] train_loss: 0.53813 valid_loss: 0.49386\n",
      "Validation loss decreased (0.493874 --> 0.493863).  Saving model ...\n",
      "[331/500] train_loss: 0.53480 valid_loss: 0.49386\n",
      "Validation loss decreased (0.493863 --> 0.493862).  Saving model ...\n",
      "[332/500] train_loss: 0.53815 valid_loss: 0.49387\n",
      "Epoch 00332: reducing learning rate of group 0 to 6.6466e-05.\n",
      "[333/500] train_loss: 0.53595 valid_loss: 0.49387\n",
      "[334/500] train_loss: 0.53962 valid_loss: 0.49387\n",
      "[335/500] train_loss: 0.53556 valid_loss: 0.49387\n",
      "[336/500] train_loss: 0.53959 valid_loss: 0.49386\n",
      "Validation loss decreased (0.493862 --> 0.493860).  Saving model ...\n",
      "[337/500] train_loss: 0.54057 valid_loss: 0.49386\n",
      "Validation loss decreased (0.493860 --> 0.493857).  Saving model ...\n",
      "[338/500] train_loss: 0.53708 valid_loss: 0.49386\n",
      "Validation loss decreased (0.493857 --> 0.493857).  Saving model ...\n",
      "[339/500] train_loss: 0.53570 valid_loss: 0.49386\n",
      "[340/500] train_loss: 0.53824 valid_loss: 0.49386\n",
      "[341/500] train_loss: 0.53556 valid_loss: 0.49387\n",
      "[342/500] train_loss: 0.53527 valid_loss: 0.49388\n",
      "[343/500] train_loss: 0.53596 valid_loss: 0.49387\n",
      "Epoch 00343: reducing learning rate of group 0 to 4.6526e-05.\n",
      "[344/500] train_loss: 0.53731 valid_loss: 0.49387\n",
      "[345/500] train_loss: 0.53601 valid_loss: 0.49387\n",
      "[346/500] train_loss: 0.53528 valid_loss: 0.49387\n",
      "[347/500] train_loss: 0.53810 valid_loss: 0.49386\n",
      "[348/500] train_loss: 0.53808 valid_loss: 0.49387\n",
      "[349/500] train_loss: 0.53846 valid_loss: 0.49387\n",
      "[350/500] train_loss: 0.53623 valid_loss: 0.49387\n",
      "[351/500] train_loss: 0.53455 valid_loss: 0.49386\n",
      "[352/500] train_loss: 0.53555 valid_loss: 0.49386\n",
      "Validation loss decreased (0.493857 --> 0.493856).  Saving model ...\n",
      "[353/500] train_loss: 0.53831 valid_loss: 0.49385\n",
      "Validation loss decreased (0.493856 --> 0.493849).  Saving model ...\n",
      "[354/500] train_loss: 0.53660 valid_loss: 0.49385\n",
      "Epoch 00354: reducing learning rate of group 0 to 3.2568e-05.\n",
      "[355/500] train_loss: 0.53802 valid_loss: 0.49385\n",
      "Validation loss decreased (0.493849 --> 0.493847).  Saving model ...\n",
      "[356/500] train_loss: 0.53363 valid_loss: 0.49385\n",
      "[357/500] train_loss: 0.53651 valid_loss: 0.49385\n",
      "[358/500] train_loss: 0.53364 valid_loss: 0.49385\n",
      "Validation loss decreased (0.493847 --> 0.493846).  Saving model ...\n",
      "[359/500] train_loss: 0.53824 valid_loss: 0.49385\n",
      "[360/500] train_loss: 0.53873 valid_loss: 0.49384\n",
      "Validation loss decreased (0.493846 --> 0.493843).  Saving model ...\n",
      "[361/500] train_loss: 0.53530 valid_loss: 0.49384\n",
      "[362/500] train_loss: 0.53713 valid_loss: 0.49384\n",
      "Validation loss decreased (0.493843 --> 0.493843).  Saving model ...\n",
      "[363/500] train_loss: 0.53187 valid_loss: 0.49384\n",
      "Validation loss decreased (0.493843 --> 0.493839).  Saving model ...\n",
      "[364/500] train_loss: 0.53787 valid_loss: 0.49383\n",
      "Validation loss decreased (0.493839 --> 0.493834).  Saving model ...\n",
      "[365/500] train_loss: 0.53581 valid_loss: 0.49384\n",
      "[366/500] train_loss: 0.53764 valid_loss: 0.49384\n",
      "[367/500] train_loss: 0.53717 valid_loss: 0.49384\n",
      "[368/500] train_loss: 0.53729 valid_loss: 0.49383\n",
      "Validation loss decreased (0.493834 --> 0.493833).  Saving model ...\n",
      "[369/500] train_loss: 0.53881 valid_loss: 0.49383\n",
      "[370/500] train_loss: 0.53763 valid_loss: 0.49383\n",
      "Validation loss decreased (0.493833 --> 0.493827).  Saving model ...\n",
      "[371/500] train_loss: 0.53647 valid_loss: 0.49383\n",
      "Validation loss decreased (0.493827 --> 0.493825).  Saving model ...\n",
      "[372/500] train_loss: 0.53360 valid_loss: 0.49383\n",
      "[373/500] train_loss: 0.53895 valid_loss: 0.49383\n",
      "Validation loss decreased (0.493825 --> 0.493825).  Saving model ...\n",
      "[374/500] train_loss: 0.53609 valid_loss: 0.49382\n",
      "Validation loss decreased (0.493825 --> 0.493822).  Saving model ...\n",
      "[375/500] train_loss: 0.53582 valid_loss: 0.49382\n",
      "Epoch 00375: reducing learning rate of group 0 to 2.2798e-05.\n",
      "[376/500] train_loss: 0.53150 valid_loss: 0.49382\n",
      "Validation loss decreased (0.493822 --> 0.493822).  Saving model ...\n",
      "[377/500] train_loss: 0.53711 valid_loss: 0.49382\n",
      "Validation loss decreased (0.493822 --> 0.493821).  Saving model ...\n",
      "[378/500] train_loss: 0.53649 valid_loss: 0.49382\n",
      "Validation loss decreased (0.493821 --> 0.493821).  Saving model ...\n",
      "[379/500] train_loss: 0.53932 valid_loss: 0.49382\n",
      "Validation loss decreased (0.493821 --> 0.493819).  Saving model ...\n",
      "[380/500] train_loss: 0.53675 valid_loss: 0.49382\n",
      "Validation loss decreased (0.493819 --> 0.493816).  Saving model ...\n",
      "[381/500] train_loss: 0.53407 valid_loss: 0.49382\n",
      "[382/500] train_loss: 0.53539 valid_loss: 0.49382\n",
      "[383/500] train_loss: 0.53899 valid_loss: 0.49382\n",
      "[384/500] train_loss: 0.53591 valid_loss: 0.49382\n",
      "[385/500] train_loss: 0.53883 valid_loss: 0.49382\n",
      "[386/500] train_loss: 0.53706 valid_loss: 0.49382\n",
      "Epoch 00386: reducing learning rate of group 0 to 1.5958e-05.\n",
      "Validation loss decreased (0.493816 --> 0.493816).  Saving model ...\n",
      "[387/500] train_loss: 0.53316 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493816 --> 0.493815).  Saving model ...\n",
      "[388/500] train_loss: 0.53867 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493815 --> 0.493814).  Saving model ...\n",
      "[389/500] train_loss: 0.53759 valid_loss: 0.49381\n",
      "[390/500] train_loss: 0.53672 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493814 --> 0.493812).  Saving model ...\n",
      "[391/500] train_loss: 0.53546 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493812 --> 0.493811).  Saving model ...\n",
      "[392/500] train_loss: 0.53707 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493811 --> 0.493811).  Saving model ...\n",
      "[393/500] train_loss: 0.53862 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493811 --> 0.493810).  Saving model ...\n",
      "[394/500] train_loss: 0.53898 valid_loss: 0.49381\n",
      "[395/500] train_loss: 0.53465 valid_loss: 0.49381\n",
      "[396/500] train_loss: 0.53535 valid_loss: 0.49381\n",
      "[397/500] train_loss: 0.53983 valid_loss: 0.49381\n",
      "Epoch 00397: reducing learning rate of group 0 to 1.1171e-05.\n",
      "[398/500] train_loss: 0.53880 valid_loss: 0.49381\n",
      "[399/500] train_loss: 0.53571 valid_loss: 0.49381\n",
      "[400/500] train_loss: 0.53573 valid_loss: 0.50974\n",
      "[401/500] train_loss: 0.53716 valid_loss: 0.49381\n",
      "[402/500] train_loss: 0.53609 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493810 --> 0.493809).  Saving model ...\n",
      "[403/500] train_loss: 0.53631 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493809 --> 0.493809).  Saving model ...\n",
      "[404/500] train_loss: 0.53417 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493809 --> 0.493809).  Saving model ...\n",
      "[405/500] train_loss: 0.53961 valid_loss: 0.49381\n",
      "[406/500] train_loss: 0.53655 valid_loss: 0.49381\n",
      "[407/500] train_loss: 0.53905 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493809 --> 0.493808).  Saving model ...\n",
      "[408/500] train_loss: 0.53521 valid_loss: 0.49381\n",
      "Epoch 00408: reducing learning rate of group 0 to 7.8196e-06.\n",
      "[409/500] train_loss: 0.53623 valid_loss: 0.49381\n",
      "[410/500] train_loss: 0.53610 valid_loss: 0.49381\n",
      "[411/500] train_loss: 0.53566 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493808 --> 0.493808).  Saving model ...\n",
      "[412/500] train_loss: 0.53701 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493808 --> 0.493807).  Saving model ...\n",
      "[413/500] train_loss: 0.53522 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493807 --> 0.493806).  Saving model ...\n",
      "[414/500] train_loss: 0.53714 valid_loss: 0.49381\n",
      "[415/500] train_loss: 0.53653 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493806 --> 0.493806).  Saving model ...\n",
      "[416/500] train_loss: 0.53550 valid_loss: 0.49381\n",
      "[417/500] train_loss: 0.53733 valid_loss: 0.49381\n",
      "[418/500] train_loss: 0.53684 valid_loss: 0.49381\n",
      "Validation loss decreased (0.493806 --> 0.493805).  Saving model ...\n",
      "[419/500] train_loss: 0.53981 valid_loss: 0.49380\n",
      "Epoch 00419: reducing learning rate of group 0 to 5.4737e-06.\n",
      "Validation loss decreased (0.493805 --> 0.493804).  Saving model ...\n",
      "[420/500] train_loss: 0.53856 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493804 --> 0.493804).  Saving model ...\n",
      "[421/500] train_loss: 0.53654 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493804 --> 0.493804).  Saving model ...\n",
      "[422/500] train_loss: 0.53584 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493804 --> 0.493803).  Saving model ...\n",
      "[423/500] train_loss: 0.53390 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493803 --> 0.493803).  Saving model ...\n",
      "[424/500] train_loss: 0.53351 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493803 --> 0.493803).  Saving model ...\n",
      "[425/500] train_loss: 0.53505 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493803 --> 0.493803).  Saving model ...\n",
      "[426/500] train_loss: 0.53440 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493803 --> 0.493803).  Saving model ...\n",
      "[427/500] train_loss: 0.53568 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493803 --> 0.493802).  Saving model ...\n",
      "[428/500] train_loss: 0.53974 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493802 --> 0.493802).  Saving model ...\n",
      "[429/500] train_loss: 0.53522 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493802 --> 0.493801).  Saving model ...\n",
      "[430/500] train_loss: 0.53915 valid_loss: 0.49380\n",
      "Epoch 00430: reducing learning rate of group 0 to 3.8316e-06.\n",
      "[431/500] train_loss: 0.53653 valid_loss: 0.49380\n",
      "[432/500] train_loss: 0.53643 valid_loss: 0.49380\n",
      "[433/500] train_loss: 0.53879 valid_loss: 0.49380\n",
      "[434/500] train_loss: 0.53525 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493801 --> 0.493801).  Saving model ...\n",
      "[435/500] train_loss: 0.53490 valid_loss: 0.49380\n",
      "[436/500] train_loss: 0.53696 valid_loss: 0.49380\n",
      "[437/500] train_loss: 0.53510 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493801 --> 0.493801).  Saving model ...\n",
      "[438/500] train_loss: 0.53843 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493801 --> 0.493801).  Saving model ...\n",
      "[439/500] train_loss: 0.53558 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493801 --> 0.493801).  Saving model ...\n",
      "[440/500] train_loss: 0.53368 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493801 --> 0.493801).  Saving model ...\n",
      "[441/500] train_loss: 0.53299 valid_loss: 0.49380\n",
      "Epoch 00441: reducing learning rate of group 0 to 2.6821e-06.\n",
      "Validation loss decreased (0.493801 --> 0.493801).  Saving model ...\n",
      "[442/500] train_loss: 0.53678 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493801 --> 0.493800).  Saving model ...\n",
      "[443/500] train_loss: 0.53567 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[444/500] train_loss: 0.53481 valid_loss: 0.49380\n",
      "[445/500] train_loss: 0.53486 valid_loss: 0.49380\n",
      "[446/500] train_loss: 0.53941 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[447/500] train_loss: 0.53510 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[448/500] train_loss: 0.53463 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[449/500] train_loss: 0.53582 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[450/500] train_loss: 0.53976 valid_loss: 0.49380\n",
      "[451/500] train_loss: 0.53569 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[452/500] train_loss: 0.53612 valid_loss: 0.49380\n",
      "Epoch 00452: reducing learning rate of group 0 to 1.8775e-06.\n",
      "[453/500] train_loss: 0.53515 valid_loss: 0.49380\n",
      "[454/500] train_loss: 0.53667 valid_loss: 0.49380\n",
      "[455/500] train_loss: 0.53522 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[456/500] train_loss: 0.53569 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493800).  Saving model ...\n",
      "[457/500] train_loss: 0.53685 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493800 --> 0.493799).  Saving model ...\n",
      "[458/500] train_loss: 0.53712 valid_loss: 0.49380\n",
      "[459/500] train_loss: 0.53794 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[460/500] train_loss: 0.53767 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[461/500] train_loss: 0.53549 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[462/500] train_loss: 0.53247 valid_loss: 0.49380\n",
      "[463/500] train_loss: 0.53553 valid_loss: 0.49380\n",
      "Epoch 00463: reducing learning rate of group 0 to 1.3142e-06.\n",
      "[464/500] train_loss: 0.53571 valid_loss: 0.49380\n",
      "[465/500] train_loss: 0.53766 valid_loss: 0.49380\n",
      "[466/500] train_loss: 0.53828 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[467/500] train_loss: 0.53669 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[468/500] train_loss: 0.53742 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[469/500] train_loss: 0.53530 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[470/500] train_loss: 0.53552 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[471/500] train_loss: 0.53814 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[472/500] train_loss: 0.53629 valid_loss: 0.49380\n",
      "[473/500] train_loss: 0.53655 valid_loss: 0.49380\n",
      "[474/500] train_loss: 0.53752 valid_loss: 0.49380\n",
      "Epoch 00474: reducing learning rate of group 0 to 9.1997e-07.\n",
      "[475/500] train_loss: 0.53535 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[476/500] train_loss: 0.53540 valid_loss: 0.49380\n",
      "[477/500] train_loss: 0.53730 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[478/500] train_loss: 0.53661 valid_loss: 0.49380\n",
      "[479/500] train_loss: 0.53682 valid_loss: 0.49380\n",
      "[480/500] train_loss: 0.53447 valid_loss: 0.49380\n",
      "[481/500] train_loss: 0.53572 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[482/500] train_loss: 0.53777 valid_loss: 0.49380\n",
      "[483/500] train_loss: 0.54087 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[484/500] train_loss: 0.53367 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[485/500] train_loss: 0.53868 valid_loss: 0.49380\n",
      "Epoch 00485: reducing learning rate of group 0 to 6.4398e-07.\n",
      "Validation loss decreased (0.493799 --> 0.493799).  Saving model ...\n",
      "[486/500] train_loss: 0.53315 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493799 --> 0.493798).  Saving model ...\n",
      "[487/500] train_loss: 0.53435 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[488/500] train_loss: 0.53739 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[489/500] train_loss: 0.53540 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[490/500] train_loss: 0.53521 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[491/500] train_loss: 0.53718 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[492/500] train_loss: 0.53438 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[493/500] train_loss: 0.53603 valid_loss: 0.49380\n",
      "[494/500] train_loss: 0.53776 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[495/500] train_loss: 0.53325 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[496/500] train_loss: 0.53644 valid_loss: 0.49380\n",
      "Epoch 00496: reducing learning rate of group 0 to 4.5079e-07.\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[497/500] train_loss: 0.53504 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[498/500] train_loss: 0.53559 valid_loss: 0.49380\n",
      "[499/500] train_loss: 0.53807 valid_loss: 0.49380\n",
      "Validation loss decreased (0.493798 --> 0.493798).  Saving model ...\n",
      "[500/500] train_loss: 0.53323 valid_loss: 0.50973\n"
     ]
    }
   ],
   "source": [
    "result = dict()\n",
    "col = ['d_close','d_open','d_high','d_low','low_rate','high_rate']\n",
    "for i in col:\n",
    "    predict,truth = train_and_get(i)\n",
    "    result.update({i:predict})\n",
    "    result.update({i+'_truth':truth})\n",
    "\n",
    "for i in ['open','close','high','low']:\n",
    "    result.update({i+'_truth':get_truth(i)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result.keys():\n",
    "    result[i] = result[i].reshape(-1)\n",
    "output = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28c56d9eca0>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output['low_error'] = output['low_rate'] - output['low_rate_truth']\n",
    "output['high_error'] = output['high_rate'] - output['high_rate_truth']\n",
    "plt.plot(output['low_error'])\n",
    "plt.plot(output['high_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('result1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00d1462882258b783cb181eb86adde5698c4f2204c9695f847918151df6a5450"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
